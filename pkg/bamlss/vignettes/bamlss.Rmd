---
title: "Distributional Regression with _bamlss_"
author: "Nikolaus Umlauf, Nadja Klein, Achim Zeileis, Thorsten Simon"
output:
  html_document:
    toc: true
    toc_float: true
    theme: flatly
bibliography: bamlss.bib
nocite: '@bamlss:Umlauf+bamlss:2018'
vignette: >
  %\VignetteIndexEntry{First Steps}
  %\VignetteEngine{knitr::rmarkdown}
  %\VignetteDepends{bamlss}
  %\VignetteKeywords{distributional regression, first steps}
  %\VignettePackage{bamlss}
---

```{r preliminaries, echo=FALSE, message=FALSE, results="hide"}
library("bamlss")
prefix <- "http://bayesr.R-Forge.R-project.org/articles/" ## ""
prefix2 <- "http://bayesr.R-Forge.R-project.org/reference/" ## ""
```

## Overview

The R package _bamlss_ provides a modular computational framework for distributional regression
models (and beyond). The implementation follows the conceptional framework presented in
@bamlss:Umlauf+Klein+Zeileis:2017, which supports Bayesian and/or frequentist estimation engines
using complex possibly nonlinear model terms of any type. The highlights of the package are:  

* A unified model description where a `formula` specifies how to set up the predictors
  from the `data` and `family` objects provide information about the response distribution.
* A generic method for setting up model terms and a `model.frame` along with the
  corresponding prior structures. A `transformer` can optionally set up modified terms, e.g.,
  using mixed model representation for smooth terms.
* Support for modular and exchangeable updating functions or complete model fitting engines
  in order to optionally implement either algorithms for maximization of the log-posterior for
  posterior mode estimation or for solving high-dimensional integrals, e.g., for posterior mean
  or median estimation.
  First, an (optional) `optimizer` function can be run, e.g., for computing posterior mode
  estimates. Second, a `sampler` is employed for full Bayesian inference with MCMC, which uses the
  posterior mode estimates from the `optimizer` as staring values. An additional step can be used
  for preparing the `results`.
* Standard post-modeling extractor functions to create sampling statistics, visualizations,
  predictions, etc.

More detailed overviews and examples are provided in the articles:

* [Available Model Terms](`r prefix`terms.html)
* [Generalized Linear Models (+)](`r prefix`glm.html)
* [BAMLSS Families](`r prefix`families.html)
* [Visualization with distreg.vis](`r prefix`distregvis.html)
* [Munich Rent Model](`r prefix`rent.html)
* [Bivariate Gaussian models for wind vectors](`r prefix`bivnorm.html)
* [LASSO-Type Penalization in the Framework of Generalized Additive Models for Location, Scale and Shape](`r prefix`lasso.html)
* [Estimation Engines](`r prefix`engines.html)
* [BAMLSS Model Frame](`r prefix`bf.html)

## Installation

The stable release version of _bamlss_ is hosted on the Comprehensive R Archive Network
(CRAN) at <https://CRAN.R-project.org/package=bamlss> and can be installed via

```{r installation-cran, eval=FALSE}
install.packages("bamlss")
```

The development version of _bamlss_ is hosted on R-Forge at
<https://R-Forge.R-project.org/projects/bayesr/> in a Subversion (SVN) repository.
It can be installed via

```{r installation-rforge, eval=FALSE}
install.packages("bamlss", repos = "http://R-Forge.R-project.org")
```

## Basic Bayesian regression

This section gives a first quick overview of the functionality of the package and
demonstrates that the usual "look & feel" when using well-established model fitting 
functions like `glm()` is an elementary part of _bamlss_, i.e., first steps and
basic handling of the package should be relatively simple. We illustrate the first steps
with _bamlss_ using a data set taken from the _Regression Book_ [@bamlss:Fahrmeir+Kneib+Lang+Marx:2013]
which is about prices of used VW Golf cars. The data is loaded with
```{r}
data("Golf", package = "bamlss")
print(head(Golf))
```
In this example the aim is to model the `price` in 1000 Euro. Using _bamlss_ a first Bayesian
linear model could be set up by first specifying a model formula
```{r}
f <- price ~ age + kilometer + TIA + abs + sunroof
```
afterwards the fully Bayesian model using MCMC simulation is estimated by
```{r, message=FALSE, results="hide"}
library("bamlss")

set.seed(111)

b1 <- bamlss(f, family = "gaussian", data = Golf)
```
Note that the default number of iterations for the MCMC sampler is 1200, the burnin-phase is 200 and
thinning is 1 (see the manual of the default MCMC sampler <code>[GMCMC()](`r prefix2`GMCMC.html)</code>).
The reason is that during the modeling process, users usually want to obtain first
results rather quickly. Afterwards, if a final model is estimated the number of iterations of the
sampler is usually set much higher to get close to i.i.d. samples from the posterior distribution.
To obtain reasonable starting values for the MCMC sampler we run a backfitting algorithm that
optimizes the posterior mode. The _bamlss_ package uses its own family objects, which can be
specified as characters using the <code>[bamlss()](`r prefix2`bamlss.html)</code> wrapper, in this
case `family = "gaussian"` (see also [BAMLSS Families](`r prefix`families.html)). In addition, the
package also supports all families provided from the
[_gamlss_ families](https://cran.r-project.org/package=gamlss.dist "CRAN gamlss.dist").

The model summary gives
```{r}
summary(b1)
```
indicating high acceptance rates as reported by the `alpha` parameter in the linear model
output, which is a sign of good mixing of the MCMC chains. The mixing can also be inspected
graphically by
```{r, eval=FALSE}
plot(b1, which = "samples")
```
```{r, fig.width = 9, fig.height = 5, fig.align = "center", echo = FALSE, dev = "png", results = 'hide', message=FALSE}
bsp <- b1
bsp$samples <- bsp$samples[, c("mu.p.(Intercept)", "sigma.p.(Intercept)")]
plot(bsp, which = "samples")
```
Note, for convenience we only show the traceplots of the intercepts. Considering significance of the
estimated effects, only variables `TIA` and `sunroof` seem to have no effect on `price` since the
credible intervals of estimated parameters contain zero. This information can also be extracted
using the implemented `confint()` method.
```{r}
confint(b1, prob = c(0.025, 0.975))
```

Since the prices cannot be negative, a possible consideration is to use a logarithmic transformation
of the response `price`
```{r, message=FALSE, results="hide"}
set.seed(111)

f <- log(price) ~ age + kilometer + TIA + abs + sunroof

b2 <- bamlss(f, family = "gaussian", data = Golf)
```
and compare the models using the `DIC()`
```{r}
DIC(b1, b2)
```
indicating that the transformation seems to improve the model fit.

Predictions can be easily computed using the `predict()` method. One major difference compared
to other regression model implementations is that predictions can be made for single variables,
only, where the user does not have to create a new data frame containing all variables. For example,
posterior mean estimates and 95% credible intervals for variable `age` can be obtained by
```{r}
nd <- data.frame("age" = seq(min(Golf$age), max(Golf$age), length = 100))

p <- predict(b2, newdata = nd, model = "mu", term = "age", FUN = c95)

print(head(p))
```
Here, we need to specify for which model predictions should be calculated, and if predictions
only for variable `age` are created, argument `term` needs also be specified.
Argument `FUN` can be any function that should be applied on the samples of the linear predictor.
For more examples see the documentation of the
<code>[predict.bamlss()](`r prefix2`predict.bamlss.html)</code> method.

Then, the estimated effect of `age` can be visualized with
```{r, eval=FALSE}
plot2d(p ~ age, data = nd,
  fill.select = c(0, 1, 0, 1),
  lty = c(2, 1, 2))
```
```{r, fig.width = 4, fig.height = 4, fig.align = "center", dev = "png", results='hide', message=FALSE, echo=FALSE, out.width="40%"}
par(mar = c(4.1, 4.1, 0.1, 0.1))
plot2d(p ~ age, data = nd,
  fill.select = c(0, 1, 0, 1), lty = c(2, 1, 2))
```
The figure clearly shows the negative effect on the logarithmic `price` for variable `age`.


## Location-scale model

```{r preliminaries2, echo=FALSE, message=FALSE, results="hide"}
data("mcycle", package = "MASS")
f <- list(accel ~ s(times, k = 20), sigma ~ s(times, k = 20))
if(!file.exists("toymodel.rda")) {
  set.seed(123)
  b <- bamlss(f, data = mcycle, family = "gaussian",
    n.iter = 12000, burnin = 2000, thin = 10)
} else {
  load("toymodel.rda")
}
```

As a second startup on how to use _bamlss_ for full distributional regression, we illustrate the basic
steps on a small textbook example using the well-known simulated motorcycle accident
data [@bamlss:Silverman:1985]. The data contain measurements of the head acceleration
(in $g$, variable `accel`) in a simulated motorcycle accident, recorded in milliseconds after
impact (variable `times`).
```{r}
data("mcycle", package = "MASS")
print(head(mcycle))
```
To estimate a Gaussian location-scale model with
$$
\texttt{accel} \sim \mathcal{N}(\mu = f(\texttt{times}), \log(\sigma) = f(\texttt{times}))
$$
we use the following model formula
```{r}
f <- list(accel ~ s(times, k = 20), sigma ~ s(times, k = 20))
```
where `s()` is the smooth term constructor from the _mgcv_ [@bamlss:Wood:2018]. Note,
that formulae are provided as `list`s of formulae, i.e., each list entry represents one
parameter of the response distribution. Also note that all
smooth terms, i.e., `te()`, `ti()`, etc., are supported by _bamlss_. This way, it is also
possible to incorporate user defined model terms. A full Bayesian model is the
estimated with
```{r, eval=FALSE}
set.seed(123)

b <- bamlss(f, data = mcycle, family = "gaussian",
  n.iter = 12000, burnin = 2000, thin = 10)
```
using `12000` iterations for the MCMC chain, a burnin of `2000` (dropped samples) and a thinning
of `10`, i.e., only every 10th sample is saved. Note that per defaul `bamlss()` uses a backfitting
algorithm to compute posterior mode estimates, afterwards these estimates are used as starting
values for the MCMC chains.
The returned object is of class `"bamlss"` for which generic extractor functions like
`summary()`, `plot()`, `predict()`, etc., are provided. For example, the estimated effects
for distribution paramaters `mu` and `sigma` can be visualized by
```{r, eval=FALSE}
plot(b, model = c("mu", "sigma"))
```
```{r, fig.width = 9, fig.height = 3.5, fig.align = "center", echo = FALSE, dev = "png", results = 'hide', message=FALSE}
par(mar = c(4.1, 4.1, 1.1, 1.1), mfrow = c(1, 2))
plot(b, pages = 1, spar = FALSE, scheme = 2, grid = 100)
```
The model summary gives
```{r}
summary(b)
```
showing, e.g., the acceptance probabilities of the MCMC chains (`alpha`), the estimated degrees of freedom of
the optimizer and the successive sampler (`edf`), the final AIC and DIC as well as parametric
model coefficients (in this case only the intercepts). As mentioned in the first example, using MCMC
involves convergence checks of the sampled parameters. The easiest diagnostics are traceplots
```{r, eval=FALSE}
plot(b, which = "samples")
```
```{r, fig.width = 9, fig.height = 5, fig.align = "center", echo = FALSE, dev = "png", results = 'hide', message=FALSE}
bsp <- b
bsp$samples <- bsp$samples[, c("mu.p.(Intercept)", "sigma.p.(Intercept)")]
plot(bsp, which = "samples")
```
Note again that this call would show all traceplots, for convenience we only show the plots for the intercepts.
In this case, the traceplots indicate convergence of the Markov chains (only the ACF for parameter `"mu"`
still shows some autocorrelation, which could be further reduced, e.g., by increasing the thinning
parameter). Further inspections are the maximum autocorrelation of all parameters, `which = "max-acf"`,
besides other convergence diagnostics, e.g., diagnostics that are part of the _coda_ package
[@bamlss:Plummer+Best+Cowles+Vines:2006].

Inspecting randomized quantile residuals [@bamlss:Dunn+Gordon:1996] is useful for judging how
well the model fits to the data
```{r, eval = FALSE}
plot(b, which = c("hist-resid", "qq-resid"))
```
```{r, fig.width = 8, fig.height = 4, fig.align = "center", echo = FALSE, dev = "png", echo=FALSE}
par(mfrow = c(1, 2))
plot(b, which = c("hist-resid", "qq-resid"), spar = FALSE)
```
Randomized quantile residuals are the default method in _bamlss_, which are computed using
the cdf function of the corresponding family object.

The posterior mean function for new data based on MCMC samples for parameter $\mu$ can be computed by
```{r, eval=FALSE}
nd <- data.frame("times" = seq(2.4, 57.6, length = 100))
nd$p <- predict(b, newdata = nd, model = "mu", FUN = mean)
plot2d(p ~ times, data = nd)
```
```{r, fig.width = 7, fig.height = 4, fig.align = "center", dev = "png", results='hide', message=FALSE, echo=FALSE, out.width="60%"}
par(mar = c(4.1, 4.1, 1.1, 1.1))
nd <- data.frame("times" = seq(2.4, 57.6, length = 100))
nd$p <- predict(b, newdata = nd, model = "mu", FUN = mean)
plot2d(p ~ times, data = nd)
```
where argument `FUN` can be any function, e.g., a function computing credible intervals from
the empirical quantiles of the MCMC samples
```{r}
foo <- function(x) {
  quantile(x, probs = c(0.025, 0.5, 0.975))
}
nd$p <- predict(b, newdata = nd, model = "mu", FUN = foo)
print(head(nd))
```

## Spatial location-scale model

This example is taken from the _R2BayesX_ package [@bamlss:Umlauf+Adler+Kneib+Lang+Zeileis:2014] and
is about undernutrition of new born children in Zambia. The data is loaded with
```{r}
data("ZambiaNutrition", package = "R2BayesX")
print(head(ZambiaNutrition))
```
Here, the primary interest is to model the dependence of `stunting` of newborn children, with an
age ranging from 0 to 5 years, on covariates such as the body mass index of the mother, the age of
the child and others. Moreover, we apply a full distributional regression model with
$$
\texttt{stunting} \sim \mathcal{N}(\mu = \eta_{\mu}, \log(\sigma) = \eta_{\sigma})),
$$
where the predictors $\eta_{\mu}$ and $\eta_{\sigma}$ are specified by the following formula
```{r}
f <- list(
  stunting ~ memployment + urban + gender + meducation +
    s(mbmi) + s(agechild) + s(district, bs = "mrf", xt = list("penalty" = K)) +
    s(district, bs = "re"),
  sigma  ~ memployment + urban + gender + meducation +
    s(mbmi) + s(agechild) + s(district, bs = "mrf", xt = list("penalty" = K)) +
    s(district, bs = "re")
)
```
Note that for setting up the Markov random field smooth term a penalty matrix `K` needs to be
provided. The penalty matrix forces penalization for neighboring regions of the districts in Zambia.
To compute the `K` matrix, we need the spatial information about the regions in Zambia, which
is shipped as a `"bnd"` object in the _R2BayesX_ package and can be loaded with
```{r}
data("ZambiaBnd", package = "R2BayesX")
```
The `K` matrix can then be computed using function
<code>[neighbormatrix()](`r prefix2`neighbormatrix.html)</code>
```{r}
K <- neighbormatrix(ZambiaBnd)
print(head(K))

## Also need to transform to factor for
## setting up the MRF smooth.
ZambiaNutrition$district <- as.factor(ZambiaNutrition$district)

## Now note that not all regions are observed,
## therefore we need to remove those regions
## from the penalty matrix
rn <- rownames(K)
lv <- levels(ZambiaNutrition$district)
i <- rn %in% lv
K <- K[i, i]
```
Then, the model can be estimated with
```{r, eval=FALSE}
set.seed(321)
b <- bamlss(f, data = ZambiaNutrition, family = "gaussian")
```
and the estimated univariate effects are plotted with
```{r, eval=FALSE}
plot(b)
```
```{r, fig.width = 7, fig.height = 7, fig.align = "center", dev = "png", results='hide', message=FALSE, echo=FALSE, out.width="60%"}
dir.create(tdir <- tempfile())
download.file("http://bayesr.r-forge.r-project.org/misc/ZambiaModel.rda", file.path(tdir, "ZM.rda"))
load(file.path(tdir, "ZM.rda"))
unlink(tdir)
par(mar = c(4.1, 4.1, 1.1, 0.1))
plot(b, ask = FALSE)
```
The plot indicates that only the effect of variable `mbmi` on the standard deviation is not
significant according the 95% credible intervals and basically follows the zero horizontal line.

To visualize the structured and unstructured spatial effects we predict using the district
information
```{r}
## First, note that we have the structured id = 'mrf1' and unstructured
## spatial effect id = 're2', also indicated in the model summary
summary(b)

## Now, to predict the spatial effects we set up new data.
nd <- data.frame("district" = levels(ZambiaNutrition$district))

## Predict for the structured spatial effects.
p_str <- predict(b, newdata = nd, term = "s(district,id='mrf1')", intercept = FALSE)

## And the unstructured spatial effect.
p_unstr <- predict(b, newdata = nd, term = "s(district,id='re2')", intercept = FALSE)
```
Now, to visualize the effects we plot all maps using the same range
for the color legends
```{r}
r_mu <- range(c(p_str$mu, p_unstr$mu))
r_mu <- c(-1 * max(abs(r_mu)), max(abs(r_mu)))

r_sigma <- range(c(p_str$sigma, p_unstr$sigma))
r_sigma <- c(-1 * max(abs(r_sigma)), max(abs(r_sigma)))
```
and plot the effects using a diverging color legend.
```{r, eval=FALSE}
## MRF smooth effect.
plotmap(ZambiaBnd, x = p_str$mu, id = nd$district, color = diverge_hcl, range = r_mu,
  main = expression(mu), shift = 0.1, title = "MRF")
plotmap(ZambiaBnd, x = p_str$sigma, id = nd$district, color = diverge_hcl, range = r_sigma,
  main = expression(sigma), shift = 0.1, title = "MRF")

## Random effects.
plotmap(ZambiaBnd, x = p_unstr$mu, id = nd$district, color = diverge_hcl, range = r_mu,
  shift = 0.1, title = "Random effect")
plotmap(ZambiaBnd, x = p_unstr$sigma, id = nd$district, color = diverge_hcl, range = r_sigma,
  shift = 0.1, title = "Random effect")
```
```{r, fig.width = 7, fig.height = 7, fig.align = "center", dev = "png", results='hide', message=FALSE, echo=FALSE, out.width="60%"}
par(mfrow = c(2, 2), mar = c(0, 0, 4.1, 0))
plotmap(ZambiaBnd, x = p_str$mu, id = nd$district, color = diverge_hcl, range = r_mu,
  main = expression(mu), shift = 0.1, title = "MRF")
plotmap(ZambiaBnd, x = p_str$sigma, id = nd$district, color = diverge_hcl, range = r_sigma,
  main = expression(sigma), shift = 0.1, title = "MRF")

plotmap(ZambiaBnd, x = p_unstr$mu, id = nd$district, color = diverge_hcl, range = r_mu,
  shift = 0.1, title = "Random effect")
plotmap(ZambiaBnd, x = p_unstr$sigma, id = nd$district, color = diverge_hcl, range = r_sigma,
  shift = 0.1, title = "Random effect")
```
The maps clearly show that the unstructured spatial effect seems to very small, if existent
at all when looking the 95% credible intervals:
```{r}
## Again predict, but now additionally compute 95% credible intervals
## using function c95().
p_unstr <- predict(b, newdata = nd, term = "s(district,id='re2')", intercept = FALSE, FUN = c95)

## Test if all effects contain zero, i.e., are not significant
## according the 95% credible intervals.
all(p_unstr$mu[["2.5%"]] < 0 & p_unstr$mu[["97.5%"]] > 0)
all(p_unstr$sigma[["2.5%"]] < 0 & p_unstr$sigma[["97.5%"]] > 0)
```


## References

