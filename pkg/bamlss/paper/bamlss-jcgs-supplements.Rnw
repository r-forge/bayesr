\documentclass[12pt]{article}
\usepackage{amsmath}
\usepackage{graphicx,psfrag,epsf}
\usepackage{enumerate}
\usepackage{natbib}
\usepackage{url} % not crucial - just used below for the URL 

%\pdfminorversion=4
% NOTE: To produce blinded version, replace "0" with "1" below.
\newcommand{\blind}{0}

% DON'T change margins - should be 1 inch all around.
\addtolength{\oddsidemargin}{-.5in}%
\addtolength{\evensidemargin}{-.5in}%
\addtolength{\textwidth}{1in}%
\addtolength{\textheight}{1.3in}%
\addtolength{\topmargin}{-.8in}%

%% bamlss specific.
\usepackage{amssymb,amsfonts,thumbpdf}
\usepackage{xcolor}
\usepackage{multirow,longtable}
\usepackage{algorithmic}
\usepackage{algorithm}
\usepackage{rotating}
\usepackage{enumitem}
\usepackage{fancyvrb}
\shortcites{bamlss:stan-software:2016}

\definecolor{darkgray}{rgb}{0.1,0.1,0.1}
\definecolor{heat1}{rgb}{0.8274510, 0.2470588, 0.4156863}
\definecolor{heat2}{rgb}{0.8823529, 0.4392157, 0.2980392}
\definecolor{heat3}{rgb}{0.9137255, 0.6039216, 0.1725490}
\definecolor{heat4}{rgb}{0.9098039, 0.7647059, 0.2352941}
\definecolor{heat5}{rgb}{0.8862745, 0.9019608, 0.7411765}
\definecolor{blue1}{RGB}{0, 126, 255}

\newcommand{\squote}[1]{`{#1}'}
\newcommand{\dquote}[1]{``{#1}''}
\newcommand{\fct}[1]{{\texttt{#1()}\index{#1@\texttt{#1()}}}}
\newcommand{\class}[1]{\dquote{\texttt{#1}}}
\let\proglang=\textsf
\newcommand{\pkg}[1]{{\fontseries{b}\selectfont #1}}
\makeatletter
\newcommand\code{\bgroup\@makeother\_\@makeother\~\@makeother\$\@codex}
\def\@codex#1{{\normalfont\ttfamily\hyphenchar\font=-1 #1}\egroup}
\makeatother
\DefineVerbatimEnvironment{Sinput}{Verbatim}{fontshape=sl}
\DefineVerbatimEnvironment{Code}{Verbatim}{fontshape=sl}
\DeclareMathOperator{\diag}{diag}

\date{}

\begin{document}

%\bibliographystyle{natbib}

\def\spacingset#1{\renewcommand{\baselinestretch}%
{#1}\small\normalsize} \spacingset{1}


%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\if0\blind
{
  \title{\bf Supplemental Material to the Main Manuscript \emph{``BAMLSS: Bayesian Additive Models for Location, Scale and Shape (and Beyond)''}}
  \author{Nikolaus Umlauf\hspace{.2cm}\\
    Department of Statistics, Universit\"at Innsbruck\\
    and \\
    Nadja Klein \\
    Chairs of Statistics and Econometrics, Universit\"at G\"ottingen\\
    and \\
    Achim Zeileis \\
    Department of Statistics, Universit\"at Innsbruck}
  \maketitle
} \fi

\if1\blind
{
  \bigskip
  \bigskip
  \bigskip
  \begin{center}
    {\LARGE\bf Supplemental Material to the Main Manuscript \emph{``BAMLSS: Bayesian Additive Models for Location, Scale and Shape (and Beyond)''}}
\end{center}
  \medskip
} \fi

\newpage

\spacingset{1.45} % DON'T change the spacing!


\section{``Lego brick'' B3, prior structures in BAMLSS}

Table~\ref{tab:mlegobricks} provides an overview of commonly used ``Lego bricks'' for GAM-type
prior structures used within building block B3 in Section~4.

\medskip

\begin{table}[ht!]
\centering
\hspace*{-28ex}\resizebox{11cm}{!} {
\begin{minipage}{\textwidth}
\begin{tabular}{llll}
\hline
Covariates & Effect types $f_{jk}(\mathbf{x}; \boldsymbol{\beta}_{jk})$ & \multicolumn{2}{l}{Prior densities $p_{jk}(\boldsymbol{\beta}_{jk}; \boldsymbol{\tau}_{jk}, \boldsymbol{\alpha}_{jk})$} \\
& &
  $d_{\boldsymbol{\beta}_{jk}}(\boldsymbol{\beta}_{jk} | \, \boldsymbol{\tau}_{jk};
     \boldsymbol{\alpha}_{\boldsymbol{\beta}_{jk}})$ &
  $d_{\tau_{ljk}}(\tau_{ljk} | \, \boldsymbol{\alpha}_{\tau_{ljk}})$ \\ \hline
\multirow{6}{*}{Scalar covariates} & Intercept $\beta$ & \multirow{2}{*}{$\propto$ constant} & \multirow{3}{*}{$\emptyset$} \\
& Linear effect $x \cdot \beta$ &
  \multirow{2}{*}{$\propto \exp \left(- \frac{1}{2}(\boldsymbol{\beta} -
    \mathbf{m})^{\top}\mathbf {P}(\boldsymbol{\tau})(\boldsymbol{\beta} -
    \mathbf{m})\right)$}
                      & \\
& Linear interaction $x_1 \cdot x_2 \cdot \beta$ & & \\ \cline{2-4}
& Smooth effect $f(x)$ & \multirow{8}{*}{$\propto |\mathbf{P}(\boldsymbol{\tau})|^{\frac{1}{2}} \exp\left(
    -\frac{1}{2}\boldsymbol{\beta}^\top\mathbf{P}(\boldsymbol{\tau})\boldsymbol{\beta}\right)$} &
    \multirow{2}{*}{IG $\propto \tau^{-(a + 1)} \exp(-b / \tau)$} \\
& Varying coefficient $f(x_2) \cdot x_1$  & & \\
& Smooth effect $f(x_1, \ldots, x_L)$  & & \multirow{2}{*}{HC $\propto (1 + \tau/a^2)^{-1}(\tau/a^2)^{-1/2}$} \\ \cline{1-1}
\multirow{2}{*}{Grouping variable $s$} & Random intercept $\beta_s$ & &  \\
& Spatial effect $f(s)$ & & \multirow{2}{*}{SD $\propto (\tau/\sqrt{\tau})^{-1/2} \exp(-(\tau/a)^{1/2})$} \\ \cline{1-1}
\multirow{2}{*}{Grouping and scalar,} & Random slope $x \cdot \beta_s$ & & \\
\multirow{2}{*}{time variable $t$}& Space-time effect $f(s, t)$ & & \multirow{2}{*}{HN $\propto \tau^{1/2 -1} \exp(-\tau/(2a^2))$} \\
& Functional random intercept $f_s(t)$ & & \\ \hline
\end{tabular}
\end{minipage}
}
\caption{\label{tab:mlegobricks} Commonly used ``Lego bricks'', building block B3, for model
  terms in BAMLSS. Priors for linear effects assume that the precision matrix $\mathbf{P}(\boldsymbol{\tau})$
  is fixed. For smooth effects, prior densities are: inverse gamma (IG), half-Cauchy (HC),
  scale-dependent (SD) and half-normal (HN).}
\end{table}

\section{``Lego bricks'' for the censored Gaussian distribution}

The following presents the ``Lego bricks'' of example Section~6.1 in the main manuscript that are
used for the construction of iteratively weighted least squares (IWLS) updating functions
$U_{jk}( \cdot )$ of the Gaussian model left censored at zero.  
\begin{enumerate}[leftmargin=*,align=left]
\item[B1.] The density function of a Gaussian distribution left censored at zero is given by
\begin{equation}
  f(y; \, \mu = \eta_{\mu}, \, \log(\sigma) = \eta_{\sigma}) = \begin{cases} 
    \frac{1}{\sigma} \phi\left(\frac{y-\mu}{\sigma} \right) & y > 0 \\
    \Phi\left(\frac{-\mu}{\sigma} \right) & \mbox{else,}
  \end{cases}
\end{equation}
where $\phi$ is the probability density and $\Phi$ the cumulative distribution function of the
standard normal distribution.

\item[B6b.] Score vectors $\mathbf{u}_k = \partial \ell(\boldsymbol{\beta}; \mathbf{y}, \mathbf{X}) /
\partial \boldsymbol{\eta}_k$ are computed with
\begin{equation*}
\frac{\partial \ell(\boldsymbol{\beta}; y, \mathbf{x})}{
\partial \eta_{\mu}} = \begin{cases}
  \frac{y - \mu}{\sigma^2} & y > 0 \\
  -\frac{1}{\sigma} \frac{\phi\left(\frac{-\mu}{\sigma} \right)}
    {\Phi\left(\frac{-\mu}{\sigma} \right)} & \mbox{else,}
\end{cases}
\end{equation*}
and
\begin{equation*}
\frac{\partial \ell(\boldsymbol{\beta}; y, \mathbf{x})}{
\partial \eta_{\sigma}} = \begin{cases} 
  -1 + \frac{(y - \mu)^2}{\sigma^2} & y > 0 \\ 
  -\frac{-\mu}{\sigma} \frac{\phi\left(\frac{-\mu}{\sigma} \right)}
    {\Phi\left(\frac{-\mu}{\sigma} \right)} & \mbox{else.}
\end{cases}
\end{equation*}
\item[B7b.] The diagonal elements of the weight matrix
$\mathbf{W}_{kk} = -\mathrm{diag}(\partial^2 \ell(\boldsymbol{\beta}; \mathbf{y}, \mathbf{X}) /
\partial \boldsymbol{\eta}_k \partial \boldsymbol{\eta}_k^\top)$ are derived using
\begin{equation*}
\frac{\partial^2 \ell(\boldsymbol{\beta}; y, \mathbf{x})}{
\partial \eta_{\mu}^2} = \begin{cases} 
  -\frac{1}{\sigma^2} & y > 0 \\
  -\frac{-\mu}{\sigma^3}\frac{\phi\left(\frac{-\mu}{\sigma} \right)}
    {\Phi\left(\frac{-\mu}{\sigma} \right)}-\frac{1}{\sigma^2}
    \frac{\phi\left(\frac{-\mu}{\sigma} \right)^2}
    {\Phi\left(\frac{-\mu}{\sigma} \right)^2}
  & \mbox{else,}
\end{cases}
\end{equation*}
and
\begin{equation*}
\frac{\partial^2 \ell(\boldsymbol{\beta}; y, \mathbf{x})}{
\partial \eta_{\sigma}^2} = \begin{cases} 
  - 2 \frac{(y-\mu)^2}{\sigma^2} & y > 0 \\
  - \frac{-\mu}{\sigma}\frac{\phi\left(\frac{-\mu}{\sigma} \right)}
    {\Phi\left(\frac{-\mu}{\sigma} \right)} - \frac{(-\mu)^3}{\sigma^3}
    \frac{\phi\left(\frac{-\mu}{\sigma} \right)}
    {\Phi\left(\frac{-\mu}{\sigma} \right)} - \frac{(-\mu)^2}{\sigma^2}
    \frac{\phi\left(\frac{-\mu}{\sigma} \right)^2}
    {\Phi\left(\frac{-\mu}{\sigma} \right)^2} & \mbox{else.}
\end{cases}
\end{equation*}
\end{enumerate}


\section{``Lego bricks'' for the Cox model}

Based on the usrvival function
$$
S(t) = \mathrm{Prob}(T > t) = \exp \, \left( -\int_0^t \lambda(u)du \right),
$$
for full Bayesian inference the following ``Lego bricks'' need to be implemented for updating
functions $U_{jk}( \cdot )$ using algorithms A1, A2a and A2b (algorithms are presented in
Section~3.2 in the main manuscript):
\begin{enumerate}[leftmargin=*,align=left]
\item[B1.] The log-likelihood function of the continuous time Cox model is given by
  $$
  \ell(\boldsymbol{\beta}; \mathbf{y}, \mathbf{X}) =
   \sum_{i = 1}^n \left( \delta_i\eta_{i\gamma} - \int_0^{t_i} \exp( \eta_{i\lambda}(u)du ) \right).
  $$
  where $\delta_i$ is the usual censoring indicator, which equals to $\delta_i = 1$ in this example,
  because we focus on real fire events.
\item[B6a.] For derivative-based estimation using Algorithm~A2a and for MCMC simulation with
  Algorithm~A2b, the score vectors and Hessian need to be computed.
  Assuming a basis function approach, the score vector of the regression coefficients for the
  time-varying part $\eta_{\lambda}(t)$ is
  $$
  \mathbf{s}\left(\boldsymbol{\beta}_{\lambda}\right) = \boldsymbol{\delta}^{\top}\mathbf{X}_{\lambda}(\mathbf{t}) -
    \sum_{i = 1}^n \exp(\eta_{i\gamma})\left( \int_0^{t_i} \exp( \eta_{i\lambda}(u) )\mathbf{x}_i(u)du \right).
  $$
\item[B7a.] The elements of the Hessian w.r.t.\ $\boldsymbol{\beta}_{\lambda}$ are
  $$
  \mathbf{H}\left(\boldsymbol{\beta}_{\lambda}\right) =
  -\sum_{i = 1}^{n} \exp\left( \eta_{i\gamma} \right) \int_{0}^{t_i} \exp( \eta_{i\lambda}(u) )
    \boldsymbol{x}_{i\lambda}(u)\boldsymbol{x}_{i\lambda}^{\top}(u)du.
  $$
  Note that the Hessian cannot be fragmented further to obtain building block B7b and
  IWLS updating functions. The reason is that the diagonal weight
  matrix based on \newline $\partial^2 \ell(\boldsymbol{\beta}; \mathbf{y}, \mathbf{X}) /
  \partial \boldsymbol{\eta}_{\lambda}(\mathbf{t}) \partial \boldsymbol{\eta}_{\lambda}(\mathbf{t})^\top$
  requires a functional derivative like the Hadamard derivative since the predictor depends on time
  $t$. However, it turns out that this derivative forms martingale residuals in the IWLS step
  \citep[see, e.g.,][]{bamlss:Barlow+Prentice:1988} which are incapable of estimating time-varying
  effects, see also \citet[Section~5.2]{bamlss:Hofner:2008} for a detailed discussion. Therefore
  updating functions $U_{jk}( \cdot )$ for the time-varying predictor $\eta_{\lambda}(t)$ are based
  on updating Equation~(17) within Algorithm~A2a and A2b.
\item[B6b \& B7b.] Constructing updating functions for the time-constant part
  $\eta_{\gamma}$ again yields an IWLS updating scheme, see Section~4, with
  working observations given by
  $$
  \mathbf{z} = \boldsymbol{\eta}_{\gamma} + \mathbf{W}^{-1}\mathbf{u},
  $$
  with the weight matrix
  $$
  \mathbf{W} = \mathrm{diag}(\mathbf{P}\exp( \boldsymbol{\eta}_{\gamma})),
  $$
  where $\mathbf{P}$ is a diagonal matrix with elements $p_{ii} = \int_0^{t_i}
  \exp( \eta_{i\lambda}(u)du )$. The score vector is
  $$
  \mathbf{u} = \boldsymbol{\delta} - \mathbf{P}\exp(\boldsymbol{\eta}_{\gamma}).
  $$
  \citep{bamlss:Hennerfeind+Brezger+Fahrmeir:2006}
\end{enumerate}


\section{Posterior mode updating based on IWLS} \label{appendix:pmodeiwls}

The following shows the steps needed to derive the iterative updating scheme based on IWLS in
Section~4.2. Focusing on the $j$-th row of (14) gives
\begin{eqnarray*}
(\mathbf{X}_{jk}^\top\mathbf{W}_{kk}\mathbf{X}_{jk} + \mathbf{G}_{jk}(\boldsymbol{\tau}_{jk}))\boldsymbol{\beta}_{jk}^{(t+1)} +
\ldots + \mathbf{X}_{jk}^\top\mathbf{W}_{kk}\mathbf{X}_{J_kk}\boldsymbol{\beta}_{J_kk}^{(t + 1)} - \\
(\mathbf{X}_{jk}^\top\mathbf{W}_{kk}\mathbf{X}_{jk} + \mathbf{G}_{jk}(\boldsymbol{\tau}_{jk}))\boldsymbol{\beta}_{jk}^{(t)} -
\ldots - \mathbf{X}_{jk}^\top\mathbf{W}_{kk}\mathbf{X}_{J_kk}\boldsymbol{\beta}_{J_kk}^{(t)}
  &=& \mathbf{X}_{jk}^\top \mathbf{u}_k^{(t)} - \mathbf{G}_{jk}(\boldsymbol{\tau}_{jk})\boldsymbol{\beta}_{jk}^{(t)}
\end{eqnarray*}
\begin{eqnarray*}
\mathbf{G}_{jk}(\boldsymbol{\tau}_{jk})(\boldsymbol{\beta}_{jk}^{(t+1)} - \boldsymbol{\beta}_{jk}^{(t)}) +
  \mathbf{X}_{jk}^\top\mathbf{W}_{kk}\mathbf{X}_{jk}\boldsymbol{\beta}_{jk}^{(t+1)} + \ldots +
  \mathbf{X}_{jk}^\top\mathbf{W}_{kk}\mathbf{X}_{J_kk}\boldsymbol{\beta}_{J_kk}^{(t+1)} - \\
  \mathbf{X}_{jk}^\top\mathbf{W}_{kk}\mathbf{X}_{jk}\boldsymbol{\beta}_{jk}^{(t)} - \ldots -
  \mathbf{X}_{jk}^\top\mathbf{W}_{kk}\mathbf{X}_{J_kk}\boldsymbol{\beta}_{J_kk}^{(t)}
  \,\,\, = \,\,\, \mathbf{X}_{jk}^\top \mathbf{u}_k^{(t)} - \mathbf{G}_{jk}(\boldsymbol{\tau}_{jk})\boldsymbol{\beta}_{jk}^{(t)} &&
\end{eqnarray*}
\begin{eqnarray*}
\mathbf{G}_{jk}(\boldsymbol{\tau}_{jk})\boldsymbol{\beta}_{jk}^{(t)} +
  \mathbf{G}_{jk}(\boldsymbol{\tau}_{jk})(\boldsymbol{\beta}_{jk}^{(t+1)} - \boldsymbol{\beta}_{jk}^{(t)}) +
  \mathbf{X}_{jk}^\top\mathbf{W}_{kk}\mathbf{X}_{jk}\boldsymbol{\beta}_{jk}^{(t+1)} + \ldots +
  \mathbf{X}_{jk}^\top\mathbf{W}_{kk}\mathbf{X}_{J_kk}\boldsymbol{\beta}_{J_kk}^{(t+1)} - \\
  \mathbf{X}_{jk}^\top\mathbf{W}_{kk}\mathbf{X}_{jk}\boldsymbol{\beta}_{jk}^{(t)} - \ldots -
  \mathbf{X}_{jk}^\top\mathbf{W}_{kk}\mathbf{X}_{J_kk}\boldsymbol{\beta}_{J_kk}^{(t)}
  \,\,\,=\,\,\, \mathbf{X}_{jk}^\top \mathbf{u}_k^{(t)} &&
\end{eqnarray*}
\begin{eqnarray*}
\mathbf{G}_{jk}(\boldsymbol{\tau}_{jk})\boldsymbol{\beta}_{jk}^{(t+1)} +
  \mathbf{X}_{jk}^\top\mathbf{W}_{kk}\mathbf{X}_{jk}\boldsymbol{\beta}_{jk}^{(t+1)} + \ldots +
  \mathbf{X}_{jk}^\top\mathbf{W}_{kk}\mathbf{X}_{J_kk}\boldsymbol{\beta}_{J_kk}^{(t+1)} - \\
  \mathbf{X}_{jk}^\top\mathbf{W}_{kk}\mathbf{X}_{jk}\boldsymbol{\beta}_{jk}^{(t)} - \ldots -
  \mathbf{X}_{jk}^\top\mathbf{W}_{kk}\mathbf{X}_{J_kk}\boldsymbol{\beta}_{J_kk}^{(t)}
  &=& \mathbf{X}_{jk}^\top \mathbf{u}_k^{(t)}
\end{eqnarray*}
\begin{eqnarray*}
\mathbf{G}_{jk}(\boldsymbol{\tau}_{jk})\boldsymbol{\beta}_{jk}^{(t+1)} +
  \mathbf{X}_{jk}^\top\mathbf{W}_{kk}\mathbf{X}_{jk}\boldsymbol{\beta}_{jk}^{(t+1)} + 
  \mathbf{X}_{jk}^\top\mathbf{W}_{kk}\boldsymbol{\eta}_{k, -j}^{(t+1)} -
  \mathbf{X}_{jk}^\top\mathbf{W}_{kk}\boldsymbol{\eta}_{k}^{(t)}
  &=& \mathbf{X}_{jk}^\top \mathbf{u}_k^{(t)}
\end{eqnarray*}
\begin{eqnarray*}
(\mathbf{X}_{jk}^\top\mathbf{W}_{kk}\mathbf{X}_{jk} + \mathbf{G}_{jk}(\boldsymbol{\tau}_{jk}))\boldsymbol{\beta}_{jk}^{(t+1)}
  &=& \mathbf{X}_{jk}^\top \mathbf{u}_k^{(t)} + \mathbf{X}_{jk}^\top\mathbf{W}_{kk}\boldsymbol{\eta}_{k}^{(t)} -
    \mathbf{X}_{jk}^\top\mathbf{W}_{kk}\boldsymbol{\eta}_{k, -j}^{(t+1)}
\end{eqnarray*}
\begin{eqnarray*}
  \boldsymbol{\beta}_{jk}^{(t+1)}
  &=& (\mathbf{X}_{jk}^\top\mathbf{W}_{kk}\mathbf{X}_{jk} + \mathbf{G}_{jk}(\boldsymbol{\tau}_{jk}))^{-1}(\mathbf{X}_{jk}^\top \mathbf{u}_k^{(t)} +
    \mathbf{X}_{jk}^\top\mathbf{W}_{kk}\boldsymbol{\eta}_{k}^{(t)} -
    \mathbf{X}_{jk}^\top\mathbf{W}_{kk}\boldsymbol{\eta}_{k, -j}^{(t+1)})
\end{eqnarray*}
\begin{eqnarray*}
  \boldsymbol{\beta}_{jk}^{(t+1)}
  &=& (\mathbf{X}_{jk}^\top\mathbf{W}_{kk}\mathbf{X}_{jk} + \mathbf{G}_{jk}(\boldsymbol{\tau}_{jk}))^{-1}\mathbf{X}_{jk}^\top(\mathbf{u}_k^{(t)} +
    \mathbf{W}_{kk}\boldsymbol{\eta}_{k}^{(t)} -
    \mathbf{W}_{kk}\boldsymbol{\eta}_{k, -j}^{(t+1)})
\end{eqnarray*}
\begin{eqnarray*}
  \boldsymbol{\beta}_{jk}^{(t+1)}
  &=& (\mathbf{X}_{jk}^\top\mathbf{W}_{kk}\mathbf{X}_{jk} + \mathbf{G}_{jk}(\boldsymbol{\tau}_{jk}))^{-1}\mathbf{X}_{jk}^\top(
    \mathbf{W}_{kk}\mathbf{W}_{kk}^{-1}\mathbf{u}_k^{(t)} +
    \mathbf{W}_{kk}\boldsymbol{\eta}_{k}^{(t)} -
    \mathbf{W}_{kk}\boldsymbol{\eta}_{k, -j}^{(t+1)})
\end{eqnarray*}
\begin{eqnarray*}
  \boldsymbol{\beta}_{jk}^{(t+1)}
  &=& (\mathbf{X}_{jk}^\top\mathbf{W}_{kk}\mathbf{X}_{jk} + \mathbf{G}_{jk}(\boldsymbol{\tau}_{jk}))^{-1}\mathbf{X}_{jk}^\top\mathbf{W}_{kk}(
    \mathbf{W}_{kk}^{-1}\mathbf{u}_k^{(t)} +
    \boldsymbol{\eta}_{k}^{(t)} -
    \boldsymbol{\eta}_{k, -j}^{(t+1)})
\end{eqnarray*}
This yields the updating function $U_{jk}( \cdot )$ shown in (16) in the main manuscript.

\section{Approximate full conditionals for MCMC} \label{appendix:fullcond1}
The following shows the steps to derive a multivariate normal jumping distribution based
on a second order Taylor series expansion of the log-posterior centered at the last state of
$\boldsymbol{\beta}_{jk}$.
\begin{eqnarray*}
  \pi(\boldsymbol{\beta}_{jk}^\star | \cdot) &\propto& \exp\left[
    \log\,\pi\left(\boldsymbol{\beta}_{jk}^{(t)} | \cdot\right) +
    \left(\boldsymbol{\beta}_{jk}^\star - \boldsymbol{\beta}_{jk}^{(t)}\right)^\top \mathbf{s}\left(\boldsymbol{\beta}_{jk}^{(t)}\right) + \right.\\
  && \left. \qquad\qquad \frac{1}{2}\left(\boldsymbol{\beta}_{jk}^\star - \boldsymbol{\beta}_{jk}^{(t)}\right)^\top
    \mathbf{H}_{kk}\left(\boldsymbol{\beta}_{jk}^{(t)}\right)
    \left(\boldsymbol{\beta}_{jk}^\star - \boldsymbol{\beta}_{jk}^{(t)}\right)\right] \\
  &\propto& \exp\left[(\boldsymbol{\beta}_{jk}^\star)^\top \mathbf{s}\left(\boldsymbol{\beta}_{jk}^{(t)}\right) +
    \left(\frac{1}{2}(\boldsymbol{\beta}_{jk}^\star)^\top\mathbf{H}_{kk}\left(\boldsymbol{\beta}_{jk}^{(t)}\right) - \right.\right. \\
  && \qquad\qquad\left.\left. \frac{1}{2}(\boldsymbol{\beta}_{jk}^{(t)})^\top\mathbf{H}_{kk}\left(\boldsymbol{\beta}_{jk}^{(t)}\right) \right)\left(\boldsymbol{\beta}_{jk}^\star - \boldsymbol{\beta}_{jk}^{(t)}\right)\right] \\
  &\propto& \exp\left[(\boldsymbol{\beta}_{jk}^\star)^\top \mathbf{s}\left(\boldsymbol{\beta}_{jk}^{(t)}\right) +
    \frac{1}{2}(\boldsymbol{\beta}_{jk}^\star)^\top\mathbf{H}_{kk}\left(\boldsymbol{\beta}_{jk}^{(t)}\right)\boldsymbol{\beta}_{jk}^\star - \right. \\
  && \left. \qquad\qquad\, \frac{1}{2}(\boldsymbol{\beta}_{jk}^\star)^\top\mathbf{H}_{kk}\left(\boldsymbol{\beta}_{jk}^{(t)}\right)\boldsymbol{\beta}_{jk}^{(t)} -
    \frac{1}{2}(\boldsymbol{\beta}_{jk}^{(t)})^\top\mathbf{H}_{kk}\left(\boldsymbol{\beta}_{jk}^{(t)}\right)\boldsymbol{\beta}_{jk}^\star \right] \\
  &=& \exp\left[
    \frac{1}{2}(\boldsymbol{\beta}_{jk}^\star)^\top\mathbf{H}_{kk}\left(\boldsymbol{\beta}_{jk}^{(t)}\right)\boldsymbol{\beta}_{jk}^\star +
    (\boldsymbol{\beta}_{jk}^\star)^\top \mathbf{s}\left(\boldsymbol{\beta}_{jk}^{(t)}\right) -
    (\boldsymbol{\beta}_{jk}^\star)^\top\mathbf{H}_{kk}\left(\boldsymbol{\beta}_{jk}^{(t)}\right)\boldsymbol{\beta}_{jk}^{(t)} \right] \\
  &=& \exp\left[
    -\frac{1}{2}(\boldsymbol{\beta}_{jk}^\star)^\top-\mathbf{H}_{kk}\left(\boldsymbol{\beta}_{jk}^{(t)}\right)\boldsymbol{\beta}_{jk}^\star +
    (\boldsymbol{\beta}_{jk}^\star)^\top \left(\mathbf{s}\left(\boldsymbol{\beta}_{jk}^{(t)}\right) - \mathbf{H}_{kk}\left(\boldsymbol{\beta}_{jk}^{(t)}\right)\boldsymbol{\beta}_{jk}^{(t)}\right) \right]
\end{eqnarray*}
Which leads to the proposal density
$q(\boldsymbol{\beta}_{jk}^\star | \, \boldsymbol{\beta}_{jk}^{(t)}) =
  \mathcal{N}(\boldsymbol{\mu}_{jk}^{(t)}, \boldsymbol{\Sigma}_{jk}^{(t)})$ with precision matrix
$$
\left(\boldsymbol{\Sigma}_{jk}^{(t)}\right)^{-1} = -\mathbf{H}_{kk}\left(\boldsymbol{\beta}_{jk}^{(t)}\right)
$$
and mean
\begin{eqnarray*}
\boldsymbol{\mu}_{jk}^{(t)} &=& \boldsymbol{\Sigma}_{jk}^{(t)}\left[
  s\left(\boldsymbol{\beta}_{jk}^{(t)}\right) -
  \mathbf{H}_{kk}\left(\boldsymbol{\beta}_{jk}^{(t)}\right)\boldsymbol{\beta}_{jk}^{(t)} \right] \\
&=& \boldsymbol{\beta}_{jk}^{(t)} -
  \mathbf{H}_{kk}\left(\boldsymbol{\beta}_{jk}^{(t)}\right)^{-1}
  \mathbf{s}\left(\boldsymbol{\beta}_{jk}^{(t)}\right) \\
 &=& \boldsymbol{\beta}_{jk}^{(t)} -
  \left[\mathbf{J}_{kk}\left( \boldsymbol{\beta}_{jk}^{(t)} \right) +
    \mathbf{G}_{jk}(\boldsymbol{\tau}_{jk}))\right]^{-1}\mathbf{s}\left( \boldsymbol{\beta}_{jk}^{(t)} \right).
\end{eqnarray*}
Using a basis function representation of functions $f_{jk}( \cdot )$ the precision matrix is
$$
\left(\boldsymbol{\Sigma}_{jk}^{(t)}\right)^{-1} = \mathbf{X}_{jk}^\top\mathbf{W}_{kk}\mathbf{X}_{jk} + \mathbf{G}_{jk}(\boldsymbol{\tau}_{jk}),
$$
with weights $\mathbf{W}_{kk} = -\mathrm{diag}(\partial^2 \ell(\boldsymbol{\beta}; \mathbf{y}, \mathbf{X}) /
\partial \boldsymbol{\eta}_k \partial \boldsymbol{\eta}_k^\top)$ and the mean can be written as
\begin{eqnarray*}
  \boldsymbol{\mu}_{jk}^{(t)} &=& \boldsymbol{\Sigma}_{jk}^{(t)}\left[
  \mathbf{X}_{jk}^\top\mathbf{u}_k^{(t)} - \mathbf{G}_{jk}(\boldsymbol{\tau}_{jk})\boldsymbol{\beta}_{jk}^{(t)} +
  (\mathbf{X}_{jk}^\top\mathbf{W}_{kk}\mathbf{X}_{jk} + \mathbf{G}_{jk}(\boldsymbol{\tau}_{jk}))\boldsymbol{\beta}_{jk}^{(t)}\right] \\
  &=& \boldsymbol{\Sigma}_{jk}^{(t)}\left[
  \mathbf{X}_{jk}^\top\mathbf{u}_k^{(t)} +
  \mathbf{X}_{jk}^\top\mathbf{W}_{kk}\mathbf{X}_{jk}\boldsymbol{\beta}_{jk}^{(t)}\right] \\
  &=& \boldsymbol{\Sigma}_{jk}^{(t)}\left[
  \mathbf{X}_{jk}^\top\mathbf{u}_k^{(t)} +
  \mathbf{X}_{jk}^\top\mathbf{W}_{kk}\left(\boldsymbol{\eta}_k^{(t)} - \boldsymbol{\eta}^{(t)}_{k,-j}\right)\right] \\
  &=& \boldsymbol{\Sigma}_{jk}^{(t)}\mathbf{X}_{jk}^\top\left[
  \mathbf{u}_k^{(t)} + \mathbf{W}_{kk}\left(\boldsymbol{\eta}_k^{(t)} - \boldsymbol{\eta}^{(t)}_{k,-j}\right)\right] \\
  &=& \boldsymbol{\Sigma}_{jk}^{(t)}\mathbf{X}_{jk}^\top\mathbf{W}_{kk}\left[
  \boldsymbol{\eta}_k^{(t)} + \mathbf{W}_{kk}^{-1}\mathbf{u}_k^{(t)}  - \boldsymbol{\eta}^{(t)}_{k,-j}\right] \\
  &=& \boldsymbol{\Sigma}_{jk}^{(t)}\mathbf{X}_{jk}^\top\mathbf{W}_{kk}\left[\mathbf{z}_k  - \boldsymbol{\eta}^{(t)}_{k,-j}\right]
\end{eqnarray*}
with working observations
$\mathbf{z}_k = \boldsymbol{\eta}_k^{(t)} + \mathbf{W}_{kk}^{-1}\mathbf{u}_k^{(t)}$.


\bibliographystyle{chicago}

\bibliography{bamlss}


\end{document}

