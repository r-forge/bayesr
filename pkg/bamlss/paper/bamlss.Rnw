\documentclass[nojss]{jss}
% \documentclass[article]{jss}
\usepackage{amsmath,amssymb,amsfonts,thumbpdf}
\usepackage{multirow,longtable}

\definecolor{darkgray}{rgb}{0.1,0.1,0.1}
\definecolor{heat1}{rgb}{0.8274510, 0.2470588, 0.4156863}
\definecolor{heat2}{rgb}{0.8823529, 0.4392157, 0.2980392}
\definecolor{heat3}{rgb}{0.9137255, 0.6039216, 0.1725490}
\definecolor{heat4}{rgb}{0.9098039, 0.7647059, 0.2352941}
\definecolor{heat5}{rgb}{0.8862745, 0.9019608, 0.7411765}
\definecolor{blue1}{RGB}{0, 126, 255}


%% additional commands
\newcommand{\squote}[1]{`{#1}'}
\newcommand{\dquote}[1]{``{#1}''}
\newcommand{\fct}[1]{{\texttt{#1()}\index{#1@\texttt{#1()}}}}
\newcommand{\class}[1]{\dquote{\texttt{#1}}}
%% for internal use
\newcommand{\fixme}[1]{\emph{\marginpar{FIXME} (#1)}}
\newcommand{\readme}[1]{\emph{\marginpar{README} (#1)}}

%% Authors: NU + rest in alphabetical order
\author{Nikolaus Umlauf\\Universit\"at Innsbruck \And
        Achim Zeileis\\Universit\"at Innsbruck}
\Plainauthor{Nikolaus Umlauf, Achim Zeileis}

\title{BAMLSS: Bayesian additive models for location, scale and shape (and beyond)}
\Plaintitle{BAMLSS: Bayesian additive models for location, scale and shape (and beyond)}

\Keywords{GAMLSS, distributional regression, MCMC, \proglang{BUGS}, \proglang{R}, software}
\Plainkeywords{GAMLSS, distributional regression, MCMC, BUGS, R, software}

\Abstract{
  Bayesian analysis provides a convenient setting for the estimation of complex generalized
  additive regression models (GAM). Since computational power has tremendously increased in the past
  decade it is now possible to tackle complicated inferential problems, e.g., with Markov chain
  Monte Carlo simulation, on virtually any modern computer. This is one of the reasons why
  Bayesian methods have become quite popular and it has lead to a number of highly specialized and
  optimized estimation engines. Because of the very general structure of the additive predictor in
  GAMs, we propose an unified modeling architecture that can deal with a wide range of types of
  model terms and can benefit from different algorithms in order to estimate Bayesian additive
  models for location, scale and shape (and beyond).
}

\Address{
  Nikolaus Umlauf, Achim Zeileis\\
  Department of Statistics\\
  Faculty of Economics and Statistics\\
  Universit\"at Innsbruck\\
  Universit\"atsstr.~15\\
  6020 Innsbruck, Austria\\
  E-mail: \email{Nikolaus.Umlauf@uibk.ac.at}, \email{Achim.Zeileis@R-project.org}\\
  URL: \url{http://eeecon.uibk.ac.at/~umlauf/},\\
  \phantom{URL: }\url{http://eeecon.uibk.ac.at/~zeileis/}
}

%% Sweave/vignette information and metadata
%% need no \usepackage{Sweave}
\SweaveOpts{engine = R, eps = FALSE, keep.source = TRUE}

<<preliminaries, echo=FALSE, results=hide>>=
options(width = 70, prompt = "R> ", continue = "+  ")
set.seed(1090)
library("bamlss")
library("maptools")
data("LondonFire")
@


\begin{document}


\section{Introduction} \label{sec:intro}

The generalized additive model for location, scale and shape
(GAMLSS,~\citealp{bamlss:Rigby+Stasinopoulos:2005}) relaxes the distributional assumptions of
an response variable in a way that allows for modeling the mean (location) as well
as higher moments (scale and shape) in terms of covariates. This is
especially useful in cases where, e.g., the response does not follow the exponential family or
particular interest lies on scale and shape parameters. Moreover, covariate effects can have
arbitrary forms such as, e.g., linear, nonlinear, spatial or random effects. Hence, each parameter
of the distribution is linked to an additive predictor in similar fashion as for the well
established generalized additive model (GAM,~\citealp{bamlss:Hastie+Tibshirani:1990}).

The terms of an additive predictor can be represented by an unified basis function approach, which
supports a general model architecture. This fact can be further exploited because each term can be
transformed into a mixed model representation \citep{bamlss:Ruppert+Wand+Carrol:2003, bamlss:Wand:2003}, 
independent on the assumptions about smoothness controlled by a penalty on the regression
coefficients. In a Bayesian setting, equivalent smooth forms and generality is achieved using, e.g.,
normal priors on the regression coefficients
\citep{bamlss:Fahrmeir+Kneib+Lang+Marx:2013, bamlss:Brezger+Lang:2006}.

The Bayesian approach is particularly attractive since it provides valid inference that does not
rely on asymptotic properties and allows extensions such as variable selection or multilevel models.
Probably for this reason, and because computational power has tremendously increased in the past
decade, the number of Bayesian estimation engines that can tackle complicated inferential problems
has seen a constant rise. As a whole, existing estimation engines already provide infrastructures
for a number of regression problems exceeding univariate responses, e.g., for multinomial,
multivariate normal or mixed discrete-continuous distributed variables, and so forth. In addition,
most of the engines support random effect estimation which in the end can in principle be utilized
for setting up complex models with additive predictors (see, e.g., \citealp{bamlss:Wood:2006}).

However, the majority of engines (Bayesian and frequentist) use different model setups and output 
functionalities, which makes it difficult for practitioners, e.g., to compare properties of
different algorithms or to select the appropriate distribution and variables, etc. The reasons are
manifold: the use of different model specification languages like
\proglang{BUGS}~\citep{bamlss:BUGS:2009} or \proglang{R}~\citep{bamlss:R}; different standalone
statistical software packages like \pkg{BayesX}~\citep{bamlss:Umlauf+Adler+Kneib+Lang+Zeileis:2014,
bamlss:Belitz+Brezger+Kneib+Lang:2011}, \pkg{JAGS}~\citep{bamlss:Plummer:2013},
\pkg{Stan}~\citep{bamlss:stan-software:2013} or
\pkg{WinBUGS}~\citep{bamlss:Lunn+Thomas+Best+Spiegelhalter:2000}; or even differences within the
same environment.

In order to ease the usage of already existing implementations and code, as well as to facilitate
the development of new algorithms and extensions, we present an unified and entirely modular
architecture for models with additive predictors which does not restrict to any type of regression
problem. Hence, the approach supports more than the GAMLSS statistical model class and is sometimes
referred to as distributional regression. However, because of the great similarities with GAMLSS we
call the conceptional framework Bayesian additive models for location, scale and shape (BAMLSS).

The remainder of the paper is as follows. In Section~\ref{sec:models} the models supported by this
framework are briefly introduced. Section~\ref{sec:arch} then presents the general modeling
architecture and model specification problem. In Section~\ref{sec:softex} a software implementation
of the concept is presented briefly together with a couple of examples.


\section{Model structure and overview} \label{sec:models}

Supposing data of $i = 1, \ldots, n$ observations is available, the models discussed in this paper
assume conditional independence of individual response observations given covariates.
Within the GAMLSS model class all parameters of the response distribution can be modeled by
explanatory variables such that
\begin{equation} \label{eqn:dreg}
y \sim \mathbf{\mathcal{D}}\left(h_{1}(\theta_{1}) = \eta_{1}, \,\,
  h_{2}(\theta_{2}) = \eta_{2}, \dots, \,\, h_{K}(\theta_{K}) =
  \eta_{K}\right),
\end{equation}
where $\mathbf{\mathcal{D}}$ denotes any distribution available for the response
variable $y$ and $\theta_k$, $k = 1, \ldots, K$, are parameters that are linked to additive predictors
using known monotonic and twice differentiable functions
$h_{k}(\cdot)$ \citep{bamlss:Rigby+Stasinopoulos:2005}. Note that the response may also be a
q-dimensional vector $\mathbf{y} = (y_{1}, \ldots, y_{q})^\top$, e.g., when
$\mathbf{\mathcal{D}}$ is a multivariate normal density
(see, e.g., \citealp{bamlss:Klein+Kneib+Klasen+Lang:2014}).
The $k$-th additive predictor is given by
\begin{equation} \label{eqn:structadd}
\eta_k = \eta_k(\mathbf{x}; \boldsymbol{\beta}_k) =
  f_{1k}(\mathbf{x}; \boldsymbol{\beta}_{1k}) + \ldots + f_{J_kk}(\mathbf{x}; \boldsymbol{\beta}_{J_kk}),
\end{equation}
with unspecified (possibly nonlinear) functions $f_{jk}(\cdot)$ of a generic covariate vector
$\mathbf{x}$, $j = 1, \ldots, J_k$ and $k = 1, \ldots, K$. In the following, we only assume that
each function $f_{jk}(\cdot)$ is a composition of covariate data $\mathbf{x}$ and regression
coefficients $\boldsymbol{\beta}_{jk}$, hence, $f_{jk}(\cdot)$ are not necessarily linear
functions or functions that have a linear representation, only. More specifically, a simple linear
effect of one continuous covariate could be given by
$f_{jk}(\mathbf{x}, \boldsymbol{\beta}_{jk}) = x_{1}\beta$, whereas a nonlinear effect
$f_{jk}(\mathbf{x}, \boldsymbol{\beta}_{jk}) = f(x_{1})$
could, e.g., be modeled by regression splines or a technique that is nonlinear in the
parameters like growth curves, amongst others. More examples that are covered by this framework are:
Spatially correlated effects of some discrete location index, varying coefficients, spatially
varying effects, random intercepts and slopes, etc. Also note that each predictor in
(\ref{eqn:dreg}) may contain a different set of covariate effects 
supporting very complex structures with arbitrary combinations. This general framework is also
known as structured additive regression
(STAR,~\citealp{bamlss:Fahrmeir+Kneib+Lang:2004, bamlss:Brezger+Lang:2006}) and covers a number of
well known model classes that have been developed for the exponential family of distributions as
special cases, e.g., generalized additive models (GAM, \citealp{bamlss:Hastie+Tibshirani:1990}),
generalized additive mixed models (GAMM, \citealp{bamlss:Lin+Zhang:1999}), geoadditive models
\citep{bamlss:Kamman+Wand:2003}, varying coefficient models \citep{bamlss:Hastie+Tibshirani:1993},
and geographically weighted regression \citep{bamlss:Fotheringham+Brunsdon+Charlton:2002}.


\section{A conceptional Lego toolbox} \label{sec:legobox}

\subsection{Terms and priors} \label{sec:termprior}

In the following, we assume that for each function $f_{jk}(\cdot)$ the vector of function evaluations
$\mathbf{f}_{jk} = (f_{jk}(\mathbf{x}_{1}),\ldots,f_{jk}(\mathbf{x}_{n}))^{\top}$ of the
$i = 1, \ldots, n$ observations is a composition of
\begin{equation} \label{eqn:functions}
\mathbf{f}_{jk} = f_{jk}(\mathbf{X}_{jk}, \boldsymbol{\beta}_{jk}),
\end{equation}
where $\mathbf{X}_{jk}$ ($n \times m_{jk}$) is a design matrix and the structure of $\mathbf{X}_{jk}$
only depends on the type of covariate(s) and prior assumptions. The vector $\boldsymbol{\beta}_{jk}$
($q_{jk} \times 1$) are regression coefficients that need to be estimated. Hence, the predictor
(\ref{eqn:structadd}) may be written as $\boldsymbol{\eta}_k = \mathbf{f}_{1k} + \ldots + \mathbf{f}_{J_kk}$.
The computation of the vector $\mathbf{f}_{jk}$ is in most cases a matrix product of the design matrix
and the coefficients, however as already noted, the framework presented here allows for arbitrary
functional types that may not have this structure, e.g., nonlinear growth curve estimation of covariates.
In the frequentist setting, to ensure regularization, e.g., for penalizing too abrupt jumps using a
P(enalised)-spline representation of $f_{jk}(\cdot)$ \citep{bamlss:Eilers+Marx:1996}, it is common to add a
penalty $\text{pen}(\mathbf{f}_{jk}) = \text{pen}(\boldsymbol{\beta}_{jk})$ to the regression problem.
Within the Bayesian formulation, the equivalent is to put prior distributions $p(\cdot)$ on
the regression coefficients $\boldsymbol{\beta}_{jk}$. The following outlines frequently used terms
and corresponding priors for parameters within STAR predictors (\ref{eqn:structadd}).

\subsubsection{Linear effects}

Linear or parametric effects can be written as a simple matrix product
$\mathbf{f}_{jk} = \mathbf{X}_{jk}\boldsymbol{\beta}_{jk}$. A common choice of $p(\boldsymbol{\beta}_{jk})$
is to use a non-informative uniform prior
\begin{equation} \label{eqn:uniprior}
p(\boldsymbol{\beta}_{jk}) \propto 1.
\end{equation}
One of the simplest
informative priors is a normal prior given by
\begin{equation} \label{eqn:linprior}
p(\boldsymbol{\beta}_{jk}) \propto \exp \left(- \frac{1}{2}
  (\boldsymbol{\beta}_{jk} - \mathbf{m}_{jk})^{\top}\mathbf{M}_{jk}^{-1}(\boldsymbol{\beta}_{jk} -
  \mathbf{m}_{jk})\right),
\end{equation}
with prior mean $\mathbf{m}_{jk}$ and prior covariance matrix $\mathbf{M}_{jk}$. In a lot of applications
a vague prior specification is used with $\mathbf{m}_{jk} = \mathbf{0}$ and a large variance.

\subsubsection{Nonlinear effects}

Although the functional forms may be rather complex within predictor (\ref{eqn:structadd}),
sometimes including more than one covariate, it is again possible to obtain a linear representation
$\mathbf{f}_{jk} = \mathbf{X}_{jk}\boldsymbol{\beta}_{jk}$ using a basis function approach. Here, the columns
of the design matrix $\mathbf{X}_{jk}$ hold the so called basis functions which are predetermined by
the type of the function chosen. More specifically, for functions of a single covariate commonly
used basis functions are B-splines or thin plate splines, which are also capable to estimate higher
dimensional functions. Moreover, higher dimensional functions can always be obtained by tensor
product basis construction of marginal basis of an arbitrary number of covariates. A detailed
overview of smooth functions constructed from various basis functions is provided in
\citet{bamlss:Fahrmeir+Kneib+Lang+Marx:2013} and \citet{bamlss:Wood:2006}.

The elegance of the approach is that regularization of the possibly very complex functions is
accomplished by placing a generic multivariate normal prior
\begin{equation} \label{eqn:shrinkprior}
p(\boldsymbol{\beta}_{jk}) \propto \left( \frac{1}{\tau_{jk}^2} \right)^{rk(\mathbf{K}_{jk}) / 2} \exp \left(- \frac{1}{2\tau_{jk}^2}
\boldsymbol{\beta}_{jk}^{\top}\mathbf{K}_{jk}\boldsymbol{\beta}_{jk}\right)
\end{equation}
on the regression coefficients $\boldsymbol{\beta}_{jk}$, where the precision matrix $\mathbf{K}_{jk}$
corresponds to the frequentist's penalty matrix and depends on the type of function
associated with the term. The variance parameter $\tau_{jk}^2$ is equivalent to the inverse smoothing
parameter in a frequentist approach and controls the trade off between flexibility and smoothness.
A common choice of prior for the variance parameter is a weakly informative inverse Gamma hyperprior
\begin{equation} \label{eqn:ig}
p(\tau_{jk}^2) = \frac{b_{jk}^{a_{jk}}}{\Gamma(a_{jk})} (\tau_{jk}^2)^{-(a_{jk} + 1)} \exp(-b_{jk} / \tau_{jk}^2).
\end{equation}
with $a_{jk} = b_{jk} = 0.001$ as a standard option. Small values for $a_{jk}$ and $b_{jk}$ correspond to an 
approximate uniform distribution for $\log(\tau_{jk}^2)$. Note that within this notion a ridge penalty
on linear effects can be obtained by $\mathbf{K}_{jk} = \mathbf{I}$, similarly independent and
identically distributed (i.i.d.) random effects can be incorporated using prior (\ref{eqn:shrinkprior}).

In addition, the individual model components $\mathbf{X}_{jk}\boldsymbol{\beta}_{jk}$ for any basis
function specification can be further decomposed into a mixed model representation given by
\begin{equation*} \label{eqn:mixed}
\mathbf{f}_{jk} = \tilde{\mathbf{X}}_{jk}\tilde{\boldsymbol{\gamma}}_{jk} +
  \mathbf{U}_{jk}\tilde{\boldsymbol{\beta}}_{jk},
\end{equation*}
where $\tilde{\boldsymbol{\gamma}}_{jk}$ represents the fixed effects parameters and 
$\tilde{\boldsymbol{\beta}}_{jk} \sim N(\mathbf{0}, \tau^2_{jk}\mathbf{I})$ i.i.d.\ random effects.
The design matrix $\mathbf{U}_{jk}$ is derived from a spectral decomposition of the penalty matrix
$\mathbf{K}_{jk}$ and $\tilde{\mathbf{X}}_{jk}$ by finding a basis of the null space of $\mathbf{K}_{jk}$
such that $\tilde{\mathbf{X}}_{jk}^{\top}\mathbf{K}_{jk} = \mathbf{0}$, i.e., parameters
$\tilde{\boldsymbol{\gamma}}_{jk}$ are not penalized (see, e.g.,
\citealp{bamlss:Fahrmeir+Kneib+Lang+Marx:2013, bamlss:Ruppert+Wand+Carrol:2003, bamlss:Wand:2003}).

\subsubsection{Further effects}

Within the scope of the presented framework we allow for functions of covariates that cannot
necessarily be written by a matrix product $\mathbf{X}_{jk}\boldsymbol{\beta}_{jk}$. As an
example, nonlinear growth curve estimation with the Gompertz function requires the evaluation of
$$
\mathbf{f}_{jk} = \beta_{1} \cdot \exp \left( -\exp\left( \beta_{2} +
  \mathbf{X}_{jk}\beta_{3} \right) \right)
$$
and particular interest may lie on the parameters describing the growth rate, lag phase, etc.
As a standard option one can put non-informative uniform priors (\ref{eqn:uniprior}) or normal
priors (\ref{eqn:linprior}) on $\boldsymbol{\beta}$. To generalize this, the framework allows for
any functions $f_{jk}(\mathbf{X}_{jk}, \boldsymbol{\beta}_{jk})$ and prior(s) $p(\boldsymbol{\beta}_{jk})$
that are available for the covariate(s).

%The representation of the terms in the structured additive predictor (\ref{eqn:structadd})
%already suggests a general and modular architecture. However, to keep the focus on maximum
%flexibility of the conceptional framework it is useful to list the typical steps needed to estimate
%the models presented in Section~\ref{sec:models}:
%\begin{enumerate}
%\item Choosing an appropriate distribution for the response.
%\item Specification of the model terms the parameters are modeled by.
%\item Setting up the corresponding design and penalty matrices.
%\item Starting the estimation engine.
%\item Processing the results for printing summaries, plotting, etc. 
%\end{enumerate}
%Step 2 thereby requires some type of generic model formula syntax to actually specify the
%dependencies of the parameters on covariates. Choosing an appropriate distribution in step 1 implies
%that the estimation engine used in step 4 includes the corresponding implementation. In addition,
%to compute, e.g., goodness of fit plots using quantile residuals \citep{bamlss:Dunn+Gordon:1996},
%the distribution specification oftentimes needs information beyond the log-likelihood function.
%Moreover, the individual steps should be modular, e.g., changing the estimation engine does not
%require additional adjustments on subsequent infrastructures. The following describes the
%conceptional building blocks in more detail.

\subsection{Response distribution} \label{sec:density}

%Any statistical software for regression models needs some description system for the supported
%distributions. Since most estimation algorithms have at least one common part, a modular system
%with reusable elements arises naturally from the following characterizations.

The main building block of regression model algorithms is the probability density function
$f(\mathbf{y} | \boldsymbol{\theta}_1, \ldots, \boldsymbol{\theta}_K)$, or for
computational reasons its logarithm.
Note that $f$ is considered to be a general density and $\boldsymbol{\theta}_k$, $k = 1, \ldots, K$,
are parameters that are linked to STAR predictors given in equation (\ref{eqn:structadd}).
Estimation typically requires to evaluate the log-likelihood function
\begin{equation} \label{eqn:density}
\ell(\boldsymbol{\beta} ; \mathbf{y}, \mathbf{X}) =
  \sum_{i = 1}^n \log \, f(y_i ; \theta_{i1} = h_1^{-1}(\eta_{i1}(\mathbf{x}_i, \boldsymbol{\beta}_1)), \ldots,
  \theta_{iK} = h_K^{-1}(\eta_{iK}(\mathbf{x}_i, \boldsymbol{\beta}_K)))
\end{equation}
a number of times, where the vector
$\boldsymbol{\beta} = (\boldsymbol{\beta}_1^\top, \ldots, \boldsymbol{\beta}_K^\top)^\top$ 
comprises all model coefficients that should be estimated, $\mathbf{X} = (\mathbf{X}_1, \ldots, \mathbf{X}_K)$
are the respective covariate matrices with $\mathbf{x}_i$ as the $i$-th row of $\mathbf{X}$ and
$\boldsymbol{\theta}_k$ are vectors of length $n$.
Assigning prior distributions to the individual model components, e.g., given by the normal
prior (\ref{eqn:shrinkprior}) and the inverse gamma prior (\ref{eqn:ig}), results in the log-posterior
\begin{equation} \label{eqn:logpost}
\log \, p(\boldsymbol{\vartheta} ; \mathbf{y}, \mathbf{X}) \propto
  \ell(\boldsymbol{\beta} ; \mathbf{y}, \mathbf{X}) +
  \sum_{k = 1}^K\sum_{j = 1}^{J_k} \left\{ \log \, p_{jk}(\boldsymbol{\vartheta}_{jk}) \right\},
\end{equation}
where, e.g., $\boldsymbol{\vartheta}_{jk} = (\boldsymbol{\beta}_{jk}^\top, (\boldsymbol{\tau}^2_{jk})^\top)^\top$
is a vector of all parameters associated to the $jk$-th term and includes all variances, too, and
prior $p_{jk}(\cdot)$ denotes the combination of all assigned priors on parameters
$\boldsymbol{\vartheta}_{jk}$. Also note that from a frequentist perspective (\ref{eqn:logpost}) can
be viewed as a penalized log-likelihood using prior (\ref{eqn:shrinkprior}) for fixed variance
parameters $\boldsymbol{\tau}_{jk}^2$.


\subsection{Model fitters} \label{sec:modelfit}

Bayesian point estimates of $\boldsymbol{\vartheta}$ are obtained by posterior mode, mean or median
estimation. While posterior mode estimation requires maximization of the log-posterior
(\ref{eqn:logpost}), which has an unique (sometimes analytical) solution in a number of cases,
posterior mean and median estimation involves solving (possibly) high-dimensional integrals, usually
requiring computer intensive techniques such as Markov chain Monte Carlo (MCMC) simulation. The
following describes the quantities needed for generic iterative algorithms for estimating Bayesian
distributional regression models.

\subsubsection{Posterior mode} \label{sec:postmode}

The mode of the posterior distribution is the mode of the log-posterior (\ref{eqn:logpost}) given by
\begin{equation}
\text{Mod}(\boldsymbol{\vartheta} ; \mathbf{y}, \mathbf{X}) =
  \underset{\boldsymbol{\vartheta}}{\text{arg max }} \log \, p(\boldsymbol{\vartheta} ; \mathbf{y}. \mathbf{X})
\end{equation}
and equals the maximum likelihood estimator
\begin{equation}
\text{ML}(\boldsymbol{\vartheta} ; \mathbf{y}, \mathbf{X}) =
  \underset{\boldsymbol{\vartheta}}{\text{arg max }} \ell(\boldsymbol{\beta} ; \mathbf{y}, \mathbf{X})
\end{equation}
assigning uniform priors (\ref{eqn:uniprior})
for $j = 1, \ldots, J_k$, $k = 1, \ldots, K$.
For models involving shrinkage priors, e.g., given by (\ref{eqn:shrinkprior}), the posterior mode is
equivalent to a penalized maximum likelihood estimator for fixed variance parameters
$\boldsymbol{\tau}_{jk}^2$ and $p(\boldsymbol{\tau}_{jk}^2) \propto 1$. Moreover, the structure of
(\ref{eqn:logpost}) prohibits simultaneous estimation of
$\boldsymbol{\vartheta}_{jk} = (\boldsymbol{\beta}_{jk}^\top, (\boldsymbol{\tau}^2_{jk})^\top)^\top$ and the
estimator $\hat{\boldsymbol{\tau}}^2_{jk}$ is usually derived by additionally
minimizing an information criterion such as the Akaike information criterion (AIC) or the Bayesian
information criterion (BIC) (see also \citealp{bamlss:Rigby+Stasinopoulos:2005} Appendix~A.2. for
a more detailed discussion on variance/hyperparameter estimation). In the following we describe
posterior mode estimation for the case of fixed and known parameters $\boldsymbol{\tau}_{jk}^2$,
i.e., $\boldsymbol{\vartheta}_{jk} = \boldsymbol{\beta}_{jk}$ for
$j = 1, \ldots, J_k$ and $k = 1, \ldots, K$.
Estimation of $\boldsymbol{\beta} = (\boldsymbol{\beta}_1^\top, \ldots, \boldsymbol{\beta}_K^\top)^\top$
requires solving equations
$\partial (\log \, p(\boldsymbol{\vartheta} | \mathbf{y})) / \partial \boldsymbol{\beta} = \mathbf{0}$
and usually the problem is solved with an iterative updating scheme of the form
\begin{equation} \label{eqn:updating}
\boldsymbol{\beta}^{(t + 1)} = U(\boldsymbol{\beta}^{(t)}),
\end{equation}
with updating function $U(\cdot)$ and $\hat{\boldsymbol{\beta}}$ as a fixed point of the iteration.
A particularly convenient way to maximize (\ref{eqn:logpost}) is a Newton-Raphson type updating
\begin{equation} \label{eqn:newton}
\boldsymbol{\beta}^{(t + 1)} = U(\boldsymbol{\beta}^{(t)}) = \boldsymbol{\beta}^{(t)} -
  \mathbf{H}\left( \boldsymbol{\beta}^{(t)} \right)^{-1}\mathbf{s}\left( \boldsymbol{\beta}^{(t)} \right)
\end{equation}
with score vector
\begin{equation} \label{eqn:score}
\mathbf{s}(\boldsymbol{\beta}) = 
  \frac{\partial \log \, p(\boldsymbol{\vartheta} ; \mathbf{y}, \mathbf{X})}{\partial \boldsymbol{\beta}}
= \frac{\partial \ell(\boldsymbol{\beta}; \mathbf{y}, \mathbf{X})}{\partial \boldsymbol{\beta}} +
    \sum_{k = 1}^K\sum_{j = 1}^{J_k} \left\{ \frac{\partial \log \, p_{jk}(\boldsymbol{\beta}_{jk})}{\partial \boldsymbol{\beta}} \right\}.
\end{equation}
and hessian matrix $\mathbf{H}(\boldsymbol{\beta})$ with components
\begin{equation} \label{eqn:hessian}
\mathbf{H}_{ks}(\boldsymbol{\beta}) =
\frac{\partial \mathbf{s}(\boldsymbol{\beta}_k)}{\partial \boldsymbol{\beta}_s^\top} =
\frac{\partial^2 \log \, p(\boldsymbol{\vartheta}; \mathbf{y}, \mathbf{X})}{\partial \boldsymbol{\beta}_k \partial \boldsymbol{\beta}_s^\top},
\end{equation}
for $k = 1, \dots, K$ and $s = 1, \dots, K$. By chain rule, the part of the score vector involving
the derivatives of the log-likelihood for the $k$th parameter can be further decomposed to
\begin{equation} \label{eqn:score2}
\frac{\partial \ell(\boldsymbol{\beta}; \mathbf{y}, \mathbf{X})}{\partial \boldsymbol{\beta}_k} =
  \frac{\partial \ell(\boldsymbol{\beta}; \mathbf{y}, \mathbf{X})}{\partial \boldsymbol{\eta}_k}
  \frac{\partial \boldsymbol{\eta}_k}{\partial \boldsymbol{\beta}_k} = 
  \frac{\partial \ell(\boldsymbol{\beta}; \mathbf{y}, \mathbf{X})}{\partial \boldsymbol{\theta}_k}
  \frac{\partial \boldsymbol{\theta}_k}{\partial \boldsymbol{\eta}_k}
  \frac{\partial \boldsymbol{\eta}_k}{\partial \boldsymbol{\beta}_k},
\end{equation}
including the derivatives of the log-likelihood with respect to parameters $\boldsymbol{\theta}_k$,
the derivative of the link functions and the derivative of the STAR predictor
$\boldsymbol{\eta}_k$ with respect to coefficients $\boldsymbol{\beta}_k$. Again by chain rule,
the components of $\mathbf{H}_{ks}$ including $\ell(\boldsymbol{\beta}; \mathbf{y}, \mathbf{X})$ can be
written as
\begin{equation} \label{hessian2}
\frac{\partial^2 \ell(\boldsymbol{\beta}; \mathbf{y}, \mathbf{X})}{\partial \boldsymbol{\beta}_k \partial \boldsymbol{\beta}_s^\top} =
\left( \frac{\partial \boldsymbol{\eta}_s}{\partial \boldsymbol{\beta}_s} \right)^\top
\frac{\partial^2 \ell(\boldsymbol{\beta}; \mathbf{y}, \mathbf{X})}{\partial \boldsymbol{\eta}_k\partial \boldsymbol{\eta}_s^\top}
\frac{\partial \boldsymbol{\eta}_k}{\partial \boldsymbol{\beta}_k}
\,\, \underbrace{
  \, + \, \frac{\partial \ell(\boldsymbol{\beta}; \mathbf{y}, \mathbf{X})}{\partial \boldsymbol{\eta}_k}
    \frac{\partial^2 \boldsymbol{\eta}_k}{\partial^2 \boldsymbol{\beta}_k}}_{\text{if } k = s},
\end{equation}
where the second term drops if all functions (\ref{eqn:functions}) can be written as a linear
combination of a design matrix and coefficients, e.g., when using a basis function approach. Within
the first term, the second derivatives of the log-likelihood involving the predictors can be written
as
\begin{equation} \label{hessian3}
\frac{\partial^2 \ell(\boldsymbol{\beta}; \mathbf{y}, \mathbf{X})}{\partial \boldsymbol{\eta}_k\partial \boldsymbol{\eta}_s^\top} =
  \frac{\partial \ell(\boldsymbol{\beta}; \mathbf{y}, \mathbf{X})}{\partial \boldsymbol{\theta}_k}
  \frac{\partial^2 \boldsymbol{\theta}_k}{\partial \boldsymbol{\eta}_k \partial \boldsymbol{\eta}_s^\top} + 
  \frac{\partial^2 \ell(\boldsymbol{\beta}; \mathbf{y}, \mathbf{X})}{\partial \boldsymbol{\theta}_k \partial \boldsymbol{\theta}_s^\top}
  \frac{\partial \boldsymbol{\theta}_k}{\partial \boldsymbol{\eta}_k}
  \frac{\partial \boldsymbol{\theta}_s}{\partial \boldsymbol{\eta}_s}
\end{equation}
involving the second derivatives of the link functions.

Although solving the updating scheme (\ref{eqn:newton}) is feasible, from the computational
perspective of the model class it is still a bit unhandy. Fortunately, the problem of iteratively
finding the roots for $\boldsymbol{\beta}$ can be partitioned into separate updating equations
using leapfrog or zigzag iteration \citep{bamlss:Smyth:1996}. Now let
\begin{eqnarray} \label{eqn:pupdate}
\boldsymbol{\beta}_1^{(t + 1)} &=& U_1(\boldsymbol{\beta}_1^{(t)}, \boldsymbol{\beta}_2^{(t)},
  \ldots, \boldsymbol{\beta}_K^{(t)}) \nonumber \\
\boldsymbol{\beta}_2^{(t + 1)} &=& U_2(\boldsymbol{\beta}_1^{(t+1)}, \boldsymbol{\beta}_2^{(t)},
  \ldots, \boldsymbol{\beta}_K^{(t)}) \nonumber \\
  &\vdots& \nonumber \\
\boldsymbol{\beta}_K^{(t + 1)} &=& U_K(\boldsymbol{\beta}_1^{(t+1)}, \boldsymbol{\beta}_2^{(t+1)},
  \ldots, \boldsymbol{\beta}_K^{(t)})
\end{eqnarray}
be a partitioned updating scheme with updating functions $U_k(\cdot)$, i.e., in each iteration
one parameter is maximized holding the other parameters fixed. Note that this updating scheme can be
further partitioned for each function within parameter block $k$, leading to a highly modular
system.

For orthogonal parameters $\boldsymbol{\beta}_k$, the updating scheme can be written as
a $k$-partitioned Newton-Raphson iteration
\begin{equation} \label{eqn:blocknewton}
\boldsymbol{\beta}_k^{(t + 1)} = U_k(\boldsymbol{\beta}_k^{(t)} | \cdot) = \boldsymbol{\beta}_k^{(t)} -
  \mathbf{H}_{kk}\left( \boldsymbol{\beta}_k^{(t)} \right)^{-1}\mathbf{s}_k\left( \boldsymbol{\beta}_k^{(t)} \right).
\end{equation}
Assuming a basis function approach for functions (\ref{eqn:functions}) with multivariate normal
priors (\ref{eqn:shrinkprior}), the hessian matrix in (\ref{eqn:blocknewton}) is given by
$$
\mathbf{H}_{kk}\left( \boldsymbol{\beta}_k^{(t)} \right) =
\begin{pmatrix}
\mathbf{X}_{1k}^\top\mathbf{W}_{kk}\mathbf{X}_{1k} + \mathbf{G}_{1k} &
  \cdots & \mathbf{X}_{1k}^\top\mathbf{W}_{kk}\mathbf{X}_{J_kk} \\
\vdots & \ddots & \vdots \\
\mathbf{X}_{J_kk}^\top\mathbf{W}_{kk}\mathbf{X}_{1k} & \cdots & \mathbf{X}_{J_kk}^\top\mathbf{W}_{kk}\mathbf{X}_{J_kk} + \mathbf{G}_{J_kk}
\end{pmatrix}^{(t)},
$$
with diagonal weight matrix $\mathbf{W}_{kk} = -\mathrm{diag}(\partial^2 \ell(\boldsymbol{\beta} | \mathbf{y}) /
\partial \boldsymbol{\eta}_k \partial \boldsymbol{\eta}_k^\top)$ and penalty matrix
$\mathbf{G}_{jk} = 0.5\tau_{jk}^{-2}\mathbf{K}_{jk}$. Similarly, the score vector is
$$
\mathbf{s}_k\left( \boldsymbol{\beta}_k^{(t)} \right) =
\begin{pmatrix}
\mathbf{X}_{1k}^\top \mathbf{u}_k^{(t)} - \mathbf{G}_{1k}\boldsymbol{\beta}_{1k}^{(t)} \\
\vdots \\
\mathbf{X}_{J_kk}^\top \mathbf{u}_k^{(t)} - \mathbf{G}_{J_kk}\boldsymbol{\beta}_{J_kk}^{(t)} \\
\end{pmatrix}
$$
and derivatives $\mathbf{u}_k = \partial \ell(\boldsymbol{\beta} | \mathbf{y}) /
\partial \boldsymbol{\eta}_k$. Focusing on the $j$th row of (\ref{eqn:blocknewton}) gives
\begin{eqnarray*}
(\mathbf{X}_{jk}^\top\mathbf{W}_{kk}\mathbf{X}_{jk} + \mathbf{G}_{jk})\boldsymbol{\beta}_{jk}^{(t+1)} +
\ldots + \mathbf{X}_{jk}^\top\mathbf{W}_{kk}\mathbf{X}_{J_kk}\boldsymbol{\beta}_{J_kk}^{(t + 1)} - \\
(\mathbf{X}_{jk}^\top\mathbf{W}_{kk}\mathbf{X}_{jk} + \mathbf{G}_{jk})\boldsymbol{\beta}_{jk}^{(t)} -
\ldots - \mathbf{X}_{jk}^\top\mathbf{W}_{kk}\mathbf{X}_{J_kk}\boldsymbol{\beta}_{J_kk}^{(t)}
  &=& \mathbf{X}_{jk}^\top \mathbf{u}_k^{(t)} - \mathbf{G}_{jk}\boldsymbol{\beta}_{jk}^{(t)}
\end{eqnarray*}
\begin{eqnarray*}
\mathbf{G}_{jk}(\boldsymbol{\beta}_{jk}^{(t+1)} - \boldsymbol{\beta}_{jk}^{(t)}) +
  \mathbf{X}_{jk}^\top\mathbf{W}_{kk}\mathbf{X}_{jk}\boldsymbol{\beta}_{jk}^{(t+1)} + \ldots +
  \mathbf{X}_{jk}^\top\mathbf{W}_{kk}\mathbf{X}_{J_kk}\boldsymbol{\beta}_{J_kk}^{(t+1)} - \\
  \mathbf{X}_{jk}^\top\mathbf{W}_{kk}\mathbf{X}_{jk}\boldsymbol{\beta}_{jk}^{(t)} - \ldots -
  \mathbf{X}_{jk}^\top\mathbf{W}_{kk}\mathbf{X}_{J_kk}\boldsymbol{\beta}_{J_kk}^{(t)}
  &=& \mathbf{X}_{jk}^\top \mathbf{u}_k^{(t)} - \mathbf{G}_{jk}\boldsymbol{\beta}_{jk}^{(t)}
\end{eqnarray*}
\begin{eqnarray*}
\mathbf{G}_{jk}\boldsymbol{\beta}_{jk}^{(t)} +
  \mathbf{G}_{jk}(\boldsymbol{\beta}_{jk}^{(t+1)} - \boldsymbol{\beta}_{jk}^{(t)}) +
  \mathbf{X}_{jk}^\top\mathbf{W}_{kk}\mathbf{X}_{jk}\boldsymbol{\beta}_{jk}^{(t+1)} + \ldots +
  \mathbf{X}_{jk}^\top\mathbf{W}_{kk}\mathbf{X}_{J_kk}\boldsymbol{\beta}_{J_kk}^{(t+1)} - \\
  \mathbf{X}_{jk}^\top\mathbf{W}_{kk}\mathbf{X}_{jk}\boldsymbol{\beta}_{jk}^{(t)} - \ldots -
  \mathbf{X}_{jk}^\top\mathbf{W}_{kk}\mathbf{X}_{J_kk}\boldsymbol{\beta}_{J_kk}^{(t)}
  &=& \mathbf{X}_{jk}^\top \mathbf{u}_k^{(t)}
\end{eqnarray*}
\begin{eqnarray*}
\mathbf{G}_{jk}\boldsymbol{\beta}_{jk}^{(t+1)} +
  \mathbf{X}_{jk}^\top\mathbf{W}_{kk}\mathbf{X}_{jk}\boldsymbol{\beta}_{jk}^{(t+1)} + \ldots +
  \mathbf{X}_{jk}^\top\mathbf{W}_{kk}\mathbf{X}_{J_kk}\boldsymbol{\beta}_{J_kk}^{(t+1)} - \\
  \mathbf{X}_{jk}^\top\mathbf{W}_{kk}\mathbf{X}_{jk}\boldsymbol{\beta}_{jk}^{(t)} - \ldots -
  \mathbf{X}_{jk}^\top\mathbf{W}_{kk}\mathbf{X}_{J_kk}\boldsymbol{\beta}_{J_kk}^{(t)}
  &=& \mathbf{X}_{jk}^\top \mathbf{u}_k^{(t)}
\end{eqnarray*}
\begin{eqnarray*}
\mathbf{G}_{jk}\boldsymbol{\beta}_{jk}^{(t+1)} +
  \mathbf{X}_{jk}^\top\mathbf{W}_{kk}\mathbf{X}_{jk}\boldsymbol{\beta}_{jk}^{(t+1)} + 
  \mathbf{X}_{jk}^\top\mathbf{W}_{kk}\boldsymbol{\eta}_{k, -j}^{(t+1)} -
  \mathbf{X}_{jk}^\top\mathbf{W}_{kk}\boldsymbol{\eta}_{k}^{(t)}
  &=& \mathbf{X}_{jk}^\top \mathbf{u}_k^{(t)}
\end{eqnarray*}
\begin{eqnarray*}
(\mathbf{X}_{jk}^\top\mathbf{W}_{kk}\mathbf{X}_{jk} + \mathbf{G}_{jk})\boldsymbol{\beta}_{jk}^{(t+1)}
  &=& \mathbf{X}_{jk}^\top \mathbf{u}_k^{(t)} + \mathbf{X}_{jk}^\top\mathbf{W}_{kk}\boldsymbol{\eta}_{k}^{(t)} -
    \mathbf{X}_{jk}^\top\mathbf{W}_{kk}\boldsymbol{\eta}_{k, -j}^{(t+1)}
\end{eqnarray*}
\begin{eqnarray*}
  \boldsymbol{\beta}_{jk}^{(t+1)}
  &=& (\mathbf{X}_{jk}^\top\mathbf{W}_{kk}\mathbf{X}_{jk} + \mathbf{G}_{jk})^{-1}(\mathbf{X}_{jk}^\top \mathbf{u}_k^{(t)} +
    \mathbf{X}_{jk}^\top\mathbf{W}_{kk}\boldsymbol{\eta}_{k}^{(t)} -
    \mathbf{X}_{jk}^\top\mathbf{W}_{kk}\boldsymbol{\eta}_{k, -j}^{(t+1)})
\end{eqnarray*}
\begin{eqnarray*}
  \boldsymbol{\beta}_{jk}^{(t+1)}
  &=& (\mathbf{X}_{jk}^\top\mathbf{W}_{kk}\mathbf{X}_{jk} + \mathbf{G}_{jk})^{-1}\mathbf{X}_{jk}^\top(\mathbf{u}_k^{(t)} +
    \mathbf{W}_{kk}\boldsymbol{\eta}_{k}^{(t)} -
    \mathbf{W}_{kk}\boldsymbol{\eta}_{k, -j}^{(t+1)})
\end{eqnarray*}
\begin{eqnarray*}
  \boldsymbol{\beta}_{jk}^{(t+1)}
  &=& (\mathbf{X}_{jk}^\top\mathbf{W}_{kk}\mathbf{X}_{jk} + \mathbf{G}_{jk})^{-1}\mathbf{X}_{jk}^\top(
    \mathbf{W}_{kk}\mathbf{W}_{kk}^{-1}\mathbf{u}_k^{(t)} +
    \mathbf{W}_{kk}\boldsymbol{\eta}_{k}^{(t)} -
    \mathbf{W}_{kk}\boldsymbol{\eta}_{k, -j}^{(t+1)})
\end{eqnarray*}
\begin{eqnarray*}
  \boldsymbol{\beta}_{jk}^{(t+1)}
  &=& (\mathbf{X}_{jk}^\top\mathbf{W}_{kk}\mathbf{X}_{jk} + \mathbf{G}_{jk})^{-1}\mathbf{X}_{jk}^\top\mathbf{W}_{kk}(
    \mathbf{W}_{kk}^{-1}\mathbf{u}_k^{(t)} +
    \boldsymbol{\eta}_{k}^{(t)} -
    \boldsymbol{\eta}_{k, -j}^{(t+1)})
\end{eqnarray*}
\begin{eqnarray} \label{eqn:blockbackfit}
  \boldsymbol{\beta}_{jk}^{(t+1)}
  &=& (\mathbf{X}_{jk}^\top\mathbf{W}_{kk}\mathbf{X}_{jk} + \mathbf{G}_{jk})^{-1}\mathbf{X}_{jk}^\top\mathbf{W}_{kk}(
    \mathbf{z}_k - \boldsymbol{\eta}_{k, -j}^{(t+1)})
\end{eqnarray}
with working observations
$\mathbf{z}_k = \boldsymbol{\eta}_{k}^{(t)} + \mathbf{W}_{kk}^{-1}\mathbf{u}_k^{(t)}$.
Hence, this leads to a backfitting algorithm and cycling through (\ref{eqn:blockbackfit}) for
terms $j = 1, \ldots, J_k$ and parameters $k = 1, \ldots, K$ is equivalent to a single
Newton-Raphson step in (\ref{eqn:newton}). Note that this is a slightly simplified version of the
backfitting algorithm developed in \citet{bamlss:Rigby+Stasinopoulos:2005} Appendix~C.2.
The updating scheme (\ref{eqn:blockbackfit}) can be further generalized to
$$
\mathbf{f}_{jk}^{(t)} = \mathbf{S}_{jk}\left(\mathbf{z}_k - \boldsymbol{\eta}_{k, -j}^{(t+1)}\right)
$$
i.e., theoretically any smoother function $\mathbf{S}_{jk}( \cdot )$ applied on the
``partial residuals'' can be used.

Note that for numerical reasons it is oftentimes better to replace the hessian by the expected
Fisher information with weights $\mathbf{W}_{kk} = -\mathrm{diag}(E(\partial^2 \ell(\boldsymbol{\beta} | \mathbf{y}) /
\partial \boldsymbol{\eta}_k \partial \boldsymbol{\eta}_k^\top))$.

\newpage

\subsubsection{Posterior mean} \label{sec:postmean}

The mean of the posterior distribution is
\begin{equation} \label{eqn:postmean}
E(\boldsymbol{\vartheta}; \mathbf{y}, \mathbf{X}) =
  \int \boldsymbol{\vartheta} p(\boldsymbol{\vartheta}; \mathbf{y}, \mathbf{X})d\boldsymbol{\vartheta}.
\end{equation}
Clearly, the problem in deriving the expectation relies on the computation of the usually
high-dimensional integral, which can be rarely solved analytically and needs to be approximated
by numerical techniques. 

MCMC simulation is commonly used in such situations as it provides an extendable framework that can 
adapt to almost any type of problem. Moreover, computational power has increases tremendously in the
past decade and even computer intensive algorithms are feasible on simple desktop computers today.
In the following we summarize three sampling techniques that are especially suited for this
modeling framework, i.e., techniques that can be used for a highly modular and extendable system.
Similarly to updating scheme (\ref{eqn:pupdate}) we consider samples that are drawn for blocks of
parameters, more specifically, the sampling schemes successively update the parameters of a single
function (\ref{eqn:functions}). Note that for some models there exist full conditionals that can
be derived in closed form from the log-posterior (\ref{eqn:logpost}), however, we especially focus
on situations were this is not generally the case.
MCMC samples for the regression coefficients $\boldsymbol{\beta}_{jk}$ can be derived by each of the
following methods:
\begin{itemize}
\item \emph{Random-walk Metropolis}: \label{sec:rwm} \\
  Probably the most important algorithm, because of its generality and easy implementation, is
  random-walk Metropolis. The sampler
  proceeds by drawing a candidate $\boldsymbol{\beta}_{jk}^{\star}$ from a symmetric jumping
  distribution $q(\boldsymbol{\beta}_{jk}^{\star}| \boldsymbol{\beta}_{jk}^{(t)})$, the
  candidate is then accepted as the new state of the Markov chain with probability
  $$
  \alpha\left( \boldsymbol{\beta}_{jk}^{\star} | \boldsymbol{\beta}_{jk}^{(t)}\right) =
  \text{min} \left\{ \frac{p(\boldsymbol{\beta}_{jk}^{\star} | \cdot)}{
    p(\boldsymbol{\beta}_{jk}^{(t)} | \cdot)}, 1 \right\}
  $$
  with the log-posterior $p(\boldsymbol{\beta}_{jk} | \cdot)$ evaluated at the proposed and
  current value.
  Commonly, the jumping distribution is a normal distribution $N(\boldsymbol{\beta}_{jk}^{(t)}, \boldsymbol{\Sigma}_{jk})$ centered at the current iterate and fixed covariance matrix. Although this algorithm
  is theoretically working for any distribution, the actual sampling performance depends heavily on
  starting values and the scaling of $\boldsymbol{\Sigma}_{jk}$. Therefore, numerous methods that
  try to optimize the behavior of the Markov chain in an adaptive phase (burnin phase) have been
  developed. In the seminal paper of \citet{bamlss:Gelman+Roberts+Gilks:1996}, strategies that
  optimize the acceptance rate to roughly $1/4$ are suggested to obtain a good mixing (see also
  \citealp{bamlss:Gareth+Roberts+Jeffrey+Rosenthal:2009}). Similarly,
  within the presented modeling framework and a basis function approach with multivariate normal
  prior (\ref{eqn:shrinkprior}), a convenient way is to set
  $\boldsymbol{\Sigma}_{jk} = \sigma_{jk}\mathbf{K}_{jk}^{-1}$ and optimize $\sigma_{jk}$ to the
  desired properties in the adaptive phase.

\item \emph{Derivative based Metropolis-Hastings}: \label{sec:dmh} \\
  A commonly used alternative for the covariance matrix of the jumping distribution
  $N(\boldsymbol{\beta}_{jk}^{(t)}, \boldsymbol{\Sigma}_{jk})$ is to use the local curvature
  information
  $$
  \boldsymbol{\Sigma}_{jk} = -\left( \frac{\partial^2 p(\boldsymbol{\vartheta}; \mathbf{y}, \mathbf{X})}{
    \partial \boldsymbol{\beta}_{jk}\boldsymbol{\beta}_{jk}^\top} \right)^{-1}
  $$
  computed at the posterior mode estimate $\hat{\boldsymbol{\beta}}_{jk}$. However, fixing
  $\boldsymbol{\Sigma}_{jk}$ during MCMC simulation might still lead to undesired behavior of the
  Markov chain especially when iterates move into regions with low probability mass of the
  posterior distribution. A solution is to construct full conditionals
  $p(\boldsymbol{\beta}_{jk} | \cdot)$
  which approximate the posterior at the current iterate and minimize the risk of slow traversing.
  The construction of the full conditional is based on a second order Taylor series expansion
  of the log-posterior centered at the last state
  \begin{eqnarray*}
  p(\boldsymbol{\beta}_{jk}^\star | \cdot) &\propto& \exp\left\{
    ln\,p\left(\boldsymbol{\beta}_{jk}^{(t)} | \cdot\right) +
    \left(\boldsymbol{\beta}_{jk}^\star - \boldsymbol{\beta}_{jk}^{(t)}\right)^\top s\left(\boldsymbol{\beta}_{jk}^{(t)}\right) + \right.\\
  && \left. \frac{1}{2}\left(\boldsymbol{\beta}_{jk}^\star - \boldsymbol{\beta}_{jk}^{(t)}\right)^\top
    \mathbf{H}_{kk}\left(\boldsymbol{\beta}_{jk}^{(t)}\right)
    \left(\boldsymbol{\beta}_{jk}^\star - \boldsymbol{\beta}_{jk}^{(t)}\right)\right\} \\
  &\propto& \exp\left\{(\boldsymbol{\beta}_{jk}^\star)^\top s\left(\boldsymbol{\beta}_{jk}^{(t)}\right) +
    \left(\frac{1}{2}(\boldsymbol{\beta}_{jk}^\star)^\top\mathbf{H}_{kk}\left(\boldsymbol{\beta}_{jk}^{(t)}\right) - \right.\right. \\
  && \qquad\qquad\left.\left. \frac{1}{2}(\boldsymbol{\beta}_{jk}^{(t)})^\top\mathbf{H}_{kk}\left(\boldsymbol{\beta}_{jk}^{(t)}\right) \right)\left(\boldsymbol{\beta}_{jk}^\star - \boldsymbol{\beta}_{jk}^{(t)}\right)\right\} \\
  &\propto& \exp\left\{(\boldsymbol{\beta}_{jk}^\star)^\top s\left(\boldsymbol{\beta}_{jk}^{(t)}\right) +
    \frac{1}{2}(\boldsymbol{\beta}_{jk}^\star)^\top\mathbf{H}_{kk}\left(\boldsymbol{\beta}_{jk}^{(t)}\right)\boldsymbol{\beta}_{jk}^\star - \right. \\
  && \left. \qquad\qquad \frac{1}{2}(\boldsymbol{\beta}_{jk}^\star)^\top\mathbf{H}_{kk}\left(\boldsymbol{\beta}_{jk}^{(t)}\right)\boldsymbol{\beta}_{jk}^{(t)} -
    \frac{1}{2}(\boldsymbol{\beta}_{jk}^{(t)})^\top\mathbf{H}_{kk}\left(\boldsymbol{\beta}_{jk}^{(t)}\right)\boldsymbol{\beta}_{jk}^\star \right\} \\
  &=& \exp\left\{
    \frac{1}{2}(\boldsymbol{\beta}_{jk}^\star)^\top\mathbf{H}_{kk}\left(\boldsymbol{\beta}_{jk}^{(t)}\right)\boldsymbol{\beta}_{jk}^\star +
    (\boldsymbol{\beta}_{jk}^\star)^\top s\left(\boldsymbol{\beta}_{jk}^{(t)}\right) -
    (\boldsymbol{\beta}_{jk}^\star)^\top\mathbf{H}_{kk}\left(\boldsymbol{\beta}_{jk}^{(t)}\right)\boldsymbol{\beta}_{jk}^{(t)} \right\} \\
  &=& \exp\left\{
    -\frac{1}{2}(\boldsymbol{\beta}_{jk}^\star)^\top-\mathbf{H}_{kk}\left(\boldsymbol{\beta}_{jk}^{(t)}\right)\boldsymbol{\beta}_{jk}^\star +
    (\boldsymbol{\beta}_{jk}^\star)^\top \left(s\left(\boldsymbol{\beta}_{jk}^{(t)}\right) - \mathbf{H}_{kk}\left(\boldsymbol{\beta}_{jk}^{(t)}\right)\boldsymbol{\beta}_{jk}^{(t)}\right) \right\}
  \end{eqnarray*}
  which is proportional to a multivariate normal distribution with precision matrix
  $$
  \left(\boldsymbol{\Sigma}_{jk}^{(t)}\right)^{-1} = -\mathbf{H}_{kk}\left(\boldsymbol{\beta}_{jk}^{(t)}\right)
  $$
  and mean
  \begin{eqnarray*}
  \boldsymbol{\mu}_{jk}^{(t)} &=& \boldsymbol{\Sigma}_{jk}^{(t)}\left\{
    s\left(\boldsymbol{\beta}_{jk}^{(t)}\right) -
    \mathbf{H}_{kk}\left(\boldsymbol{\beta}_{jk}^{(t)}\right)\boldsymbol{\beta}_{jk}^{(t)} \right\} \\
  &=& \boldsymbol{\beta}_{jk}^{(t)} -
    \mathbf{H}_{kk}\left(\boldsymbol{\beta}_{jk}^{(t)}\right)^{-1}
    s\left(\boldsymbol{\beta}_{jk}^{(t)}\right).
  \end{eqnarray*}
  Note that the mean is simply one Newton or Fisher scoring iteration towards the posterior mode.
  Hence, the proposal density for $\boldsymbol{\beta}_{jk}$ is
  $q(\boldsymbol{\beta}_{jk}^\star | \boldsymbol{\beta}_{jk}^{(t)}) =
    N(\boldsymbol{\mu}_{jk}^{(t)}, \boldsymbol{\Sigma}_{jk}^{(t)})$ and the acceptance probability
  of the candidate is then computed by
  $$
  \alpha\left( \boldsymbol{\beta}_{jk}^{\star} | \boldsymbol{\beta}_{jk}^{(t)}\right) =
  \text{min} \left\{ \frac{p(\boldsymbol{\beta}_{jk}^{\star} | \cdot)q(\boldsymbol{\beta}_{jk}^{(t)} | \boldsymbol{\beta}_{jk}^\star)}{
    p(\boldsymbol{\beta}_{jk}^{(t)} | \cdot)q(\boldsymbol{\beta}_{jk}^\star | \boldsymbol{\beta}_{jk}^{(t)})  }, 1 \right\}.
  $$
  Again, assuming a basis
  function approach with multivariate normal priors (\ref{eqn:shrinkprior}) the precision matrix is
  $$
  \left(\boldsymbol{\Sigma}_{jk}^{(t)}\right)^{-1} = \mathbf{X}_{jk}^\top\mathbf{W}_{kk}\mathbf{X}_{jk} + \mathbf{G}_{jk},
  $$
  with weights $\mathbf{W}_{kk} = -\mathrm{diag}(\partial^2 \ell(\boldsymbol{\beta}; \mathbf{y}, \mathbf{X}) /
\partial \boldsymbol{\eta}_k \partial \boldsymbol{\eta}_k^\top)$ and the mean can be written as
  \begin{eqnarray*}
  \boldsymbol{\mu}_{jk}^{(t)} &=& \boldsymbol{\Sigma}_{jk}^{(t)}\left\{
  \mathbf{X}_{jk}^\top\mathbf{u}_k^{(t)} - \mathbf{G}_{jk}\boldsymbol{\beta}_{jk}^{(t)} +
  (\mathbf{X}_{jk}^\top\mathbf{W}_{kk}\mathbf{X}_{jk} + \mathbf{G}_{jk})\boldsymbol{\beta}_{jk}^{(t)}\right\} \\
  &=& \boldsymbol{\Sigma}_{jk}^{(t)}\left\{
  \mathbf{X}_{jk}^\top\mathbf{u}_k^{(t)} +
  \mathbf{X}_{jk}^\top\mathbf{W}_{kk}\mathbf{X}_{jk}\boldsymbol{\beta}_{jk}^{(t)}\right\} \\
  &=& \boldsymbol{\Sigma}_{jk}^{(t)}\left\{
  \mathbf{X}_{jk}^\top\mathbf{u}_k^{(t)} +
  \mathbf{X}_{jk}^\top\mathbf{W}_{kk}\left(\boldsymbol{\eta}_k^{(t)} - \boldsymbol{\eta}^{(t)}_{k,-j}\right)\right\} \\
  &=& \boldsymbol{\Sigma}_{jk}^{(t)}\mathbf{X}_{jk}^\top\left\{
  \mathbf{u}_k^{(t)} + \mathbf{W}_{kk}\left(\boldsymbol{\eta}_k^{(t)} - \boldsymbol{\eta}^{(t)}_{k,-j}\right)\right\} \\
  &=& \boldsymbol{\Sigma}_{jk}^{(t)}\mathbf{X}_{jk}^\top\mathbf{W}_{kk}\left\{
  \boldsymbol{\eta}_k^{(t)} + \mathbf{W}_{kk}^{-1}\mathbf{u}_k^{(t)}  - \boldsymbol{\eta}^{(t)}_{k,-j}\right\} \\
  &=& \boldsymbol{\Sigma}_{jk}^{(t)}\mathbf{X}_{jk}^\top\mathbf{W}_{kk}\left\{\mathbf{z}_k  - \boldsymbol{\eta}^{(t)}_{k,-j}\right\}
  \end{eqnarray*}
  with working observations
  $\mathbf{z}_k = \boldsymbol{\eta}_k^{(t)} + \mathbf{W}_{kk}^{-1}\mathbf{u}_k^{(t)}$. Therefore,
  the computation of the mean is equivalent to a full Newton step, or Fisher scoring when using
  $-E(\partial^2 \ell(\boldsymbol{\beta}; \mathbf{y}, \mathbf{X}) /
    \partial \boldsymbol{\eta}_k \partial \boldsymbol{\eta}_k^\top)$,
  in each iteration of the MCMC sampler using iteratively reweighted least squares (IWLS)
  \citep{bamlss:Gamerman:1997, bamlss:Brezger+Lang:2006, bamlss:Klein+Kneib+Klasen+Lang:2014}.

\item \emph{Slice sampling}: \label{sec:smcmc} \\
  Slice sampling \citep{bamlss:Neal:2003} is a gradient free MCMC sampling scheme that produces
  samples with $100\%$ acceptance rate. Therefore, and because of the simplicity of the algorithm,
  slice sampling is especially useful for automated general purpose MCMC implementations that allow
  for sampling from many distributions. The basic slice
  sampling algorithm samples univariate directly under the plot of the log-posterior
  (\ref{eqn:logpost}). Updates for the $i$-th parameter with $\boldsymbol{\vartheta}_{jk}$ are
  generated by:
  \begin{enumerate}
  \item Sample $h \sim U(0, p(\vartheta_{ijk}^{(t)} | \cdot))$.
  \item Sample $\vartheta_{ijk}^{(t+1)} \sim U(S)$ from the horizontal slice
    $S = \{\vartheta_{ijk}: h < p(\vartheta_{ijk} | \cdot)\}$.
  \end{enumerate}
\end{itemize}

In similar fashion the full conditionals $p(\tau_{jk} | \cdot)$ for smoothing variances using
priors (\ref{eqn:shrinkprior}) and (\ref{eqn:ig}) are again an inverse gamma distribution
with
$$
\tilde{a}_{jk} = \frac{1}{2}rk(\mathbf{K}_{jk}) + a_{jk}, \qquad
  \tilde{b}_{jk} = \frac{1}{2}(\boldsymbol{\beta}_{jk}^\star)^\top\mathbf{K}_{jk}
  \boldsymbol{\beta}_{jk}^\star + b_{jk}.
$$


\subsection{Inference and prediction} \label{sec:infpred}

\subsection{Lego bricks} \label{sec:leobricks}

From the above, it can be recognized that the following quantities are repeatedly used within
BAMLSS candidate algorithms:
\begin{itemize}
\item The density function $f(\mathbf{y} | \boldsymbol{\theta}_1, \ldots, \boldsymbol{\theta}_K)$.
\item The first order derivatives
  $\partial \ell(\boldsymbol{\vartheta} | \mathbf{y}) / \partial \boldsymbol{\theta}_k$,
  $\partial \boldsymbol{\theta}_k / \partial \boldsymbol{\eta}_k$ and
  $\partial \boldsymbol{\eta}_k / \partial \boldsymbol{\vartheta}_k$.
\item Second order derivatives
  $\partial^2 \ell(\boldsymbol{\vartheta} | \mathbf{y}) / \partial \boldsymbol{\eta}_k \partial \boldsymbol{\eta}_k^\top$.
\item Derivatives for priors, e.g., $\log \, p(\boldsymbol{\beta}_{jk} | \tau_{jk}^2)$ and $\log \, p(\tau_{jk}^2)$.
\end{itemize}
Hence, a modular family specification system can in principle be used to implement various
estimation algorithms. A simple generic algorithm for BAMLSS models is outlined by the 
following pseudo code:
\begin{center}
\begin{minipage}[c]{13cm}
\code{while(eps >} $\varepsilon\,$ \code{\& i < maxit) \{ } \\
\hspace*{0.5cm} \code{for(k in 1:K) \{} \\
\hspace*{1cm} \code{for(j in 1:p) \{} \\
\hspace*{1.5cm} Compute $\boldsymbol{\eta}^{\texttt{[k]}}_{\texttt{-j}} = \boldsymbol{\eta}^{\texttt{[k]}} - \mathbf{f}_{\texttt{j}}^{\texttt{[k]}}$. \\
\hspace*{1.5cm} Obtain new $(\boldsymbol{\beta}^{\texttt{[k]}}_{\texttt{j}}, {\tau^2}^{\texttt{[k]}}_{\texttt{j}})^\top = \texttt{u}_{\texttt{j}}^{\texttt{[k]}}(\mathbf{y}, \boldsymbol{\eta}^{\texttt{[k]}}_{\texttt{-j}},
  \mathbf{x}^{\texttt{[k]}}_{\texttt{j}}, \boldsymbol{\beta}^{\texttt{[k]}}_{\texttt{j}}, {\tau^2}^{\texttt{[k]}}_{\texttt{j}}, \texttt{family}, \texttt{k})$. \\
\hspace*{1.5cm} Update $\boldsymbol{\eta}^{\texttt{[k]}}$. \\
\hspace*{1cm} \code{\}} \\
\hspace*{0.5cm} \code{\}} \\
\hspace*{0.5cm} Compute new \code{eps} \\
\code{\}}
\end{minipage}
\end{center}
The algorithm does not distinguish between the frequentist or Bayesian approach, because the 
functions $\texttt{u}_{\texttt{j}}^{\texttt{[k]}}( \cdot )$ could either return proposals from
a MCMC sampler or updates from an optimizing algorithm like the IWLS. Therefore, $\varepsilon$ 
(e.g., $0.0001$) and \code{eps} represent the stopping mechanism in an optimizer while
\code{maxit} controls the maximum iterations of a MCMC sampler, too. To achieve this flexibility a
\code{family} object that contains all distribution specific information to compute the new
parameters
$(\boldsymbol{\beta}^{\texttt{[k]}}_{\texttt{j}}, {\tau^2}^{\texttt{[k]}}_{\texttt{j}})^\top$ is
required, e.g., containing the log-likelihood function, the first and second order derivatives,
etc., as described in the above.

%In practice, only few implementations support an entirely modular setup that can be
%extended by the user. Examples that do support some flexibility are the \proglang{R}
%model fitting functions \fct{glm}, \fct{gam} as well as function \fct{gamlss} of package \pkg{gamlss}
%\citep{bamlss:Stasinopoulos+Rigby:2014}.


\section{Computational tools and strategies for implementation} \label{sec:comptools}

\subsection{Symbolic descriptions} \label{sec:symdesc}

Based on \citet{bamlss:Wilkinson+Rogers:1973} symbolic descriptions for specifying models have been
implemented for various computer programs. The statistical environment \proglang{R} provides such
a syntax (see also \citealp{bamlss:Chambers+Hastie:1992}), which is familiar to almost any common
\proglang{R} user today. Without such specifications, that in the end translate model formulae into
model frames, the estimation of regression models is very circumstantial, especially in the case of
structured additive predictors (\ref{eqn:structadd}). Therefore, the \proglang{R} model formula
language is also extensible. The recommended package \pkg{mgcv}~\citep{bamlss:Wood:2014} for
estimating GAMs additionally provides the generic descriptor \code{s()} for smooth terms. However,
to conveniently specify the models presented in Section~\ref{sec:models}, a slightly enhanced syntax
is required.

Hereinafter, we follow the notation of the \proglang{R} formula language and denote
smooth and random effect terms with the \code{s()} descriptor. A typical linear regression model 
with a response variable \code{y} and covariates \code{x1} and \code{x2} is then represented by
\begin{center}
\code{y} $\sim$ \code{x1 + x2}
\end{center}
A model with two additional nonlinear modeled terms of covariates \code{z1}, \code{z2} and \code{z3}
is set up with
\begin{center}
\begin{tabular}{l}
\code{y} $\sim$ \code{x1 + x2 + s(z1) + s(z2, z3)}
\end{tabular}
\end{center}
However, in the context of distributional regression we need formula extensions for multiple
parameters. A convenient way to specify, e.g., the parameters of a normal model with
$y~\sim~N(\mu = \eta_{\mu}, log(\sigma) = \eta_{\sigma})$ is given by
\begin{center}
{ \renewcommand{\arraystretch}{1}
\begin{tabular}{l}
\code{list(} \\
$\quad$ \code{y} $\sim$ \code{x1 + x2 + s(z1) + s(z2),} \\
$\quad$ \code{sigma} $\sim$ \code{x1 + x2 + s(z1)} \\
\code{)}
\end{tabular}
}
\end{center}
i.e., two formulas are provided where the first represents the description of the mean $\mu$
and the second of the scale parameter $\sigma$. Furthermore, the two formulas
are symbolically connected by a list of formulas that is send to the subsequent processor. This way
any number of parameters can be easily specified, e.g., a four parameter example is
\begin{center}
{ \renewcommand{\arraystretch}{1}
\begin{tabular}{l}
\code{list(} \\
$\quad$ \code{y} $\sim$ \code{x1 + x2 + s(z1) + s(z2),} \\
$\quad$ \code{sigma2} $\sim$ \code{x1 + x2 + s(z1),} \\
$\quad$ \code{nu} $\sim$ \code{s(z1),} \\
$\quad$ \code{tau} $\sim$ \code{s(z2)} \\
\code{)}
\end{tabular}
}
\end{center}
A convention we make at this point is that
the mean formula is always the one including the response variable and all other formulas
have the corresponding parameter name on the left hand side. Hence, a mapping of terms with 
parameters is provided.

Within this syntax it is also possible to incorporate multilevel models with STAR predictor
\citep{bamlss:Lang+Umlauf+Wechselberger+Harttgen+Kneib:2014}, where a hierarchy of
units or clusters grouped at different levels is given. Suppose there is data on three levels
available, where variable \code{id1} denotes the indicator from the individual observations to the
second level with lower resolution and \code{id2} is another indicator mapping from the second to
third level. A four parameter model with 3 levels can be specified with
\begin{center}
{ \renewcommand{\arraystretch}{1}
\begin{tabular}{l}
\code{list(} \\
$\quad$ \code{y} $\sim$ \code{x1 + x2 + s(z1) + s({\color{heat1}{id1}}),} \\
$\quad$ {\color{heat1}{\code{id1} $\sim$ \code{x3 + s(z3) + s(}}}\code{\color{blue1}{id2}}\code{\color{heat1})}\code{,} \\
$\quad$ {\color{blue1}{\code{id2} $\sim$ \code{s(z4)}}}\code{,} \\
$\quad$ \code{sigma2} $\sim$ \code{x1 + x2 + s(z1),} \\
$\quad$ \code{nu} $\sim$ \code{s(z1) + s({\color{heat1}{id1}}),} \\
$\quad$ \code{tau} $\sim$ \code{s(z2)} \\
\code{)}
\end{tabular}
}
\end{center}
Note that the mean and \code{nu} parameter include the indicator variable \code{id1}, therefore,
the level two and three formulas are incorporated in both specifications.

In addition, models with categorical responses can be formulated in a similar fashion. A model
with three categories within the \code{y} variable, e.g., a multinomial model with some
reference category can be defined by
\begin{center}
{ \renewcommand{\arraystretch}{1}
\begin{tabular}{l}
\code{list(} \\
$\quad$ \code{y} $\sim$ \code{x1 + s(z1) + s(z2),} \\
$\quad$ $\sim$ \code{x1 + x2 + s(z1) + s(z3)} \\
\code{)}
\end{tabular}
}
\end{center}
where all subsequent formulas do not need a left hand side. The only additional assumption here is
that the order of the formulas represents the order of the categories. Another option is to specify
the formulas of each category explicitly by
\begin{center}
{ \renewcommand{\arraystretch}{1}
\begin{tabular}{l}
\code{list(} \\
$\quad$ \code{response1} $\sim$ \code{x1 + s(z1) + s(z2),} \\
$\quad$ \code{response2} $\sim$ \code{x1 + x2 + s(z1) + s(z3)} \\
\code{)}
\end{tabular}
}
\end{center}
i.e., variable \code{response1} is a dummy variable indicating whether the $i$th observation is in
category 1 and \code{response2} in category 2, respectively.

In summary, the described model definition syntax does not restrict to any type of regression
model, number of parameters and hierarchies.

\subsection{Building blocks} \label{sec:blocks}

The architecture of the conceptional framework is illustrated in Figure~\ref{fig:blocks}.
\begin{figure}[ht!]
\centering
\setlength{\unitlength}{1cm}
\setlength{\fboxsep}{0pt}
\begin{picture}(10.53, 5.7)(0, 0)
\put(0, 5){\fcolorbox{black}{heat5}{\framebox(2, 0.7)[c]{\footnotesize Formula}}}
\put(2.5, 5){\fcolorbox{black}{heat5}{\framebox(2, 0.7)[c]{\footnotesize Family}}}
\put(5, 5){\fcolorbox{black}{heat5}{\framebox(2, 0.7)[c]{\footnotesize Data}}}
\put(2.5, 3.5){\fcolorbox{black}{heat4}{\framebox(2, 0.7)[c]{\footnotesize Parser}}}
\put(2.5, 2.5){\fcolorbox{black}{heat3}{\framebox(2, 0.7)[c]{\footnotesize Transformer}}}
\put(6, 3.5){\fcolorbox{black}{heat2}{\framebox(2, 0.7)[c]{\footnotesize Setup}}}
\put(6, 2.5){\fcolorbox{black}{heat1}{\framebox(2, 0.7)[c]{\footnotesize Engine}}}
\put(6, 1.5){\fcolorbox{black}{heat2}{\framebox(2, 0.7)[c]{\footnotesize Results}}}
\put(1, 0){\fcolorbox{black}{heat5}{\framebox(2, 0.7)[c]{\footnotesize Summaries}}}
\put(3.5, 0){\fcolorbox{black}{heat5}{\framebox(2, 0.7)[c]{\footnotesize Plotting}}}
\put(6, 0){\fcolorbox{black}{heat5}{\framebox(2, 0.7)[c]{\footnotesize Selection}}}
\put(8.5, 0){\fcolorbox{black}{heat5}{\framebox(2, 0.7)[c]{\footnotesize Prediction}}}
\put(3.5, 4.2){\line(0, 1){0.8}}
\put(1, 5){\line(0, -1){0.4}}
\put(6, 5){\line(0, -1){0.4}}
\put(1, 4.605){\line(1, 0){5}}
\put(3.5, 3.2){\line(0, 1){0.3}}
\put(4.51, 2.85){\line(1, 0){0.8}}
\put(5.3, 2.85){\line(0, 1){1}}
\put(5.3, 3.85){\line(1, 0){0.72}}
\put(2, 0.7){\line(0, 1){0.4}}
\put(4.5, 0.7){\line(0, 1){0.4}}
\put(7, 0.7){\line(0, 1){0.8}}
\put(9.5, 0.7){\line(0, 1){0.4}}
\put(2, 1.098){\line(1, 0){7.5}}
\put(7, 2.2){\line(0, 1){0.3}}
\put(7, 3.2){\line(0, 1){0.3}}
\end{picture}
\caption{\label{fig:blocks} Conceptional overview.}
\end{figure}
The framework can be divided in three parts: First, functions that describe distribution families,
formulas, together with the data that is used for modeling. Secondly, functions that actually
compute estimates of parameters, and thirdly, functions for visualization and output statistics.

To set up the necessary model frame, the parser function translates the model formula
(see Section~\ref{sec:symdesc}) and the data. For some algorithms a modified version of the
model frame is needed, e.g., using the mixed model representation (\ref{eqn:mixed}) for smooth
terms. Hence, a transformer function might be required. For using standalone estimation engines
additional specifications could be required, e.g., the model code when using \proglang{BUGS} 
implementations. This task is handled by the setup function. The actual estimation function then
calls the engine, either an external device or code within the same environment. The data obtained
from the engine is then uniformly processed in a results function to be able to plot estimates, 
create summary statistics, and so forth.

The main advantage of the architecture is that the blocks parser, transformer, setup, engine and 
results are entirely exchangeable and reusable, i.e., it is relatively easy to incorporate new
algorithms or engines. The other blocks are assumed to be more or less stable such that no extra
coding is required. An implementation of this concept together with examples is illustrated in the
next section.


\section{Software implementation and examples} \label{sec:softex}


\subsection{Precepitation climatolgy from daily observations} \label{sec:censreg}

<<echo=FALSE, results=hide>>=
if(!file.exists("rainmodel.rda")) {
  if(file.exists("homstart.rda")) {
    load("homstart.rda")
  } else {
    dpath <- system.file(package = "bamlss", "data")
    if(!file.exists(file <- file.path(dpath, "homstart.rda"))) {
      homstart_data(dir = dirname(file), load = TRUE)
    } else load(file)
    file.copy(file, file.path(getwd(), "homstart.rda"))
  }

  homstart$raw[homstart$raw < 0] <- 0
  homstart2 <- subset(homstart, year > 1979)

  f <- list(
    "mu" = sqrt(raw) ~ elevation + s(day, bs="cc") + s(long, lat, bs="tp", k=30) +
      te(day,long,lat, bs=c("cc","tp"), d=c(1,2), k=c(10,30), mp=FALSE),
    "sigma" = ~ elevation + s(day, bs="cc") + s(long, lat, bs="tp", k=30) +
      te(day,long,lat, bs=c("cc","tp"), d=c(1,2), k=c(10,30), mp=FALSE)
  )

  rainmodel <- bamlss(f, data = homstart2, family = "cnorm",
    n.iter = 4000, burnin = 2000, thin = 10,
    binning = TRUE, before = FALSE, cores = 7)

  save(rainmodel, file = "rainmodel.rda")
}
@

\subsection{Cox-regression} \label{sec:coxreg}

<<echo=FALSE, results=hide>>=
if(!file.exists("firemodel.rda")) {
  f <- list(
    Surv(arrivaltime) ~ ti(arrivaltime,k=20) + ti(arrivaltime,lon,lat,d=c(1,2),mp=FALSE,k=c(5,30)),
    gamma ~ s(fsintens) + s(daytime,bs="cc",k=30) + s(lon,lat,k=60)
  )

  f <- list(
    Surv(arrivaltime) ~ ti(arrivaltime,k=20) + ti(arrivaltime,lon,lat,d=c(1,2),mp=FALSE,k=c(5,30)),
    gamma ~ s(fsintens) + ti(daytime,bs="cc",k=30) + ti(lon,lat,k=80,d=2) +
      ti(daytime,lon,lat,bs=c("cc","cr"),d=c(1,2),mp=FALSE,k=c(10,30))
  )

  firemodel <- bamlss(f, data = LondonFire, family = "cox",
    subdivisions = 10, nu = 0.01, n.iter = 2000, burnin = 1000, thin = 5, cores = 4,
    maxit = 1000)


  plot_daytime <- function(x, daytime = 1, n = 30, ...)
  {
    options(warn = -1)
    library("maptools")
    data("LondonFire")
    gpclibPermit()
    xy <- bbox(LondonBoroughs)
    co <- expand.grid(
      "lon" = seq(min(xy[1, 1]), max(xy[1, 2]), length = n),
      "lat" = seq(min(xy[2, 1]), max(xy[2, 2]), length = n)
    )
    ob <- unionSpatialPolygons(LondonBoroughs,
      rep(1L, length = length(LondonBoroughs)))
    nob <- length(slot(slot(ob, "polygons")[[1]], "Polygons"))
    pip <- NULL
    for(j in 1:nob) {
     oco <- slot(slot(slot(ob, "polygons")[[1]], "Polygons")[[j]], "coords")
     pip <- cbind(pip, point.in.polygon(co$lon, co$lat, oco[, 1L], oco[, 2L], mode.checked = FALSE) < 1L)
    }
    pip <- apply(pip, 1, function(x) all(x))
    co <- co[pip < 1, , drop = FALSE]
    co$daytime <- daytime
    co$spatial_daytime <- predict(x, newdata = co, model = "gamma",
      term = "ti(daytime,lon,lat)", intercept = FALSE)
    plot(LondonBoroughs, xlab = "Longitude [deg]", ylab = "Latitude [deg]",
      main = paste("Daytime =", daytime))
    lr <- range(co$spatial_daytime)
    lr <- c(-1 * max(abs(lr)), max(abs(lr)))
    xr <- quantile(co$spatial_daytime, probs = 0.9)
    xr <- c(-1 * xr, xr)
    xymap(lon, lat, spatial_daytime, data = co, pos = "bottomright",
      layout = FALSE, map = FALSE, color = diverge_hcl, swap = TRUE,
      shift = c(0.03, 0.05), symmetric = TRUE, add = TRUE,
      side.legend = 2, digits = 1, range = xr, lrange = round(lr, 1),
      width = 0.2, height = 0.04, ...)
    plot(LondonBoroughs, add = TRUE)
    box()
    axis(1)
    axis(2)
    options(warn = 0)
    return(invisible(NULL))
  }

  data_basehaz <- function(n = 30, target = 6, k = 20, ...)
  {
    require("spatstat")
    require("sp")
    require("maptools")
    library("raster")
    data("LondonFire")

    gpclibPermit()

    spatial_daytime <- grepl("(daytime,lon,lat)",
      paste(all.labels.formula(terms(firemodel, model = "gamma")), collapse = "+"), fixed = TRUE)

    firemodel$family <- cox.bamlss()

    xy <- bbox(LondonBoroughs)
    co <- expand.grid(
      "lon" = seq(min(xy[1, 1]), max(xy[1, 2]), length = n),
      "lat" = seq(min(xy[2, 1]), max(xy[2, 2]), length = n)
    )
    ob <- unionSpatialPolygons(LondonBoroughs,
      rep(1L, length = length(LondonBoroughs)))
    nob <- length(slot(slot(ob, "polygons")[[1]], "Polygons"))
    pip <- NULL
    for(j in 1:nob) {
     oco <- slot(slot(slot(ob, "polygons")[[1]], "Polygons")[[j]], "coords")
     pip <- cbind(pip, point.in.polygon(co$lon, co$lat, oco[, 1L], oco[, 2L], mode.checked = FALSE) < 1L)
    }
    pip <- apply(pip, 1, function(x) all(x))

    co <- co[pip < 1, , drop = FALSE]
    nd <- NULL
    atime <- with(as.data.frame(LondonFire), seq(min(arrivaltime), max(arrivaltime), length = 100))
    if(spatial_daytime)
      dtime <- with(as.data.frame(LondonFire), seq(min(daytime), max(daytime), length = 100))
    for(i in 1:nrow(co)) {
      td <- if(spatial_daytime) {
        data.frame("arrivaltime" = atime, "daytime" = dtime)
      } else data.frame("arrivaltime" = atime)
      td$lon <- co$lon[i]
      td$lat <- co$lat[i]
      td$id <- i
      nd <- rbind(nd, td)
    }

    nd$id <- as.factor(nd$id)
    nd$p50atime <- predict(firemodel, newdata = nd, model = "lambda", FUN = mean, intercept = FALSE)
    if(spatial_daytime)
      nd$p50dtime <- predict(firemodel, newdata = nd, model = "gamma", term = "daytime", intercept = FALSE)
    nd$atime <- nd$arrivaltime
    if(spatial_daytime)
      nd$dtime <- nd$daytime

    fbh <- fdt <- NULL
    for(i in unique(nd$id)) {
      j <- nd$id == i
      fbh <- cbind(fbh, nd$p50atime[j])
      if(spatial_daytime)
        fdt <- cbind(fdt, nd$p50dtime[j])
    }
    fbh <- cbind(atime, fbh)
    if(spatial_daytime)
      fdt <- cbind(dtime, fdt)

    nd$daytime <- 8.5
    nd$arrivaltime <- target
    nd <- unique(nd[, c("lon", "lat", "arrivaltime", "daytime")])

    co <- bbox(LondonBoroughs)
    co <- owin(co["x", ], co["y", ])
    fsppp <- ppp(x = LondonFStations$lon, LondonFStations$lat, window = co)
    fsintens <- density.ppp(fsppp, bw.diggle)
    fsintens <- raster(fsintens)
    proj4string(fsintens) <- CRS("+init=epsg:4326")
    nd$fsintens <- extract(fsintens, as.matrix(nd[ , c("lon", "lat")]))

    nd$spatial_prob <- predict(firemodel, newdata = nd,
      term = c("(arrivaltime)", "(arrivaltime,lon,lat)", "(fsintens)", "(daytime)", "(lon,lat)"),
      intercept = TRUE, type = "prob", time = target, ...)
    nd$spatial_tc <- predict(firemodel, newdata = nd, model = "gamma",
      term = "(lon,lat)", intercept = FALSE)
    nd$spatial_td <- predict(firemodel, newdata = nd, model = "lambda",
      term = "(arrivaltime,lon,lat)", intercept = FALSE)
    if(spatial_daytime) {
      nd$spatial_daytime <- predict(firemodel, newdata = nd, model = "gamma",
        term = "(daytime,lon,lat)", intercept = FALSE)
    }

    return(list("curves" = fbh, "daytime" = fdt, "spatial" = nd, "target" = target))
  }

  firemodel_plotdata <- data_basehaz(80, 6, subdivisions = 15)

  save(firemodel, firemodel_plotdata, file = "firemodel_plotdata.rda")
}

if(!file.exists("figures"))
  dir.create("figures")

if(!file.exists("figures/firemodel-data.png")) {
  load("firemodel.rda")

  plot.firemodel <- function(data, what = c("curves", "spatial_prob", "spatial_td", "spatial_tc",
    "fsintens", "daytime"),
    main = NULL, spar = TRUE, ...)
  {
    require("maptools")

    if(is.null(main)) {
      main = c("Baseline-hazard effects", 
        paste("Prob(T > ", data$target, ")", sep = ""),
        paste("Time-dependent spatial effect (t = ", data$target, ")", sep = ""),
        "Time-constant spatial effect")
    }
    main <- rep(main, length.out = 4)
    if(spar)
      par(mfrow = n2mfrow(length(what)))
    if("curves" %in% what) {
      matplot(data$curves[, 1], data$curves[, -1], type = "l", lty = 1,
        col = rgb(0.1, 0.1, 0,1, alpha = 0.01), xlab = "Arrivaltime",
        ylab = "Log relative risk", main = main[1])
      plot2d(firemodel$results$lambda$s.effects[["ti(arrivaltime)"]], c.select = c(1, 3),
        col.lines = rainbow_hcl(1), add = TRUE, rug = FALSE, lwd = 2)
      abline(v = data$target, col = "blue", lty = 2)
      legend("bottomright", c("Mean baseline", "Spatial-varying baseline"), lwd = c(2, 1),
        col = c(rainbow_hcl(1), "black"), box.col = NA, bg = NA, cex = 0.95)
    }
    if("daytime_curves" %in% what & !is.null(data$daytime)) {
      matplot(seq(0, 24, length = 100), data$daytime[, -1], type = "l", lty = 1,
        col = rgb(0.1, 0.1, 0,1, alpha = 0.01), xlab = "Time of day",
        ylab = "Effect of time of day", main = "Spatial-varying daytime effect")
      plot2d(firemodel$results$gamma$s.effects[["ti(daytime)"]], c.select = c(1, 3),
        col.lines = rainbow_hcl(1), add = TRUE, rug = FALSE, lwd = 2)
      abline(v = 8.5, col = "blue", lty = 2)
      legend("bottomright", c("Mean daytime effect", "Spatial-varying effect"), lwd = c(2, 1),
        col = c(rainbow_hcl(1), "black"), box.col = NA, bg = NA, cex = 0.95)
    }
    if("spatial_prob" %in% what) {
      plot(LondonBoroughs, main = main[2],
        xlab = "Longitude [deg]", ylab = "Latitude [deg]")
      lr <- c(0, max(data$spatial$spatial_prob))
      xr <- c(0, quantile(data$spatial$spatial_prob, probs = 0.95))
      xymap(lon, lat, spatial_prob, data = data$spatial, pos = "bottomright",
        layout = FALSE, map = FALSE, color = heat_hcl,
        shift = c(0.03, 0.05), symmetric = FALSE, add = TRUE,
        side.legend = 2, digits = 1, range = xr, lrange = round(lr, 1),
        width = 0.2, height = 0.04, ...)
      plot(LondonBoroughs, add = TRUE)
      box()
      axis(1)
      axis(2)
    }
    if("fsintens" %in% what) {
      plot(firemodel, model = "gamma", term = "(fsintens)", spar = FALSE,
        xlab = "Fire station intensity", main = "Effect of fire station intensity",
        ylab = "Effect", rug = FALSE, scheme = 2, grid = 100)
    }
    if("spatial_td" %in% what) {
      plot(LondonBoroughs, main = main[3],
        xlab = "Longitude [deg]", ylab = "Latitude [deg]")
      lr <- range(data$spatial$spatial_td)
      lr <- c(-1 * max(abs(lr)), max(abs(lr)))
      xr <- quantile(data$spatial$spatial_td, probs = 0.9)
      xr <- c(-1 * xr, xr)
      xymap(lon, lat, spatial_td, data = data$spatial, pos = "bottomright",
        layout = FALSE, map = FALSE, color = diverge_hcl, swap = TRUE,
        shift = c(0.03, 0.05), symmetric = TRUE, add = TRUE,
        side.legend = 2, digits = 1, range = xr, lrange = round(lr, 1),
        width = 0.2, height = 0.04, ...)
      plot(LondonBoroughs, add = TRUE)
      box()
      axis(1)
      axis(2)
    }
    if("daytime" %in% what) {
      plot(firemodel, model = "gamma", term = "(daytime)", spar = FALSE, rug = FALSE,
        xlab = "Time of day", main = "Effect of time of day", ylab = "Effect",
        scheme = 2, grid = 100)
    }
    if("spatial_tc" %in% what) {
      plot(LondonBoroughs, main = main[4],
        xlab = "Longitude [deg]", ylab = "Latitude [deg]")
      lr <- range(data$spatial$spatial_tc)
      lr <- c(-1 * max(abs(lr)), max(abs(lr)))
      xr <- quantile(data$spatial$spatial_tc, probs = 0.9)
      xr <- c(-1 * xr, xr)
      xymap(lon, lat, spatial_tc, data = data$spatial, pos = "bottomright",
        layout = FALSE, map = FALSE, color = diverge_hcl, swap = TRUE,
        shift = c(0.03, 0.05), symmetric = TRUE, add = TRUE,
        side.legend = 2, digits = 1, range = xr, lrange = round(lr, 1),
        width = 0.2, height = 0.04, ...)
      plot(LondonBoroughs, add = TRUE)
      box()
      axis(1)
      axis(2)
    }
    if("spatial_daytime" %in% what) {
      plot(LondonBoroughs, main = "Spatial daytime effect",
        xlab = "Longitude [deg]", ylab = "Latitude [deg]")
      lr <- range(data$spatial$spatial_daytime)
      lr <- c(-1 * max(abs(lr)), max(abs(lr)))
      xr <- quantile(data$spatial$spatial_daytime, probs = 0.9)
      xr <- c(-1 * xr, xr)
      xymap(lon, lat, spatial_daytime, data = data$spatial, pos = "bottomright",
        layout = FALSE, map = FALSE, color = diverge_hcl, swap = TRUE,
        shift = c(0.03, 0.05), symmetric = TRUE, add = TRUE,
        side.legend = 2, digits = 1, range = xr, lrange = round(lr, 1),
        width = 0.2, height = 0.04, ...)
      plot(LondonBoroughs, add = TRUE)
      box()
      axis(1)
      axis(2)
    }
    if("stations" %in% what) {
      co <- bbox(LondonBoroughs)
      scale <- 0.5
      ylim <- c(co["y", 1], co["y", 2] + scale * abs(diff(co["y", ])))
      plot(LondonBoroughs, main = "",
        xlab = "Longitude [deg]", ylab = "", ylim = ylim,
        col = gray(0.9))
      cfun <- function(n) { heat_hcl(n, alpha = 0.8) }
      pal <- make_pal(col = cfun, ncol = 99, data = LondonFire$arrivaltime, symmetric = FALSE,
        range = c(3, 8), swap = TRUE)
      ld <- as.data.frame(LondonFire)
      ld <- ld[order(ld$arrivaltime, decreasing = TRUE), ]
      points(ld$lon, ld$lat, pch = 4, col = pal$map(ld$arrivaltime), cex = 0.6)
      points(LondonFStations, pch = 16, col = "blue", cex = 0.8)
      points(LondonFStations, cex = 0.8)
      box()
      axis(1)
      axis(4, at = round(seq(co[2, 1] + 0.07, co[2, 2] - 0.07, length = 4), 2))
      mtext(paste("Latitude [deg]",
        paste(rep(" ", 30), collapse = "", sep = "")), side = 4, line = 3)
      legend("bottomleft", c("Fire", "Station"), pch = c(4, 21),
        col = c("black", "black"), pt.bg = c(NA, "blue"), box.col = NA, bg = NA, cex = 0.95)
      i <- order(LondonFire$arrivaltime, decreasing = TRUE)
      atimes <- LondonFire$arrivaltime[i]
      lon <- LondonFire$lon[i]
      shift <- scale * 0.1 * abs(diff(co["y", ]))
      y <- scale2(atimes, co["y", 2] + shift, ylim[2])
      rect(lon - 0.0015, rep(co["y", 2] + shift,length = length(y)), lon + 0.0015, y,
        col = pal$map(atimes), border = NA)
      lines(co[1, ], rep(co["y", 2] + shift, 2))
      wt <- c(min(atimes), 6, 12, 20)
      wt2 <- scale2(wt, co["y", 2] + shift, ylim[2])
      axis(2, at = wt2, labels = fmt(wt, 2, 0))
      mtext(paste(paste(rep(" ", 70), collapse = "", sep = ""), "Arrivaltime [min]"), side = 2, line = 3)
    }
  }

  grid_plot <- function(n = 10, FUN = NULL,
    color = heat_hcl, symmetric = FALSE, swap = TRUE, eps = 0.0001, ...)
  {
    library("bamlss")
    library("maptools")
    library("hexbin")

    if(is.null(FUN))
      FUN <- function(x) { sum(x > 6) / length(x) }
    data("LondonFire", package = "bamlss")
    d <- as.data.frame(LondonFire)[, c("lon", "lat", "arrivaltime")]

    hbin <- hexbin(d$lon, d$lat ,IDs=TRUE)
    hvp <- plot(hbin)
    mtrans <- hexTapply(hbin, d$arrivaltime, median, na.rm=TRUE)
    pushHexport(hvp$plot.vp)
    plot(LondonBoroughs)
    grid.hexagons(hbin, pen = 0, border = 'red', use.count = FALSE, cell.at = mtrans)

    xy <- bbox(LondonBoroughs)
    co <- expand.grid(
      "lon" = seq(min(xy[1, 1]), max(xy[1, 2]), length = n),
      "lat" = seq(min(xy[2, 1]), max(xy[2, 2]), length = n)
    )
    ob <- unionSpatialPolygons(LondonBoroughs,
      rep(1L, length = length(LondonBoroughs)))
    nob <- length(slot(slot(ob, "polygons")[[1]], "Polygons"))
    pip <- NULL
    for(j in 1:nob) {
     oco <- slot(slot(slot(ob, "polygons")[[1]], "Polygons")[[j]], "coords")
     pip <- cbind(pip, point.in.polygon(co$lon, co$lat, oco[, 1L], oco[, 2L], mode.checked = FALSE) < 1L)
    }
    pip <- apply(pip, 1, function(x) all(x))

    co <- co[pip < 1, , drop = FALSE]
    dx <- diff(co[, 1])
    dx <- min(dx[dx > 0]) / 4
    dy <- diff(co[, 2])
    dy <- min(dy[dy > 0]) / 4
    
    polys <- lapply(1:nrow(co), function(i) {
      pol <- hexpolygon(co[i, 1], co[i, 2], dx = dx, dy = dy)
      cbind(pol$x, pol$y)
    })

    plot(LondonBoroughs)
    points(co)
    for(i in seq_along(polys))
      polygon(polys[[i]])

    d$X <- round(d$lon, digits)
    d$Y <- round(d$lat, digits)
    d$cell <- d$X + 360 * d$Y
    val <- by(d, d$cell, function(d) c(d$X[1], d$Y[1], FUN(d$arrivaltime)))
    val.m <- matrix(unlist(val), nrow = 3)
    rownames(val.m) <- c("X", "Y", "FUN")
    val.m <- as.data.frame(t(val.m))
    pp <- cbind(val.m$X, val.m$Y)
    dx <- abs(diff(pp[, 1])); dy <- abs(diff(pp[, 2]))
    dx <- dx[dx != 0]; dy <- dy[dy != 0]
    dx <- dx[abs(dx) > eps]
    dy <- dy[abs(dy) > eps]
    res <- c(min(dx), min(dy))
    colors <- colorlegend(x = val.m$FUN, plot = FALSE,
      color = color, symmetric = symmetric, swap = swap, ...)
    col <- colors$map(val.m$FUN)
    plot(LondonBoroughs)
    rect(pp[, 1] - res[1] / 2, pp[, 2] - res[2] / 2, pp[, 1] + res[1] / 2, pp[, 2] + res[2] / 2,
      col = col, border = col, lwd = 0)
    plot(LondonBoroughs, add = TRUE)
    colorlegend(x = val.m$FUN, plot = FALSE, add = TRUE,
      color = color, swap = swap, symmetric = symmetric, width = 0.2, height = 0.04,
      pos = "bottomright", side.legend = 2, ...)
  }

grid_plot()

  png("figures/firemodel-data.png", units = "in", res = 120, width = 4, height = 4,
    pointsize = 8)
  par(mar = c(4.1, 4.1, 0.1, 4.1))
  plot.firemodel(firemodel_plotdata, what = "stations", spar = FALSE)
  dev.off()

  png("figures/firemodel-effects.png", units = "in", res = 120, width = 6, height = 8)
  plot.firemodel(firemodel_plotdata)
  dev.off()
}
@

\begin{figure}[t!]
\centering
\includegraphics[width=0.56\textwidth]{figures/firemodel-data}
\caption{\label{fig:londonfiredata} Distribution of dwelling fires and fire stations in London (2015).}
\end{figure}

\begin{figure}[t!]
\centering
\includegraphics[width=0.92\textwidth]{figures/firemodel-effects}
\caption{\label{fig:londonfiredeffects} Estimated effects of the fire emergency response times
  survival model.}
\end{figure}


\section{Summary}\label{sec:conclusion}


\section*{Acknowledgments}


\bibliography{bamlss}


\clearpage


\begin{appendix}

\end{appendix}


\end{document}

