\documentclass[nojss]{jss}
% \documentclass[article]{jss}
\usepackage{amsmath,amssymb,amsfonts,thumbpdf}
\usepackage{multirow,longtable}
\usepackage{algorithmic}
\usepackage{algorithm}
\usepackage{rotating}

\definecolor{darkgray}{rgb}{0.1,0.1,0.1}
\definecolor{heat1}{rgb}{0.8274510, 0.2470588, 0.4156863}
\definecolor{heat2}{rgb}{0.8823529, 0.4392157, 0.2980392}
\definecolor{heat3}{rgb}{0.9137255, 0.6039216, 0.1725490}
\definecolor{heat4}{rgb}{0.9098039, 0.7647059, 0.2352941}
\definecolor{heat5}{rgb}{0.8862745, 0.9019608, 0.7411765}
\definecolor{blue1}{RGB}{0, 126, 255}


%% additional commands
\newcommand{\squote}[1]{`{#1}'}
\newcommand{\dquote}[1]{``{#1}''}
\newcommand{\fct}[1]{{\texttt{#1()}\index{#1@\texttt{#1()}}}}
\newcommand{\class}[1]{\dquote{\texttt{#1}}}
%% for internal use
\newcommand{\fixme}[1]{\emph{\marginpar{FIXME} (#1)}}
\newcommand{\readme}[1]{\emph{\marginpar{README} (#1)}}

%% Authors: NU + rest in alphabetical order
\author{Nikolaus Umlauf\\Universit\"at Innsbruck \And
        Nadja Klein\\Universit\"at G\"ottingen \And
        Achim Zeileis\\Universit\"at Innsbruck}
\Plainauthor{Nikolaus Umlauf, Nadja Klein, Achim Zeileis}

\title{BAMLSS: Bayesian additive models for location, scale and shape (and beyond)}
\Plaintitle{BAMLSS: Bayesian additive models for location, scale and shape (and beyond)}

\Keywords{GAMLSS, distributional regression, MCMC, \proglang{BUGS}, \proglang{R}, software}
\Plainkeywords{GAMLSS, distributional regression, MCMC, BUGS, R, software}

\Abstract{
  Bayesian analysis provides a convenient setting for the estimation of complex generalized
  additive regression models. Since computational power has tremendously increased in the past
  decade it is now possible to tackle complicated inferential problems, e.g., with Markov chain
  Monte Carlo simulation, on virtually any modern computer. This is one of the reasons why
  Bayesian methods have become quite popular and it has lead to a number of highly specialized and
  optimized estimation engines. The paper exploits the quite general structures that arise
  for the estimation of a variety of GAM-type models and proposes a unified modeling architecture
  that can deal with a wide range of types of model terms. We show that within this framework
  implementing algorithms for complex regression problems, as well as the integration of already
  existing software, is relatively straightforward. We further emphasize the usefulness with an
  example of a very large climatology model for over $1.2$ million daily precipitation observations,
  as well as on a computational demanding Cox model for continuous time with space-time interactions
  on a data set with over five thousand ``individuals''.
}

\Address{
  Nikolaus Umlauf, Achim Zeileis\\
  Department of Statistics\\
  Faculty of Economics and Statistics\\
  Universit\"at Innsbruck\\
  Universit\"atsstr.~15\\
  6020 Innsbruck, Austria\\
  E-mail: \email{Nikolaus.Umlauf@uibk.ac.at}, \email{Achim.Zeileis@R-project.org}\\
  URL: \url{http://eeecon.uibk.ac.at/~umlauf/},\\
  \phantom{URL: }\url{http://eeecon.uibk.ac.at/~zeileis/}\\

  Nadja Klein\\
  Chairs of Statistics and Econometrics\\
  Universit\"at G\"ottingen\\
  Humboldtallee 3\\
  37073 G\"ottingen, Germany\\
  E-mail: \email{nklein@uni-goettingen.de}\\
  URL: \url{https://www.uni-goettingen.de/de/325353.html},\\
  \phantom{URL: }\url{https://www.uni-goettingen.de/de/325353.html}
}

%% Sweave/vignette information and metadata
%% need no \usepackage{Sweave}
\SweaveOpts{engine = R, eps = FALSE, keep.source = TRUE}

<<preliminaries, echo=FALSE, results=hide>>=
options(width = 70, prompt = "R> ", continue = "+  ")
set.seed(1090)

library("bamlss")
library("survival")
source("models.R")
source("figures.R")
@


\begin{document}
%%\SweaveOpts{concordance=TRUE}

\section{Introduction} \label{sec:intro}

The generalized additive model for location, scale and shape
(GAMLSS,~\citealp{bamlss:Rigby+Stasinopoulos:2005}) relaxes the distributional assumptions of
a response variable in a way that allows for modeling the mean (location) as well
as higher moments (scale and shape) in terms of covariates. This is
especially useful in cases where, e.g., the response does not follow the exponential family or
particular interest lies on scale and shape parameters. Moreover, covariate effects can have
flexible forms such as, e.g., linear, nonlinear, spatial or random effects. Hence, each parameter
of the distribution is linked to an additive predictor in similar fashion as for the well
established generalized additive model (GAM,~\citealp{bamlss:Hastie+Tibshirani:1990}).

The terms of an additive predictor are most commonly represented by basis function approaches. This
leads to a very generic model structure and can be further exploited because each term
can be transformed into a mixed model representation
\citep{bamlss:Ruppert+Wand+Carrol:2003, bamlss:Wand:2003}. In a fully Bayesian setting this
generality remains because priors on parameters can also be formalized in a general way, e.g.,
by assigning normal priors to the regression coefficients of smooth terms
\citep{bamlss:Fahrmeir+Kneib+Lang+Marx:2013, bamlss:Brezger+Lang:2006}.

The fully Bayesian approach using Markov chain Monte Carlo (MCMC) simulation techniques is particularly
attractive since the inferential framework provides valid credible intervals for estimators in
situations where confidence intervals for corresponding maximum likelihood estimators based on
asymptotic properties fail. This is specifically the case in more complex models, e.g., with response
distributions outside the exponential family or when multiple predictors contain several smooth
effects \citep{bamlss:Klein+Kneib+Lang:2015}. In addition, extensions such as variable selection,
non-standard priors for hyper-parameters or multilevel models are easily included.
Probably for this reason and because computational power has tremendously increased in the past
decade, the number of Bayesian estimation engines, but also frequentist, that can tackle complicated
inferential problems has kept increasing. Existing estimation engines already provide infrastructures
for a number of regression problems exceeding univariate responses, e.g., for multinomial,
multivariate normal or mixed discrete-continuous distributed variables, and so forth. In addition,
most of the engines support random effect estimation which in the end can in principle be utilized
for setting up complex models with additive predictors
(see, e.g., \citealp{bamlss:Wood:2006}, \citealp{bamlss:Wood:2016b}).

However, the majority of engines use different model setups and output 
formats, which makes it difficult for practitioners, e.g., to compare properties of
different algorithms or to select the appropriate distribution and variables, etc. The reasons are
manifold: the use of different model specification languages like
\proglang{BUGS}~\citep{bamlss:BUGS:2009} or \proglang{R}~\citep{bamlss:R}; different standalone
statistical software packages like \pkg{BayesX}~\citep{bamlss:Umlauf+Adler+Kneib+Lang+Zeileis:2014,
bamlss:Belitz+Brezger+Kneib+Lang+Umlauf:2016}, \pkg{JAGS}~\citep{bamlss:Plummer:2013},
\pkg{Stan}~\citep{bamlss:stan-software:2016} or
\pkg{WinBUGS}~\citep{bamlss:Lunn+Thomas+Best+Spiegelhalter:2000}; or even differences within the
same environment, e.g., the \proglang{R} packages \pkg{mgcv}
\citep{bamlss:Wood:2016}, \pkg{gamlss} \citep{bamlss:Stasinopoulos+Rigby:2016} and \pkg{VGAM}
\citep{bamlss:Yee:2016} implement all model term infrastructures in their own fashion style. This is
particularly problematic if all packages are loaded into \proglang{R}'s global environment, because
some functions that are supposed to fulfill the same purpose have different interpretations.

In this article we present a kind of unified conceptional ``Lego toolbox'' for complex
regression models. We show that iterative estimation algorithms, e.g., for posterior mode or
mean estimation based on MCMC simulation, exhibit very similar structures such that the process of
model building becomes relatively straightforward, since the model architecture is only a
combination of single ``bricks''. Moreover, the framework embeds very general model terms which are
not necessarily a linear combination of a design matrix and regression coefficients. Hence,
the concept supports more than the GAMLSS statistical model class, however, because of the great
similarities with GAMLSS we call the conceptional framework Bayesian additive models for location,
scale and shape (BAMLSS) and beyond. The concept can be exploited in three ways: First, to develop
new models and algorithms, secondly, to compare algorithms and third to easily integrate existing
implementations. A prove of concept is given in the corresponding \proglang{R} implementation
\pkg{bamlss} \citep{bamlss:Umlauf+Klein+Zeileis:2016}.

The remainder of the paper is as follows. In Section~\ref{sec:model} the models supported by this
framework are briefly introduced. Section~\ref{sec:legobox} presents the ``Lego bricks'' that can be
utilized for model building and estimation. In Section~\ref{sec:strategies} computational strategies
for the implementation are presented. Finally, Section~\ref{sec:softex} briefly introduces the
\proglang{R} package \pkg{bamlss}. The framework is further illustrated on a very large model for
daily precipitation observations, as well as on the computationally challenging Cox model for
continuous time.


\newpage


\section{Model structure} \label{sec:model}

Based on data for $i = 1, \ldots, n$ observations, the models discussed in this paper
assume conditional independence of individual response observations given covariates.
Within the GAMLSS or distributional model class
\citep{bamlss:Rigby+Stasinopoulos:2005,bamlss:Klein+Kneib+Lang+Sohn:2015}
all parameters of the response distribution can be modeled by
explanatory variables such that
\begin{equation*} \label{eqn:dreg}
y \sim \mathbf{\mathcal{D}}\left(h_{1}(\theta_{1}) = \eta_{1}, \,\,
  h_{2}(\theta_{2}) = \eta_{2}, \dots, \,\, h_{K}(\theta_{K}) =
  \eta_{K}\right),
\end{equation*}
where $\mathbf{\mathcal{D}}$ denotes a parametric distribution available for the response
variable $y$ and $\theta_k$, $k = 1, \ldots, K$, are parameters that are linked to additive predictors
using known monotonic and twice differentiable functions
$h_{k}(\cdot)$. Note that the response may also be a
$q$-dimensional vector $\mathbf{y} = (y_{1}, \ldots, y_{q})^\top$, e.g., when
$\mathbf{\mathcal{D}}$ is a distribution of random vectors with dimension $q$.
(see, e.g., \citealp{bamlss:Klein+Kneib+Klasen+Lang:2015}).
The $k$-th additive predictor is given by
\begin{equation} \label{eqn:addpred}
\eta_k = \eta_k(\mathbf{x}; \boldsymbol{\beta}_k) =
  f_{1k}(\mathbf{x}; \boldsymbol{\beta}_{1k}) + \ldots + f_{J_kk}(\mathbf{x}; \boldsymbol{\beta}_{J_kk}),
\end{equation}
with unspecified (possibly nonlinear) functions $f_{jk}(\cdot)$ of subvectors of a vector
$\mathbf{x}$ collecting all available covariate information, $j = 1, \ldots, J_k$ and $k = 1, \ldots, K$ and
$\boldsymbol{\beta}_k = (\boldsymbol{\beta}_{1k}, \ldots, \boldsymbol{\beta}_{J_kk})^\top$ are
regression coefficients that need to estimated from the data. The vector of function
evaluations $\mathbf{f}_{jk} = (f_{jk}(\mathbf{x}_{1}; \boldsymbol{\beta}_{jk}), \ldots,
  f_{jk}(\mathbf{x}_{n}; \boldsymbol{\beta}_{jk}))^{\top}$ of the
$i = 1, \ldots, n$ observations is then given by
\begin{equation} \label{eqn:functions}
\mathbf{f}_{jk} = \begin{pmatrix}
f_{jk}(\mathbf{x}_{1}; \boldsymbol{\beta}_{jk}) \\
\vdots \\
f_{jk}(\mathbf{x}_{n}; \boldsymbol{\beta}_{jk})
\end{pmatrix} = f_{jk}(\mathbf{X}_{jk}; \boldsymbol{\beta}_{jk}),
\end{equation}
where $\mathbf{X}_{jk}$ ($n \times m_{jk}$) is a design matrix and the structure of $\mathbf{X}_{jk}$
only depends on the type of covariate(s) and prior assumptions about $f_{jk}( \cdot )$.
In this notation the $k$-th parameter vector is given by
\begin{equation*} \label{eqn:addpar}
h_k(\boldsymbol{\theta}_k) = \boldsymbol{\eta}_k = \eta_k(\mathbf{X}_k ; \boldsymbol{\beta}_k) =
  \mathbf{f}_{1k} + \ldots + \mathbf{f}_{J_kk},
\end{equation*}
where $\mathbf{X}_k = (\mathbf{X}_{1k}, \ldots, \mathbf{X}_{J_kk})^\top$ is the combined design
matrix for parameter $k$.

The functions $f_{jk}(\cdot)$ are usually based on a basis function approach, where $\eta_k$ then is
a typical GAM-type or so called structured additive predictor
(STAR,~\citealp{bamlss:Fahrmeir+Kneib+Lang:2004, bamlss:Brezger+Lang:2006}). However, in this paper
we relax this assumption and let $f_{jk}(\cdot)$ be an unspecified composition of covariate data
$\mathbf{x}$ and regression coefficients $\boldsymbol{\beta}_{jk}$ ($q_{jk} \times 1$). For example,
if a linear basis function approach is used
$$
\mathbf{f}_{jk} = \mathbf{X}_{jk}\boldsymbol{\beta}_{jk},
$$
if $f_{jk}( \cdot )$ represents a growth curve like the Gompertz function the vector of function
evaluations yields
$$
\mathbf{f}_{jk} = \beta_{1} \cdot \exp \left( -\exp\left( \beta_{2} +
  \mathbf{X}_{jk}\beta_{3} \right) \right),
$$
where $f_{jk}( \cdot )$ is also nonlinear in the parameters $\boldsymbol{\beta}_{jk}$. However,
even more complex model terms are supported by the BAMLSS framework.

Note that in a linear basis representation the individual model components
$\mathbf{X}_{jk}\boldsymbol{\beta}_{jk}$ can be further
decomposed into a mixed model representation given by
\begin{equation} \label{eqn:mixed}
\mathbf{f}_{jk} = \tilde{\mathbf{X}}_{jk}\tilde{\boldsymbol{\gamma}}_{jk} +
  \mathbf{U}_{jk}\tilde{\boldsymbol{\beta}}_{jk},
\end{equation}
where $\tilde{\boldsymbol{\gamma}}_{jk}$ represents the fixed effects parameters and 
$\tilde{\boldsymbol{\beta}}_{jk} \sim \mathcal{N}(\mathbf{0}, \tau^2_{jk}\mathbf{I})$ i.i.d.\ random effects.
The design matrix $\mathbf{U}_{jk}$ is derived from a spectral decomposition of the penalty matrix
$\mathbf{K}_{jk}$ and $\tilde{\mathbf{X}}_{jk}$ by finding a basis of the null space of $\mathbf{K}_{jk}$
such that $\tilde{\mathbf{X}}_{jk}^{\top}\mathbf{K}_{jk} = \mathbf{0}$, i.e., parameters
$\tilde{\boldsymbol{\gamma}}_{jk}$ are not penalized (see, e.g.,
\citealp{bamlss:Ruppert+Wand+Carrol:2003, bamlss:Wand:2003, bamlss:Fahrmeir+Kneib+Lang+Marx:2013}).


\section{A conceptional Lego toolbox} \label{sec:legobox}

\subsection{Priors} \label{sec:priors}

In Bayesian models prior distributions need to be assigned for each parameter vector
$\boldsymbol{\beta}_{jk}$, where the choice of prior distributions depends on the type of
model and application. A simple choice for a non-informative prior is
\begin{equation} \label{eqn:oneprior}
p_{jk}(\boldsymbol{\beta}_{jk}) \propto 1.
\end{equation}

Since the functions $f_{jk}( \cdot )$ are commonly unspecified and flexible some kind regularization
needs to be ensured, e.g., for penalizing too abrupt jumps in the functional form using a
P(enalized)-spline representation of $f_{jk}( \cdot )$ \citep{bamlss:Eilers+Marx:1996}. In the
frequentist setting this is accounted for by adding a penalty
$\text{pen}(\mathbf{f}_{jk}) = \text{pen}(\boldsymbol{\beta}_{jk})$ to the regression
problem. In the Bayesian formulation the equivalent is, e.g., to assign a normal prior
on $\boldsymbol{\beta}_{jk}$ (see,~e.g.,~\citealp{bamlss:Fahrmeir+Kneib+Lang+Marx:2013}).
However, within the presented BAMLSS framework we relax this assumption and assign the rather general
prior
\begin{equation} \label{eqn:gprior}
p_{jk}(\boldsymbol{\beta}_{jk}; \boldsymbol{\tau}_{jk}, \boldsymbol{\alpha}_{jk}) \propto
   d_{\boldsymbol{\beta}_{jk}}(\boldsymbol{\beta}_{jk} | \, \boldsymbol{\tau}_{jk};
     \boldsymbol{\alpha}_{\boldsymbol{\beta}_{jk}}) \cdot
   d_{\boldsymbol{\tau}_{jk}}(\boldsymbol{\tau}_{jk} | \, \boldsymbol{\alpha}_{\boldsymbol{\tau}_{jk}}),
\end{equation}
where $d_{\boldsymbol{\beta}_{jk}}( \cdot )$ and $d_{\boldsymbol{\tau}_{jk}}( \cdot )$ are prior
densities (or combinations of densities) that depend on the type of covariate and prior
assumptions about $f_{jk}( \cdot )$.
Here, the vector $\boldsymbol{\tau}_{jk}$ represents additional prior parameters,
typically, these parameters are variances, e.g., that account for the
degree of smoothness of $f_{jk}( \cdot )$ or the amount of correlation between observations.
For example, using a spline representation of $f_{jk}( \cdot )$ in combination with a normal
prior for $d_{\boldsymbol{\beta}_{jk}}( \cdot )$ the variances can be interpreted as the inverse
smoothing parameters in a penalized regression context. Within this very general setup
$\boldsymbol{\alpha}_{jk} = \{\boldsymbol{\alpha}_{\boldsymbol{\beta}_{jk}},
\boldsymbol{\alpha}_{\boldsymbol{\tau}_{jk}}\}$ is a set of fixed
prior specifications, e.g., further controlling the shape of $d_{\boldsymbol{\beta}_{jk}}( \cdot )$
and $d_{\boldsymbol{\tau}_{jk}}( \cdot )$, incorporating prior beliefs about
$\boldsymbol{\beta}_{jk}$, or for GAM-type models $\boldsymbol{\alpha}_{jk}$ usually holds
the so called penalty matrices, amongst others.

In numerous applications geographical information and spatial covariates are given at
different resolutions. For example, spatial data that is measured within different regions, for
which additional regional covariates are available. Whenever there is such a nested structure
in the data, it is possible to model the complex (spatial) heterogeneity effects using
a compound prior
\begin{equation*} \label{eqn:compoundprior}
\boldsymbol{\beta}_{jk} = \tilde{\boldsymbol{\eta}}_{jk}(\mathbf{x}; \tilde{\boldsymbol{\beta}}_{jk})
  + \boldsymbol{\varepsilon}_{jk},
\end{equation*}
where $\boldsymbol{\varepsilon}_{jk} \sim \mathcal{N}(\mathbf{0}, \tilde{\tau}^2\mathbf{I})$ is a vector
of i.i.d.\ Gaussian random effects and
$\tilde{\boldsymbol{\eta}}_{jk}(\mathbf{x}; \tilde{\boldsymbol{\beta}}_{jk})$ represents a full
predictor of nested covariates, e.g., including a discrete regional spatial
effect. This way, potential costly operations during estimation
can be avoided since the number of observations in
$\tilde{\boldsymbol{\eta}}_{jk}(\mathbf{x}; \tilde{\boldsymbol{\beta}}_{jk})$ is equal to
the number of coefficients in $\boldsymbol{\beta}_{jk}$, which is usually much smaller than the
actual number of observations $n$. Moreover, the full conditionals for
$\tilde{\boldsymbol{\beta}}_{jk}$ are Gaussian regardless of the response distribution and leads to
highly efficient estimation algorithms,
see~\citet{bamlss:Lang+Umlauf+Wechselberger+Harttgen+Kneib:2014}.


\subsection{Response distribution} \label{sec:density}

The main building block of regression model algorithms is the probability density function
$d(\mathbf{y} | \boldsymbol{\theta}_1, \ldots, \boldsymbol{\theta}_K)$, or for
computational reasons its logarithm. Estimation typically requires to evaluate the log-likelihood
function
\begin{equation*} \label{eqn:loglik}
\ell(\boldsymbol{\beta} ; \mathbf{y}, \mathbf{X}) =
  \sum_{i = 1}^n \log \, d(y_i ; \theta_{i1} = h_1^{-1}(\eta_{i1}(\mathbf{x}_i; \boldsymbol{\beta}_1)), \ldots,
  \theta_{iK} = h_K^{-1}(\eta_{iK}(\mathbf{x}_i; \boldsymbol{\beta}_K)))
\end{equation*}
a number of times, where the vector
$\boldsymbol{\beta} = (\boldsymbol{\beta}_1^\top, \ldots, \boldsymbol{\beta}_K^\top)^\top$ 
comprises all model coefficients that should be estimated, $\mathbf{X} = (\mathbf{X}_1, \ldots, \mathbf{X}_K)$
are the respective covariate matrices with $\mathbf{x}_i$ as the $i$-th row of $\mathbf{X}$ and
$\boldsymbol{\theta}_k$ are vectors of length $n$.
Assigning prior distributions to the individual model components results in the log-posterior
\begin{equation} \label{eqn:logpost}
\log \, \pi(\boldsymbol{\beta}, \boldsymbol{\tau} ; \mathbf{y}, \mathbf{X}, \boldsymbol{\alpha}) \propto
  \ell(\boldsymbol{\beta} ; \mathbf{y}, \mathbf{X}) +
  \sum_{k = 1}^K\sum_{j = 1}^{J_k} \left[ \log \,
  p_{jk}(\boldsymbol{\beta}_{jk}; \boldsymbol{\tau}_{jk}, \boldsymbol{\alpha}_{jk}) \right],
\end{equation}
where $\boldsymbol{\tau} = (\boldsymbol{\tau}_1^\top, \ldots, \boldsymbol{\tau}_K^\top)^\top =
(\boldsymbol{\tau}_{11}^\top, \ldots, \boldsymbol{\tau}_{J_11}^\top, \ldots,
\boldsymbol{\tau}_{1K}^\top, \ldots, \boldsymbol{\tau}_{J_KK}^\top)^\top$ is
the vector of all assigned hyper-parameters used within prior functions $p_{jk}( \cdot )$
and similarly $\boldsymbol{\alpha}$ is the set of all fixed prior specifications. Note that from a
frequentist perspective (\ref{eqn:logpost}) can be viewed as a penalized log-likelihood, e.g., if
a normal prior is used within $p_{jk}( \cdot )$ for $\boldsymbol{\beta}_{jk}$
(see Section~\ref{sec:priorsbricks}).


\subsection{Model fitting} \label{sec:modelfit}

Bayesian point estimates of $\boldsymbol{\beta}$ and $\boldsymbol{\tau}$ are obtained by:
\begin{enumerate}
\item[(a)] Maximization of the log-posterior for posterior mode estimation.
\item[(b)] Solving high dimensional integrals, e.g., for posterior mean or median estimation.
\end{enumerate}
For the possibly very complex models within the BAMLSS framework, problems (a) and (b) are commonly
solved by computer intensive iterative algorithms, since analytical solutions are available only in
a small number of cases. These algorithms perform the updating scheme
\begin{equation} \label{eqn:updating}
(\boldsymbol{\beta}^{(t + 1)}, \boldsymbol{\tau}^{(t + 1)}) =
  U(\boldsymbol{\beta}^{(t)}, \boldsymbol{\tau}^{(t)};
    \mathbf{y}, \mathbf{X}, \boldsymbol{\alpha}),
\end{equation}
where function $U( \cdot )$ is an updating function, e.g., for generating one Newton-Raphson step
in (a) or getting the next step in a MCMC simulation in (b), amongst others. The updating scheme can
be partitioned into separate updating equations using leapfrog or zigzag iteration (see, e.g.,
\citealp{bamlss:Smyth:1996}). Now let
\begin{eqnarray} \label{eqn:blockupdate}
(\boldsymbol{\beta}_1^{(t + 1)}, \boldsymbol{\tau}_1^{(t + 1)}) &=&
  U_1(\boldsymbol{\beta}_1^{(t)}, \boldsymbol{\beta}_2^{(t)},
    \ldots, \boldsymbol{\beta}_K^{(t)}, \boldsymbol{\tau}_1^{(t)}, \boldsymbol{\tau}_2^{(t)},
    \ldots, \boldsymbol{\tau}_K^{(t)}; \mathbf{y}, \mathbf{X}_1, \boldsymbol{\alpha}_1) \nonumber \\
(\boldsymbol{\beta}_2^{(t + 1)}, \boldsymbol{\tau}_2^{(t + 1)}) &=&
  U_2(\boldsymbol{\beta}_1^{(t + 1)}, \boldsymbol{\beta}_2^{(t)},
    \ldots, \boldsymbol{\beta}_K^{(t)}, \boldsymbol{\tau}_1^{(t + 1)}, \boldsymbol{\tau}_2^{(t)},
    \ldots, \boldsymbol{\tau}_K^{(t)}; \mathbf{y}, \mathbf{X}_2, \boldsymbol{\alpha}_2) \\
  &\vdots& \nonumber \\
(\boldsymbol{\beta}_K^{(t + 1)}, \boldsymbol{\tau}_K^{(t + 1)}) &=&
  U_K(\boldsymbol{\beta}_1^{(t + 1)}, \boldsymbol{\beta}_2^{(t + 1)},
    \ldots, \boldsymbol{\beta}_K^{(t)}, \boldsymbol{\tau}_1^{(t + 1)}, \boldsymbol{\tau}_2^{(t + 1)},
    \ldots, \boldsymbol{\tau}_K^{(t)}; \mathbf{y}, \mathbf{X}_K, \boldsymbol{\alpha}_K) \nonumber
\end{eqnarray}
be a partitioned updating scheme with updating functions $U_k( \cdot )$, i.e., in each iteration
updates for the $k$-th parameter are computed while holding all other parameters
fixed. This splitting method can even be applied for all terms within a parameter
\begin{equation} \label{eqn:blockblockupdate}
(\boldsymbol{\beta}_{jk}^{(t + 1)}, \boldsymbol{\tau}_{jk}^{(t + 1)}) =
  U_{jk}(\boldsymbol{\beta}_{jk}^{(t)}, \boldsymbol{\tau}_{jk}^{(t)} | \, \cdot \,) \qquad
    j = 1, \ldots, J_k, \quad k = 1, \ldots, K,
\end{equation}
and $U_{jk}( \cdot )$ is an updating function for a single model term.

In a partitioned updating system functions $U_{jk}( \cdot )$ need not necessarily have the same
structure, e.g., for posterior mode estimation some updating functions could be based on
iteratively weighted least squares (IWLS) and some on ordinary Newton-Raphson steps (see, e.g.,
example Section~\ref{sec:coxreg}). In MCMC simulation it is common to mix between several sampling
methods depending on the type of model term. Hence, by using highly modular systems like
(\ref{eqn:blockupdate}) and (\ref{eqn:blockblockupdate}) it is possible to develop flexible
estimation algorithms for numerous possibly very complex models.


\section{Lego bricks} \label{sec:legobricks}

\subsection{Priors} \label{sec:priorsbricks}

In the following we summarize commonly used specifications for priors $p_{jk}( \cdot )$ used for
estimating GAM-type models.

\subsubsection{Linear effects}

For simple linear effects a common choice for $p_{jk}( \cdot )$ is to use a non-informative
flat prior given in (\ref{eqn:oneprior}). One of the simplest informative priors is a normal
prior given by
\begin{equation*} \label{eqn:linprior}
p_{jk}(\boldsymbol{\beta}_{jk}; \boldsymbol{\tau}_{jk}, \boldsymbol{\alpha}_{jk}) \propto
  \exp \left(- \frac{1}{2}(\boldsymbol{\beta}_{jk} -
    \mathbf{m})^{\top}\mathbf {P}_{jk}(\boldsymbol{\tau}_{jk})(\boldsymbol{\beta}_{jk} -
    \mathbf{m})\right),
\end{equation*}
where $\boldsymbol{\tau}_{jk}$ are assumed to be fixed with $d_{\boldsymbol{\tau}_{jk}}( \cdot ) = 1$
and $\boldsymbol{\alpha}_{jk} = \{\boldsymbol{\alpha}_{\boldsymbol{\beta}_{jk}} = \{\mathbf{m}\}\}$
with $\mathbf{m}$ as a prior mean for $\boldsymbol{\beta}_{jk}$. The matrix $\mathbf{P}_{jk}(\boldsymbol{\tau}_{jk})$
is a fixed prior precision matrix, e.g., $\mathbf{P}_{jk} = diag(\boldsymbol{\tau}_{jk})$. In a lot of
applications a vague prior specification is used with $\mathbf{m} = \mathbf{0}$ and a large
precision (see, e.g., \citealp{bamlss:Fahrmeir+Kneib+Lang+Marx:2013}).

\subsubsection{Nonlinear effects} \label{sec:smootheffects}

If the nonlinear functions in (\ref{eqn:addpred}) are modeled using a basis function approach
the usual choice of prior $p_{jk}( \cdot )$ is based on a multivariate normal kernel for
$\boldsymbol{\beta}_{jk}$ given by
\begin{equation} \label{eqn:mvnormprior}
d_{\boldsymbol{\beta}_{jk}}(\boldsymbol{\beta}_{jk} | \,
  \boldsymbol{\tau}_{jk}, \boldsymbol{\alpha}_{\boldsymbol{\beta}_{jk}}) \propto
  |\mathbf{P}_{jk}(\boldsymbol{\tau}_{jk})|^{\frac{1}{2}} \exp\left(
    -\frac{1}{2}\boldsymbol{\beta}_{jk}^\top\mathbf{P}_{jk}(\boldsymbol{\tau}_{jk})\boldsymbol{\beta}_{jk}\right).
\end{equation}
Here, the precision matrix $\mathbf{P}_{jk}(\boldsymbol{\tau}_{jk})$ is build up of
prespecified so called penalty matrices
$\boldsymbol{\alpha}_{\boldsymbol{\beta}_{jk}} = \{\mathbf{K}_{1jk}, \ldots, \mathbf{K}_{Ljk}\}$,
e.g., for tensor product smooths the precision matrix is
$\mathbf{P}_{jk}(\boldsymbol{\tau}_{jk}) = \sum_{l = 1}^{L_{jk}} \frac{1}{\tau_{ljk}^2}\mathbf{K}_{ljk}$.
Note that $\mathbf{P}_{jk}(\boldsymbol{\tau}_{jk})$ is often
not of full rank, therefore, $d_{\boldsymbol{\beta}_{jk}}( \cdot )$ is partially improper.
The variances $\tau_{ljk}^2$ account for the amount of smoothness (regularization) of the
function and can be interpreted as the inverse smoothing parameters in the frequentist approach.
A common choice for the prior for $\boldsymbol{\tau}_{jk}$ is based on inverse Gamma distributions
for each $\boldsymbol{\tau}_{jk} = (\tau_{1jk}, \ldots, \tau_{L_{jk}jk})^\top$
\begin{equation} \label{eqn:igprior}
d_{\boldsymbol{\tau}_{jk}}(\boldsymbol{\tau}_{jk} | \, \boldsymbol{\alpha}_{\boldsymbol{\tau}_{jk}}) =
  \prod_{l = 1}^{L_{jk}} \frac{b_{ljk}^{a_{ljk}}}{\Gamma(a_{ljk})} (\tau_{ljk}^2)^{-(a_{ljk} + 1)} \exp(-b_{ljk} / \tau_{ljk}^2),
\end{equation}
with fixed parameters $\boldsymbol{\alpha}_{\boldsymbol{\tau}_{jk}} = \{ \mathbf{a}_{jk}, \mathbf{b}_{jk} \}$.
Small values for $\mathbf{a}_{jk}$ and $\mathbf{b}_{jk}$ correspond to approximate flat priors for
$\log(\tau_{ljk}^2)$. Setting $\mathbf{b}_{jk} = \mathbf{0}$ and
$\mathbf{a}_{jk}=-\mathbf{1}$ or $\mathbf{a}_{jk} = -1/2 \cdot \mathbf{1}$ yields flat priors for
$\tau_{ljk}^2$ and $\tau_{ljk}$, respectively.
However, the inverse Gamma prior as a default option for variance parameters needs to be
reconsidered in some cases, e.g., it might be problematic if $\tau_{ljk}$ is close to zero, since the
results a very sensitive on the choice of $\mathbf{a}_{jk}$ and $\mathbf{b}_{jk}$. Therefore,
\citet{bamlss:Gelman:2006} proposes to use the half-Cauchy prior
\begin{equation*} \label{eqn:hcauchy}
d_{\boldsymbol{\tau}_{jk}}(\boldsymbol{\tau}_{jk} | \, \boldsymbol{\alpha}_{\boldsymbol{\tau}_{jk}}) =
  \prod_{l = 1}^{L_{jk}}\frac{2A_{ljk}}{\pi (\tau_{ljk}^2 + A_{ljk}^2)}, \quad A_{ljk} > 0,
\end{equation*}
with hyper-parameters $\boldsymbol{\alpha}_{\boldsymbol{\tau}_{jk}} = \{\mathbf{A}_{jk}\}$.
For $A_{ljk} \rightarrow \infty$ the priors are uniform, hence large values
(e.g., $A_{ljk} = 25$) result in weakly informative priors. A desirable property of the half-Cauchy
is that for $\tau_{ljk} = 0$ the density is a nonzero constant, whereas the density of the inverse
Gamma for $\tau_{ljk} \rightarrow 0$ vanishes (see also \citealp{bamlss:Polson+Scott:2012}). Note
that in our framework the priors for variances $\tau_{ljk}^2$ rather than $\tau_{ljk}$ are usually
considered. Hence, the corresponding priors have to be proposed. Another question is the actual
choice of hyper-parameters. A recent suggestion reducing this issue to the choice of a scale
parameter that is directly related to the functions $f_{jk}( \cdot )$ (and thus much better
interpretable and accessible for the user) is given in~\citet{bamlss:Klein+Kneib:2016} for several
different hyper-priors for $\tau_{ljk}^2$, such as resulting priors from half-Cauchy, half-normal
or uniform priors for $\tau_{ljk}$ or resulting penalized complexity
priors~\citep{bamlss:Simpson:Rue:Martins:Riebler:Sorbye:2015}, so called scale dependent priors.


\subsection{Model fitting} \label{sec:bricksmodelfit}

In the following we describe the quantities needed for the updating functions in the algorithms
presented in Section~\ref{sec:modelfit}.

\subsubsection{Posterior mode} \label{sec:postmode}

The mode of the posterior distribution is the mode of the log-posterior (\ref{eqn:logpost}) given by
\begin{equation*}
\text{Mode}(\boldsymbol{\beta}, \boldsymbol{\tau} | \, \mathbf{y}, \mathbf{X}, \boldsymbol{\alpha}) =
  \underset{\boldsymbol{\beta}, \boldsymbol{\tau}}{\text{arg max }} \log \,
  \pi(\boldsymbol{\beta}, \boldsymbol{\tau} ; \mathbf{y}, \mathbf{X}, \boldsymbol{\alpha})
\end{equation*}
and is equivalent to  the maximum likelihood estimator
\begin{equation*}
\text{ML}(\boldsymbol{\beta} | \, \mathbf{y}, \mathbf{X}) =
  \underset{\boldsymbol{\beta}}{\text{arg max }} \ell(\boldsymbol{\beta} ; \mathbf{y}, \mathbf{X})
\end{equation*}
when assigning flat priors (\ref{eqn:oneprior}) on $\boldsymbol{\beta}_{jk}$
for $j = 1, \ldots, J_k$, $k = 1, \ldots, K$.
For models involving shrinkage priors, e.g., in GAM-type models given by (\ref{eqn:mvnormprior}),
the posterior mode is equivalent to a penalized maximum likelihood estimator for fixed parameters
$\boldsymbol{\tau}_{jk}$ and prior densities $d_{\boldsymbol{\tau}_{jk}}( \cdot ) \propto 1$.
Moreover, the structure of (\ref{eqn:logpost}) usually prohibits estimation of
$\boldsymbol{\tau}_{jk}$ through maximization of the log-posterior and the
estimator $\hat{\boldsymbol{\tau}}_{jk}$ is commonly derived by additionally
minimizing an information criterion such as the Akaike information criterion (AIC) or the Bayesian
information criterion (BIC), see also \citealp{bamlss:Rigby+Stasinopoulos:2005} Appendix~A.2. for
a more detailed discussion on smoothing parameter estimation. In Section~\ref{sec:modchoice},
we briefly discuss a stepwise approach for smoothing parameter selection.

For developing general updating functions we begin with describing posterior mode estimation for
the case of fixed parameters $\boldsymbol{\tau}_{jk}$, because these updating functions form the
basis of estimation algorithms for $\boldsymbol{\tau}_{jk}$.
Estimation of
$\boldsymbol{\beta} = (\boldsymbol{\beta}_1^\top, \ldots, \boldsymbol{\beta}_K^\top)^\top$
requires solving equations
$\partial (\log \, \pi(\boldsymbol{\beta}, \boldsymbol{\tau} ; \mathbf{y}, \mathbf{X}, \boldsymbol{\alpha})) / \partial \boldsymbol{\beta} = \mathbf{0}$.
A particularly convenient updating function (\ref{eqn:updating}) for maximization of (\ref{eqn:logpost})
is a Newton-Raphson type updating 
\begin{equation} \label{eqn:newton}
\boldsymbol{\beta}^{(t + 1)} = U(\boldsymbol{\beta}^{(t)} | \, \cdot) = \boldsymbol{\beta}^{(t)} -
  \mathbf{H}\left( \boldsymbol{\beta}^{(t)} \right)^{-1}\mathbf{s}\left( \boldsymbol{\beta}^{(t)} \right)
\end{equation}
with score vector
\begin{equation*} \label{eqn:score}
\mathbf{s}(\boldsymbol{\beta}) = 
  \frac{\partial \log \, \pi(\boldsymbol{\beta}, \boldsymbol{\tau} ; \mathbf{y}, \mathbf{X}, \boldsymbol{\alpha})}{
    \partial \boldsymbol{\beta}}
= \frac{\partial \ell(\boldsymbol{\beta}; \mathbf{y}, \mathbf{X})}{\partial \boldsymbol{\beta}} +
    \sum_{k = 1}^K\sum_{j = 1}^{J_k} \left[ \frac{\partial \log \,
    p_{jk}(\boldsymbol{\beta}_{jk}; \boldsymbol{\tau}_{jk}, \boldsymbol{\alpha}_{jk})}{\partial \boldsymbol{\beta}} \right].
\end{equation*}
and Hessian matrix $\mathbf{H}(\boldsymbol{\beta})$ with components
\begin{equation*} \label{eqn:hessian}
\mathbf{H}_{ks}(\boldsymbol{\beta}) =
\frac{\partial \mathbf{s}(\boldsymbol{\beta}_k)}{\partial \boldsymbol{\beta}_s^\top} =
\frac{\partial^2 \log \,  \pi(\boldsymbol{\beta}, \boldsymbol{\tau} ; \mathbf{y}, \mathbf{X}, \boldsymbol{\alpha})}{
  \partial \boldsymbol{\beta}_k \partial \boldsymbol{\beta}_s^\top},
\end{equation*}
for $k = 1, \dots, K$ and $s = 1, \dots, K$. By chain rule, the part of the score vector involving
the derivatives of the log-likelihood for the $k$-th parameter can be further decomposed to
\begin{equation*} \label{eqn:score2}
\frac{\partial \ell(\boldsymbol{\beta}; \mathbf{y}, \mathbf{X})}{\partial \boldsymbol{\beta}_k} =
  \frac{\partial \ell(\boldsymbol{\beta}; \mathbf{y}, \mathbf{X})}{\partial \boldsymbol{\eta}_k}
  \frac{\partial \boldsymbol{\eta}_k}{\partial \boldsymbol{\beta}_k} = 
  \frac{\partial \ell(\boldsymbol{\beta}; \mathbf{y}, \mathbf{X})}{\partial \boldsymbol{\theta}_k}
  \frac{\partial \boldsymbol{\theta}_k}{\partial \boldsymbol{\eta}_k}
  \frac{\partial \boldsymbol{\eta}_k}{\partial \boldsymbol{\beta}_k},
\end{equation*}
including the derivatives of the log-likelihood with respect to parameters $\boldsymbol{\theta}_k$,
the derivative of the link functions and the derivative of the predictor
$\boldsymbol{\eta}_k$ with respect to coefficients $\boldsymbol{\beta}_k$. Again by chain rule,
the components of $\mathbf{H}_{ks}$ including $\ell(\boldsymbol{\beta}; \mathbf{y}, \mathbf{X})$ can be
written as
\begin{equation} \label{eqn:hessian2}
\mathbf{F}_{ks}(\boldsymbol{\beta}) = \frac{\partial^2 \ell(\boldsymbol{\beta}; \mathbf{y}, \mathbf{X})}{\partial \boldsymbol{\beta}_k \partial \boldsymbol{\beta}_s^\top} =
\left( \frac{\partial \boldsymbol{\eta}_s}{\partial \boldsymbol{\beta}_s} \right)^\top
\frac{\partial^2 \ell(\boldsymbol{\beta}; \mathbf{y}, \mathbf{X})}{\partial \boldsymbol{\eta}_k\partial \boldsymbol{\eta}_s^\top}
\frac{\partial \boldsymbol{\eta}_k}{\partial \boldsymbol{\beta}_k}
\,\, \underbrace{
  \, + \, \frac{\partial \ell(\boldsymbol{\beta}; \mathbf{y}, \mathbf{X})}{\partial \boldsymbol{\eta}_k}
    \frac{\partial^2 \boldsymbol{\eta}_k}{\partial^2 \boldsymbol{\beta}_k}}_{\text{if } k = s},
\end{equation}
where the second term on the right hand side cancels out if all functions (\ref{eqn:functions}) can
be written as a linear combination of a design matrix and coefficients, e.g., when using a basis
function approach. Within the first term, the second derivatives of the log-likelihood involving the
predictors can be written as
\begin{equation} \label{eqn:hessian3}
\frac{\partial^2 \ell(\boldsymbol{\beta}; \mathbf{y}, \mathbf{X})}{\partial \boldsymbol{\eta}_k\partial \boldsymbol{\eta}_s^\top} =
  \frac{\partial \ell(\boldsymbol{\beta}; \mathbf{y}, \mathbf{X})}{\partial \boldsymbol{\theta}_k}
  \frac{\partial^2 \boldsymbol{\theta}_k}{\partial \boldsymbol{\eta}_k \partial \boldsymbol{\eta}_s^\top} + 
  \frac{\partial^2 \ell(\boldsymbol{\beta}; \mathbf{y}, \mathbf{X})}{\partial \boldsymbol{\theta}_k \partial \boldsymbol{\theta}_s^\top}
  \frac{\partial \boldsymbol{\theta}_k}{\partial \boldsymbol{\eta}_k}
  \frac{\partial \boldsymbol{\theta}_s}{\partial \boldsymbol{\eta}_s}
\end{equation}
involving the second derivatives of the link functions.

Using a $k$-partitioned updating scheme as presented in (\ref{eqn:blockupdate}) updating functions
$U_k( \cdot )$ are given by
\begin{equation} \label{eqn:blocknewton}
\boldsymbol{\beta}_k^{(t + 1)} = U_k(\boldsymbol{\beta}_k^{(t)} | \, \cdot) = \boldsymbol{\beta}_k^{(t)} -
  \mathbf{H}_{kk}\left( \boldsymbol{\beta}_k^{(t)} \right)^{-1}\mathbf{s}\left( \boldsymbol{\beta}_k^{(t)} \right).
\end{equation}
Assuming a linear basis function approach for functions (\ref{eqn:functions}) the Hessian matrix
in (\ref{eqn:blocknewton}) is given by
$$
\mathbf{H}_{kk}\left( \boldsymbol{\beta}_k^{(t)} \right) =
\begin{pmatrix}
\mathbf{X}_{1k}^\top\mathbf{W}_{kk}\mathbf{X}_{1k} + \mathbf{G}_{1k}(\boldsymbol{\tau}_{1k}) &
  \cdots & \mathbf{X}_{1k}^\top\mathbf{W}_{kk}\mathbf{X}_{J_kk} \\
\vdots & \ddots & \vdots \\
\mathbf{X}_{J_kk}^\top\mathbf{W}_{kk}\mathbf{X}_{1k} & \cdots & \mathbf{X}_{J_kk}^\top\mathbf{W}_{kk}\mathbf{X}_{J_kk} + \mathbf{G}_{J_kk}(\boldsymbol{\tau}_{J_kk})
\end{pmatrix}^{(t)},
$$
with diagonal weight matrix $\mathbf{W}_{kk} = -\mathrm{diag}(\partial^2 \ell(\boldsymbol{\beta}; \mathbf{y}, \mathbf{X}) /
\partial \boldsymbol{\eta}_k \partial \boldsymbol{\eta}_k^\top)$ and matrices
\begin{equation} \label{eqn:gmatrix}
\mathbf{G}_{jk}(\boldsymbol{\tau}_{jk}) = \frac{\partial^2 \log \,
  p_{jk}(\boldsymbol{\beta}_{jk}; \boldsymbol{\tau}_{jk},
  \boldsymbol{\alpha}_{jk})}{\partial \boldsymbol{\beta}_{jk} \partial \boldsymbol{\beta}_{jk}^\top}.
\end{equation}
Here, we want to emphasize that the influence of these prior derivatives matrices is usually controlled by
$\boldsymbol{\tau}_{jk}$, however, note once again that the $\boldsymbol{\tau}_{jk}$ are held fixed
for the moment and usually estimation cannot be done with maximization of the
log-posterior (see also Section~\ref{sec:modchoice}). Typically, using a linear basis function
representation of functions $f_{jk}( \cdot )$ and priors based on multivariate normal kernels
(\ref{eqn:mvnormprior}) matrices $\mathbf{G}_{jk}(\boldsymbol{\tau}_{jk})$ are a simple product of
smoothing variances and penalty matrices, e.g., with only one smoothing variance
$\mathbf{G}_{jk}(\tau_{jk}) = \tau_{jk}^{-2}\mathbf{K}_{jk}$ with corresponding penalty matrix
$\mathbf{K}_{jk}$.

Similarly, the score vector is
$$
\mathbf{s}\left( \boldsymbol{\beta}_k^{(t)} \right) =
\begin{pmatrix}
\mathbf{X}_{1k}^\top \mathbf{u}_k^{(t)} - \mathbf{G}_{1k}(\boldsymbol{\tau}_{1k})\boldsymbol{\beta}_{1k}^{(t)} \\
\vdots \\
\mathbf{X}_{J_kk}^\top \mathbf{u}_k^{(t)} - \mathbf{G}_{J_kk}(\boldsymbol{\tau}_{J_kk})\boldsymbol{\beta}_{J_kk}^{(t)} \\
\end{pmatrix}
$$
and derivatives $\mathbf{u}_k = \partial \ell(\boldsymbol{\beta}; \mathbf{y}, \mathbf{X}) /
\partial \boldsymbol{\eta}_k$. Focusing on the $j$-th row of (\ref{eqn:blocknewton}) leads to single
model term updating functions $U_{jk}( \cdot )$ as presented in algorithm (\ref{eqn:blockblockupdate}).
The updates are based on an iteratively weighted least squares scheme given by
\begin{equation} \label{eqn:blockbackfit}
  \boldsymbol{\beta}_{jk}^{(t+1)}
  = U_{jk}(\boldsymbol{\beta}_{jk}^{(t)} | \, \cdot ) =
    (\mathbf{X}_{jk}^\top\mathbf{W}_{kk}\mathbf{X}_{jk} +
      \mathbf{G}_{jk}(\boldsymbol{\tau}_{jk}))^{-1}\mathbf{X}_{jk}^\top\mathbf{W}_{kk}(
      \mathbf{z}_k - \boldsymbol{\eta}_{k, -j}^{(t+1)})
\end{equation}
with working observations
$\mathbf{z}_k = \boldsymbol{\eta}_{k}^{(t)} + \mathbf{W}_{kk}^{-1}\mathbf{u}_k^{(t)}$
(in Appendix~\ref{appendix:pmodeiwls} the detailed derivations are presented).
Hence, this leads to a backfitting algorithm and cycling through (\ref{eqn:blockbackfit}) for
terms $j = 1, \ldots, J_k$ and parameters $k = 1, \ldots, K$ is approximate to a single
Newton-Raphson step in (\ref{eqn:newton}), because cross derivatives are not incorporated and
therefore overall convergence will be slower.
Note that this yields the ingredients of the \emph{RS}-algorithm developed in
\citet{bamlss:Rigby+Stasinopoulos:2005} Appendix~B.2. The updating scheme (\ref{eqn:blockbackfit})
can be further generalized to
\begin{equation*} \label{eqn:gbackfit}
\boldsymbol{\beta}_{jk}^{(t + 1)} = U_{jk}\left(\boldsymbol{\beta}_{jk}^{(t)},
  \mathbf{z}_k - \boldsymbol{\eta}_{k, -j}^{(t+1)} | \, \cdot\right)
\end{equation*}
i.e., theoretically any updating function applied on the ``partial residuals''
$\mathbf{z}_k - \boldsymbol{\eta}_{k, -j}^{(t+1)}$ can be used. Note also that this result
is equivalent to updating function
\begin{eqnarray} \label{eqn:blockblocknewton}
\boldsymbol{\beta}_{jk}^{(t + 1)} &=& U_{jk}(\boldsymbol{\beta}_{jk}^{(t)} | \, \cdot) \nonumber \\
 &=& \boldsymbol{\beta}_{jk}^{(t)} -
  \mathbf{H}_{kk}\left( \boldsymbol{\beta}_{jk}^{(t)} \right)^{-1}\mathbf{s}\left( \boldsymbol{\beta}_{jk}^{(t)} \right) \\
 &=& \boldsymbol{\beta}_{jk}^{(t)} -
  \left[\mathbf{F}_{kk}\left( \boldsymbol{\beta}_{jk}^{(t)} \right) +
    \mathbf{G}_{jk}(\boldsymbol{\tau}_{jk})\right]^{-1}\mathbf{s}\left( \boldsymbol{\beta}_{jk}^{(t)} \right), \nonumber
\end{eqnarray}
where matrix $\mathbf{F}_{kk}( \cdot )$ is the derivative matrix given in (\ref{eqn:hessian2}).

For optimization, different strategies of the backfitting
algorithm (\ref{eqn:blockbackfit}) can be applied. One alternative is an inner backfitting algorithm
for each parameter $k$, i.e., the backfitting procedure updates
$\boldsymbol{\beta}_{jk}$, for $j = 1, \ldots, J_k$ until convergence, afterwards updates for
parameters for the next $k$ are calculated again by an inner backfitting algorithm, and so forth
(see also \citealp{bamlss:Rigby+Stasinopoulos:2005}).

Note that for numerical reasons it is oftentimes better to replace the Hessian by the expected
Fisher information with weights
$\mathbf{W}_{kk} = -\mathrm{diag}(E(\partial^2 \ell(\boldsymbol{\beta}; \mathbf{y}, \mathbf{X}) /
\partial \boldsymbol{\eta}_k \partial \boldsymbol{\eta}_k^\top))$,~\citep{bamlss:Klein+Kneib+Lang:2015}.
Moreover, to achieve convergence, algorithms for posterior mode usually initialize the parameter
vectors $\boldsymbol{\theta}_k$. Then, after one inner backfitting iteration the algorithm
can proceed in a full zigzag fashion or again with inner iterations. For all updating schemes it
might also be appropriate to vary the updating step length, possibly in each iteration. This is
relatively straightforward to implement, because step length optimization is a one dimensional
problem.


\subsubsection{Posterior mean} \label{sec:postmean}

The mean of the posterior distribution is
\begin{equation*} \label{eqn:postmean}
E(\boldsymbol{\beta}, \boldsymbol{\tau} | \, \mathbf{y}, \mathbf{X}, \boldsymbol{\alpha}) =
  \int \begin{pmatrix} \boldsymbol{\beta} \\ \boldsymbol{\tau} \end{pmatrix}
  \pi(\boldsymbol{\beta}, \boldsymbol{\tau} ; \mathbf{y}, \mathbf{X}, \boldsymbol{\alpha})
  d\begin{pmatrix} \boldsymbol{\beta} \\ \boldsymbol{\tau} \end{pmatrix}.
\end{equation*}
Clearly, the problem in deriving the expectation, and other quantities like the posterior median,
relies on the computation of usually high-dimensional integrals, which can be rarely solved
analytically and thus need to be approximated by numerical techniques.  

MCMC simulation is commonly used in such situations as it provides an extendable framework that can 
adapt to almost any type of problem. In the following we summarize sampling techniques that
are especially suited within the BAMLSS framework, i.e., techniques that can be used for a highly
modular and extendable system. In this context we describe sampling functions for the updating
scheme presented in (\ref{eqn:blockblockupdate}), i.e., the functions $U_{jk}( \cdot )$ now generate
the next step in a Markov chain.

Note that for some models there exist full conditionals that can be derived in closed form from the
log-posterior (\ref{eqn:logpost}). However, we especially focus on situations were this is not
generally the case. MCMC samples for the regression coefficients $\boldsymbol{\beta}_{jk}$ can be
derived by each of the following methods:
\begin{itemize}
\item \emph{Random-walk Metropolis}: \label{sec:rwm} \\
  Probably the most important algorithm, because of its generality and easy implementation, is
  random-walk Metropolis. The sampler
  proceeds by drawing a candidate $\boldsymbol{\beta}_{jk}^{\star}$ from a symmetric jumping
  distribution $q(\boldsymbol{\beta}_{jk}^{\star}| \, \boldsymbol{\beta}_{jk}^{(t)})$, the
  candidate is then accepted as the new state of the Markov chain with probability
  $$
  \alpha\left( \boldsymbol{\beta}_{jk}^{\star} | \, \boldsymbol{\beta}_{jk}^{(t)}\right) =
  \text{min} \left[ \frac{\pi(\boldsymbol{\beta}_{jk}^{\star} | \, \cdot)}{
    \pi(\boldsymbol{\beta}_{jk}^{(t)} | \, \cdot)}, 1 \right]
  $$
  with the log-posterior $\pi(\boldsymbol{\beta}_{jk} | \, \cdot)$ evaluated at the proposed and
  current value.
  Commonly, the jumping distribution is a normal distribution $\mathcal{N}(\boldsymbol{\beta}_{jk}^{(t)}, \boldsymbol{\Sigma}_{jk})$ centered at the current iterate and fixed covariance matrix. Although this algorithm
  is theoretically working for any distribution, the actual sampling performance depends heavily on
  starting values and the scaling of $\boldsymbol{\Sigma}_{jk}$. Therefore, numerous methods that
  try to optimize the behavior of the Markov chain in an adaptive phase (burnin phase) have been
  developed. In the seminal paper of \citet{bamlss:Gelman+Roberts+Gilks:1996}, strategies that
  optimize the acceptance rate to roughly $1/4$ are suggested to obtain a good mixing (see also
  \citealp{bamlss:Gareth+Roberts+Jeffrey+Rosenthal:2009}). Similarly,
  within the presented modeling framework and a basis function approach with multivariate normal
  prior (\ref{eqn:mvnormprior}), a convenient way is to set
  $\boldsymbol{\Sigma}_{jk} = \sigma_{jk}\mathbf{P}_{jk}(\boldsymbol{\tau}_{jk})^{-1}$ and optimize
  $\sigma_{jk}$ to the desired properties in the adaptive phase.

\item \emph{Derivative based Metropolis-Hastings}: \label{sec:dmh} \\
  A commonly used alternative for the covariance matrix of the jumping distribution
  $\mathcal{N}(\boldsymbol{\beta}_{jk}^{(t)}, \boldsymbol{\Sigma}_{jk})$ is to use the local curvature
  information
  $$
  \boldsymbol{\Sigma}_{jk} = -\left( \frac{\partial^2 \pi(\boldsymbol{\vartheta}; \mathbf{y}, \mathbf{X})}{
    \partial \boldsymbol{\beta}_{jk}\boldsymbol{\beta}_{jk}^\top} \right)^{-1},
  $$
  or its expectation, computed at the posterior mode estimate $\hat{\boldsymbol{\beta}}_{jk}$.
  However, fixing $\boldsymbol{\Sigma}_{jk}$ during MCMC simulation might still lead to undesired
  behavior of the Markov chain especially when iterates move into regions with low probability mass of the
  posterior distribution. A solution is to construct full conditionals
  $\pi(\boldsymbol{\beta}_{jk} | \cdot)$
  which approximate the posterior at the current iterate and minimize the risk of slow traversing.
  The construction of the full conditional is based on a second order Taylor series expansion
  of the log-posterior centered at the last state and yields a multivariate normal distribution
  (see Appendix~\ref{appendix:fullcond1} for a detailed derivation) with precision matrix
  $$
  \left(\boldsymbol{\Sigma}_{jk}^{(t)}\right)^{-1} = -\mathbf{H}_{kk}\left(\boldsymbol{\beta}_{jk}^{(t)}\right)
  $$
  and mean
  \begin{eqnarray*}
  \boldsymbol{\mu}_{jk}^{(t)} &=& \boldsymbol{\Sigma}_{jk}^{(t)}\left[
    s\left(\boldsymbol{\beta}_{jk}^{(t)}\right) -
    \mathbf{H}_{kk}\left(\boldsymbol{\beta}_{jk}^{(t)}\right)\boldsymbol{\beta}_{jk}^{(t)} \right] \\
  &=& \boldsymbol{\beta}_{jk}^{(t)} -
    \mathbf{H}_{kk}\left(\boldsymbol{\beta}_{jk}^{(t)}\right)^{-1}
    \mathbf{s}\left(\boldsymbol{\beta}_{jk}^{(t)}\right) \\
 &=& \boldsymbol{\beta}_{jk}^{(t)} -
  \left[\mathbf{F}_{kk}\left( \boldsymbol{\beta}_{jk}^{(t)} \right) +
    \mathbf{G}_{jk}(\boldsymbol{\tau}_{jk})\right]^{-1}\mathbf{s}\left( \boldsymbol{\beta}_{jk}^{(t)} \right),
  \end{eqnarray*}
  which is equivalent to the updating function given in (\ref{eqn:blockblocknewton}).
  Hence, the mean is simply one Newton or Fisher scoring iteration towards the posterior mode at
  the current step.
  The proposal density for $\boldsymbol{\beta}_{jk}$ then is
  $q(\boldsymbol{\beta}_{jk}^\star | \boldsymbol{\beta}_{jk}^{(t)}) =
    \mathcal{N}(\boldsymbol{\mu}_{jk}^{(t)}, \boldsymbol{\Sigma}_{jk}^{(t)})$ and the acceptance probability
  of the candidate is then computed by
  $$
  \alpha\left( \boldsymbol{\beta}_{jk}^{\star} | \, \boldsymbol{\beta}_{jk}^{(t)}\right) =
  \text{min} \left[ \frac{\pi(\boldsymbol{\beta}_{jk}^{\star} | \, \cdot)q(\boldsymbol{\beta}_{jk}^{(t)} | \, \boldsymbol{\beta}_{jk}^\star)}{
    \pi(\boldsymbol{\beta}_{jk}^{(t)} | \, \cdot)q(\boldsymbol{\beta}_{jk}^\star | \, \boldsymbol{\beta}_{jk}^{(t)})  }, 1 \right].
  $$
  Again, assuming a basis function approach for functions $f_{jk}( \cdot )$ the precision matrix is
  $$
  \left(\boldsymbol{\Sigma}_{jk}^{(t)}\right)^{-1} =
    \mathbf{X}_{jk}^\top\mathbf{W}_{kk}\mathbf{X}_{jk} + \mathbf{G}_{jk}(\boldsymbol{\tau}_{jk}),
  $$
  with weights $\mathbf{W}_{kk} = -\mathrm{diag}(\partial^2 \ell(\boldsymbol{\beta}; \mathbf{y}, \mathbf{X}) /
\partial \boldsymbol{\eta}_k \partial \boldsymbol{\eta}_k^\top)$, or the corresponding expectation.
  The mean can be written as
  \begin{equation*}
  \boldsymbol{\mu}_{jk}^{(t)} =
    \boldsymbol{\Sigma}_{jk}^{(t)}\mathbf{X}_{jk}^\top\mathbf{W}_{kk}\left(\mathbf{z}_k  -
      \boldsymbol{\eta}^{(t)}_{k,-j}\right)
  \end{equation*}
  with working observations
  $\mathbf{z}_k = \boldsymbol{\eta}_k^{(t)} + \mathbf{W}_{kk}^{-1}\mathbf{u}_k^{(t)}$
  (see again Appendix~\ref{appendix:fullcond1} for a detailed derivation). Note again,
  the computation of the mean is equivalent to a full Newton step as given in updating function
  (\ref{eqn:blockblocknewton}), or Fisher scoring when using
  $-E(\partial^2 \ell(\boldsymbol{\beta}; \mathbf{y}, \mathbf{X}) /
    \partial \boldsymbol{\eta}_k \partial \boldsymbol{\eta}_k^\top)$,
  in each iteration of the MCMC sampler using iteratively weighted least squares (IWLS)
  \citep{bamlss:Gamerman:1997, bamlss:Brezger+Lang:2006, bamlss:Klein+Kneib+Klasen+Lang:2015}. If
  the computation of the weights $\mathbf{W}_{kk}$ is expensive, one simple strategy is to update
  $\mathbf{W}_{kk}$ only after samples of all parameters of $\boldsymbol{\theta}_k$ are drawn.

\item \emph{Slice sampling}: \label{sec:smcmc} \\
  Slice sampling \citep{bamlss:Neal:2003} is a gradient free MCMC sampling scheme that produces
  samples with $100\%$ acceptance rate. Therefore, and because of the simplicity of the algorithm,
  slice sampling is especially useful for automated general purpose MCMC implementations that allow
  for sampling from many distributions. The basic slice
  sampling algorithm samples univariate directly under the plot of the log-posterior
  (\ref{eqn:logpost}). Updates for the $i$-th parameter in $\boldsymbol{\beta}_{jk}$ are
  generated by:
  \begin{enumerate}
  \item Sample $h \sim \mathcal{U}(0, \pi(\beta_{ijk}^{(t)} | \, \cdot))$.
  \item Sample $\beta_{ijk}^{(t+1)} \sim \mathcal{U}(S)$ from the horizontal slice
    $S = \{\beta_{ijk}: h < \pi(\beta_{ijk} | \, \cdot)\}$.
  \end{enumerate}
\end{itemize}

The full conditional $\pi(\boldsymbol{\tau}_{jk} | \, \cdot)$ for smoothing variances is commonly
constructed using priors for $\boldsymbol{\tau}_{jk}$ that lead to known distributions, i.e.,
simple Gibbs sampling is possible. E.g., this is the case when using a linear basis function
approach and only one smoothing variance $\tau_{jk}$ is assigned. Then, by using an
inverse Gamma prior (\ref{eqn:igprior}) for $\tau_{jk}$ in combination with the normal prior
(\ref{eqn:mvnormprior}) for $\boldsymbol{\beta}_{jk}$ the full-conditional $\pi(\tau_{jk} | \, \cdot)$
is again an inverse Gamma distribution with
$$
\tilde{a}_{jk} = \frac{1}{2}rk(\mathbf{K}_{jk}) + a_{jk}, \qquad
  \tilde{b}_{jk} = \frac{1}{2}(\boldsymbol{\beta}_{jk}^\star)^\top\mathbf{K}_{jk}
  \boldsymbol{\beta}_{jk}^\star + b_{jk},
$$
and matrix $\mathbf{K}_{jk}$ is again a penalty matrix that depends on the type of model term.
As mentioned in Section \ref{sec:priorsbricks}, other priors than the inverse Gamma might be
desirable. Then,  Metropolis-Hastings steps also for the variances can be constructed,
see~\citet{bamlss:Klein+Kneib:2016} for details. If a simple Gibbs sampling step cannot be
derived, e.g., for multi-dimensional tensor product splines, another strategy is to use slice
sampling, since the number of smoothing variances is usually not very large the computational
burden does most of the times not exceed possible benefits.


\subsection{Model choice} \label{sec:modchoice}

\subsubsection{Model fit} \label{sec:modfit}

Quantile residuals defined as
$\hat{r}_i = \Phi^{-1}(\mathbf{\mathcal{F}}(y_i | \, \hat{\boldsymbol{\theta}}_{i}))$ with the inverse 
cumulative distribution function of a standard normal distribution $\Phi^{-1}$ and
$\mathbf{\mathcal{F}}( \cdot )$ denoting the cumulative distribution
function (CDF) of the modeled distribution $\mathcal{D}( \cdot )$ with estimated
parameters $\hat{\boldsymbol{\theta}}_{i} = (\hat{\theta}_{i1}, \ldots, \hat{\theta}_{iK})^\top$
plugged in, should at least approximately be standard normally distributed if the correct model has
been specified \citep{bamlss:Dunn:Smyth:1996}. Resulting residuals can be assessed graphically in
terms of quantile-quantile-plots. Strong deviations from the diagonal line are then a sign for an
inappropriate model fit. Instead of looking at residuals one can use the 
probability integral transform (PIT) which considers
$u_i = \mathbf{\mathcal{F}}(y_i | \, \hat{\boldsymbol{\theta}}_i)$ without applying the inverse
standard normal CDF. If the estimated model is a good approximation to the true data generating 
process, the $u_i$ will then approximately follow a uniform distribution on $[0, 1]$.
Graphically, histograms of the $u_i$ can be used for this purpose.


\subsubsection{Smoothing variances with posterior mode} \label{sec:smoothvarsel}

As already mentioned in Section~\ref{sec:bricksmodelfit}, depending on the structure of the priors
(\ref{eqn:gprior}) parameters $\boldsymbol{\tau}_{jk}$ cannot be estimated by maximization of the
log-posterior (\ref{eqn:logpost}). For example, this is the case for GAM-type models in
combination with priors based on multivariate normal kernels (\ref{eqn:mvnormprior}) where
$\boldsymbol{\tau}_{jk}$ represents smoothing variances.

Therefore, goodness of fit criteria like the Akaike information criterion (AIC), or the corrected AIC,
as well as the Bayesian information criterion (BIC), amongst others, are commonly used for selecting
the smoothing variances $\boldsymbol{\tau}_{jk}$. These criteria try to penalize overly complex models,
i.e., are trying to prevent over-fitting. For models using a linear basis function approach,
estimating model complexity using (possibly) nonlinear functions is based on the so called
equivalent degrees of freedom (EDF). For each model component the EDF used to estimate the function
are calculated by
$$
\mathrm{edf}_{jk}(\boldsymbol{\tau}_{jk}) := \mathrm{trace}\left[
  \mathbf{F}_{kk}(\boldsymbol{\beta}_{jk})(
  \mathbf{F}_{kk}(\boldsymbol{\beta}_{jk}) + \mathbf{G}_{jk}(\boldsymbol{\tau}_{jk}))^{-1} \right],
$$
where $\mathbf{F}_{kk}( \cdot )$ is the derivative matrix given in (\ref{eqn:hessian2}) and
matrix $\mathbf{G}_{jk}(\boldsymbol{\tau}_{jk})$ is the prior
derivative matrix as given in (\ref{eqn:gmatrix}). The
total degrees of freedom used to fit the model are then estimated by
$\sum_{k}\sum_{j} \mathrm{edf}_{jk}(\boldsymbol{\tau}_{jk})$. Note that the definition
of EDF here is slightly more general and is usually defined as the
trace of the smoother matrix (see, e.g., \citealp{bamlss:Hastie+Tibshirani:1990}) and can be applied
even for more complex likelihood structures, e.g., in a flexible Cox-model \citep{bamlss:Hofner:2008}.

Instead of global optimization of smoothing variances, a fast strategy is the stepwise selection
during backfitting, which is proposed by \citet{bamlss:Belitz+Lang:2008} and can be adapted to
distributional regression models. In each cycle of the backfitting
algorithm (\ref{eqn:blocknewton}) or (\ref{eqn:blockbackfit}), the smoothing variances for each
model component are optimized holding all other variances fixed, see Algorithm~\ref{fig:umode}. To
further speed up convergence, the search space for smoothing variances can be based on a small grid
of candidate values. This will not lead to a global optimum according the goodness of fit
criterion, however, the resulting parameters serve as good starting values for full Bayesian
inference with MCMC. This is especially useful for multi-dimensional effects with multiple
smoothing variances since convergence of the MCMC chains will usually take much longer with
improper starting values.


\subsubsection{Variable selection with posterior mean} \label{sec:varselpostmean}

The deviance information criterion (DIC) can be used for model choice and variable selection in
Bayesian inference. It is easily be computed from the MCMC output without requiring additional
computational effort. If $\boldsymbol{\beta}^{(1)}, \ldots, \boldsymbol{\beta}^{(T)}$ is a
MCMC sample from the posterior for the complete parameter vector $\boldsymbol{\beta}$, the
DIC is given by
$\overline{D(\boldsymbol{\beta})} + \mathrm{pd} = 2 \overline{D(\boldsymbol{\beta})} -
D(\overline{\boldsymbol{\beta}}) =
\tfrac{2}{T}\sum D(\boldsymbol{\beta}^{(t)}) - D(\tfrac{1}{T}\sum\boldsymbol{\beta}^{(t)})$
where $D(\boldsymbol{\beta}) = -2 \cdot \ell(\boldsymbol{\beta}; \mathbf{y}, \mathbf{X})$
is the model deviance and $\mathrm{pd} = \overline{D(\boldsymbol{\beta})} -
D(\overline{\boldsymbol{\beta}})$ is an effective parameter count.


\subsection{Inference and prediction} \label{sec:infpred}

Under suitable regularity conditions inference for parameters $\boldsymbol{\beta}_{jk}$ can be based
on the asymptotic normality of the posterior distribution
$$ \label{eqn:asynorm}
\boldsymbol{\beta}_{jk} \, | \, \mathbf{y} \overset{a}{\sim} \mathcal{N}\left(\hat{\boldsymbol{\beta}}_{jk},
  \mathbf{H}(\hat{\boldsymbol{\beta}}_{jk})^{-1}\right),
$$
with $\hat{\boldsymbol{\beta}}_{jk}$ as the posterior mode estimate. However, this approach is
problematic since it does not take into account the uncertainty of estimated smoothing parameters.
Moreover, from a computational perspective it can be difficult to derive the full Hessian
information, because this might involve complex cross derivatives of the parameters and there
are cases where standard numerical techniques cannot be applied (see Section~\ref{sec:coxreg}).

Instead, applying fully Bayesian inference is relatively easy by direct computation of desired
statistics from posterior samples. Computational costs are relatively low, since only samples for
parameters $\boldsymbol{\beta}_{jk}$ and $\boldsymbol{\tau}_{jk}^2$ need to be saved (in practice
about 2000--3000 are sufficient) from which inference of any combination of terms is straightforward,
too.

The posterior predictive distribution is approximated similarly. Random samples for response
observations given new covariate values $\mathbf{x}^\star$ are computed by drawing samples
from the response distribution
\begin{equation*} \label{eqn:pdist}
y^\star \sim \mathbf{\mathcal{D}}\left(h_{1}(\theta_{1}) = \eta_1(\mathbf{x}^\star; \boldsymbol{\beta}_1^{(t)}),
  \,\, \dots, \,\, h_{K}(\theta_{K}) = \eta_K(\mathbf{x}^\star; \boldsymbol{\beta}_K^{(t)})\right)
\end{equation*}
for each posterior sample $\boldsymbol{\beta}_k^{(t)}$; $k = 1, \dots, K$, $t = 1, \ldots, T$.


\section{BAMLSS algorithms} \label{sec:algorithms}

Using flexible partitioned updating systems like (\ref{eqn:blockupdate}) and
(\ref{eqn:blockblockupdate}), from Section~\ref{sec:priorsbricks} to \ref{sec:infpred}, it can be
recognized that the following ``Lego bricks'' are repeatedly used within the BAMLSS framework:
\begin{itemize}
\item The log-likelihood function $\ell(\boldsymbol{\beta}; \mathbf{y}, \mathbf{X})$,
\item link functions $h_k( \cdot )$ and derivatives of inverse link functions,
\item the first order derivatives $\frac{\partial \ell(\boldsymbol{\beta}; \mathbf{y}, \mathbf{X})}{\partial \boldsymbol{\beta}_{jk}}$ and $\frac{\partial \boldsymbol{\eta}_k}{\partial \boldsymbol{\beta}_{jk}}$,
\item the second order derivatives $\frac{\partial^2 \ell(\boldsymbol{\beta}; \mathbf{y}, \mathbf{X})}{\partial \boldsymbol{\beta}_{jk} \partial \boldsymbol{\beta}_{jk}^\top}$,
\item derivatives for log-priors, e.g., $\frac{\partial \log \, p_{jk}(\boldsymbol{\beta}_{jk};
  \boldsymbol{\tau}_{jk}, \boldsymbol{\alpha}_{jk})}{\partial \boldsymbol{\beta}_{jk}}$.
\end{itemize}
As shown in Section~\ref{sec:bricksmodelfit}, first and second order derivatives of the log-likelihood
w.r.t.\ $\boldsymbol{\beta}_{jk}$ can in most cases be computed by fragmenting with the chain rule into
derivatives of the log-likelihood w.r.t.\ parameters $\boldsymbol{\theta}_k$, derivatives of link
functions w.r.t.\ the linear predictors $\boldsymbol{\eta}_k$ and derivatives of
$\boldsymbol{\eta}_k$ w.r.t.\ the regression coefficients $\boldsymbol{\beta}_{jk}$. Hence, most of the
structures can be reused when implementing new models and/or algorithms, i.e., applying the
backfitting algorithm (\ref{eqn:blockbackfit}) and derivative based MCMC usually only requires the
calculation of derivatives of the log-likelihood w.r.t.\ parameters $\boldsymbol{\theta}_k$ anew.

Therefore, even though the models can be rather complex, it is possible to construct a quite simple
modular system from which it is easy to create algorithms for numerous regression problems.
Moreover, using the ideas of partitioned updating as presented in Section~\ref{sec:modelfit} for
posterior mode estimation or MCMC, a simple generic algorithm for BAMLSS can be constructed, which
is outlined in Algorithm~\ref{fig:algodesign}.
\begin{algorithm}[!h]
\caption{\label{fig:algodesign} Generic BAMLSS model fitting algorithm.}
\begin{algorithmic}
\renewcommand{\algorithmicrequire}{\textbf{Input:}}
\REQUIRE{$\mathbf{y}$, $\mathbf{X}$, $\boldsymbol{\alpha}$.}
\renewcommand{\algorithmicrequire}{\textbf{Initialize:}}
\renewcommand{\algorithmicensure}{\textbf{Set:}}
\ENSURE{Stopping criterion $\varepsilon$, number of iterations $T$, e.g., $\varepsilon = 0.0001$, $T = 1000$.}
\REQUIRE{$\boldsymbol{\beta}$, $\boldsymbol{\eta}$, $\boldsymbol{\tau}$, e.g.,
  $\boldsymbol{\beta} = \mathbf{0}, \boldsymbol{\tau} = 0.001 \cdot \mathbf{1}$,
  $\Delta = \varepsilon + 1$, $t = 1$.}
\WHILE{$(\Delta > \varepsilon) \, \& \, (t < T)$}
  \STATE Set $\mathring{\boldsymbol{\eta}} = \boldsymbol{\eta}^{(t)}$.
  \FOR{$k = 1$ to $K$}
    \FOR{$j = 1$ to $J_k$}
      \STATE Obtain new state $(\boldsymbol{\beta}_{jk}^{(t + 1)}, \boldsymbol{\tau}_{jk}^{(t + 1)}) \leftarrow
        U_{jk}(\boldsymbol{\beta}_{jk}^{(t)}, \boldsymbol{\tau}_{jk}^{(t)} | \, \cdot \,)$.
      \STATE Compute $\mathbf{f}_{jk}^{(t + 1)} \leftarrow f_{jk}(\mathbf{X}_{jk}, \boldsymbol{\beta}_{jk}^{(t + 1)})$.
      \STATE Update $\boldsymbol{\eta}_k^{(t + 1)} \leftarrow \boldsymbol{\eta}_k^{(t)} - \mathbf{f}_{jk}^{(t)} + \mathbf{f}_{jk}^{(t + 1)}$.
    \ENDFOR
  \ENDFOR
  \STATE Compute $\Delta \leftarrow \text{rel.change}(\mathring{\boldsymbol{\eta}}, \boldsymbol{\eta}^{(t + 1)})$.
  \STATE Increase $t \leftarrow t + 1$.
\ENDWHILE
\renewcommand{\algorithmicensure}{\textbf{Output:}}
\ENSURE{Posterior mode estimates $\hat{\boldsymbol{\beta}} = \boldsymbol{\beta}^{(t)}$,
  $\hat{\boldsymbol{\tau}} = \boldsymbol{\tau}^{(t)}$; \\
  or MCMC samples $\boldsymbol{\beta}^{(t)}$, $\boldsymbol{\tau}^{(t)}$, $t = 1, \ldots, T$.}
\end{algorithmic}
\end{algorithm}
Here, the updating functions $U_{jk}( \cdot )$ either create a new state in posterior maximization or
a Markov chain. In practice, for full Bayesian inference the algorithm is applied twice, i.e.,
first computing posterior mode estimates before running a MCMC simulation with these estimates
as starting values. Finding good starting values is especially important for complex model terms
that, e.g., include multiple smoothing variances. In Algorithm~\ref{fig:umode}, the stepwise selection
of smoothing variances, similar to \citet{bamlss:Belitz+Lang:2008}, is shown.
\begin{algorithm}[!h]
\caption{\label{fig:umode} Posterior mode updating $U_{jk}( \cdot )$ with smoothing variance selection.}
\begin{algorithmic}
\renewcommand{\algorithmicrequire}{\textbf{Input:}}
\renewcommand{\algorithmicensure}{\textbf{Set:}}
\REQUIRE{$\mathbf{y}$, $\mathbf{X}$, $\boldsymbol{\alpha}$, $\boldsymbol{\beta}^{(t)}$, $\boldsymbol{\tau}^{(t)}$.}
\ENSURE{Goodness of fit criterion $C$.}
\FOR{$l = 1$ to $L_{jk}$}
  \STATE Set search interval for $\tau_{ljk}^{(t + 1)}$, e.g., $\mathcal{I} = [\tau_{ljk}^{(t)} \cdot 10^{-1}, \tau_{ljk}^{(t)} \cdot 10]$.
  \STATE Find $\tau_{ljk}^{(t + 1)} \leftarrow
    \underset{\tau_{ljk}^\star \in \mathcal{I}}{\text{arg min }} C(U_{jk}(\boldsymbol{\beta}_{jk}^{(t)}, \tau_{ljk}^\star | \, \cdot))$.
\ENDFOR
\STATE Update $\boldsymbol{\beta}_{jk}^{(t + 1)} \leftarrow U_{jk}(\boldsymbol{\beta}_{jk}^{(t)}, \boldsymbol{\tau}_{jk}^{(t + 1)} | \, \cdot)$.
\renewcommand{\algorithmicensure}{\textbf{Output:}}
\ENSURE{Updates $\boldsymbol{\beta}_{jk}^{(t + 1)}$, $\boldsymbol{\tau}_{jk}^{(t + 1)}$.}
\end{algorithmic}
\end{algorithm}
In each step of Algorithm~\ref{fig:algodesign}, each of the
$\boldsymbol{\tau}_{jk} = (\tau_{1jk}, \ldots, \tau_{L_{jk}jk})^\top$ smoothing variances is
optimized one after the other using an adaptive search interval. Hence, the optimization problem
is reduced to a one-dimensional search that is relative straightforward to implement. The algorithm
does not guarantee a global minimum given the goodness of fit criterion, however, the solution
is at least close and serves as good starting points for full MCMC. The MCMC updating functions
usually either accept or reject samples of the parameters and smoothing variances are sampled
after $\boldsymbol{\beta}_{jk}$. In Algorithm~\ref{fig:mcmc} the general sampling scheme is shown.
\begin{algorithm}[!h]
\caption{\label{fig:mcmc} MCMC updating $U_{jk}( \cdot )$.}
\begin{algorithmic}
\renewcommand{\algorithmicrequire}{\textbf{Input:}}
\renewcommand{\algorithmicensure}{\textbf{Set:}}
\REQUIRE{$\mathbf{y}$, $\mathbf{X}$, $\boldsymbol{\alpha}$, $\boldsymbol{\beta}^{(t)}$, $\boldsymbol{\tau}^{(t)}$.}
\ENSURE{Sampling method, e.g., derivative based MCMC.}
\STATE Sample $\boldsymbol{\beta}_{jk}^\star \leftarrow q(\boldsymbol{\beta}_{jk}^\star | \,\boldsymbol{\beta}_{jk}^{(t)})$.
\STATE Compute acceptance probability $\alpha\left( \boldsymbol{\beta}_{jk}^{\star} | \, \boldsymbol{\beta}_{jk}^{(t)}\right)$.
\IF{uniform draw $U(0, 1) \leq \alpha\left( \boldsymbol{\beta}_{jk}^{\star} | \, \boldsymbol{\beta}_{jk}^{(t)}\right)$}
    \STATE $\boldsymbol{\beta}_{jk}^{(t + 1)} \leftarrow \boldsymbol{\beta}_{jk}^{\star}$
  \ELSE
    \STATE $\boldsymbol{\beta}_{jk}^{(t + 1)} \leftarrow \boldsymbol{\beta}_{jk}^{(t)}$.
  \ENDIF
\STATE Generate $\boldsymbol{\tau}_{jk}^{(t + 1)}$ similarly as above.
\renewcommand{\algorithmicensure}{\textbf{Output:}}
\ENSURE{Next state $\boldsymbol{\beta}_{jk}^{(t + 1)}$, $\boldsymbol{\tau}_{jk}^{(t + 1)}$.}
\end{algorithmic}
\end{algorithm}

In summary, building algorithms for BAMLSS is practically based on combining updating functions
which are itself based on repeated ``Lego bricks''. Hence, once the infrastructure is in place, it
is relatively easy to implement algorithms for a variety of models. In Table~\ref{tab:mlegobricks}
we further exemplify the modular structure for model term specifications using commonly used
priors for regression coefficients $\boldsymbol{\beta}_{jk}$ and smoothing variances $\boldsymbol{\tau}_{jk}$.
\begin{sidewaystable}
\centering
\begin{tabular}{l|l|ll}
\multicolumn{1}{c}{Covariates} & \multicolumn{1}{c}{Effect types $f_{jk}(\mathbf{x}; \boldsymbol{\beta}_{jk})$} & \multicolumn{2}{c}{Prior densities $p_{jk}(\boldsymbol{\beta}_{jk}; \boldsymbol{\tau}_{jk}, \boldsymbol{\alpha}_{jk})$} \\
& &
  \multicolumn{1}{c}{$d_{\boldsymbol{\beta}}(\boldsymbol{\beta} | \, \boldsymbol{\tau};
     \boldsymbol{\alpha}_{\boldsymbol{\beta}})$} &
  \multicolumn{1}{c}{$d_{\boldsymbol{\tau}}(\boldsymbol{\tau} | \, \boldsymbol{\alpha}_{\boldsymbol{\tau}})$} \\ \hline
\multirow{6}{*}{Scalar covariates} & Intercept $\beta$ & \multirow{2}{*}{$\propto$ const} & \multicolumn{1}{c}{\multirow{3}{*}{$\emptyset$}} \\
& Linear effect $x \cdot \beta$ &
  \multirow{2}{*}{$\propto \exp \left(- \frac{1}{2}(\boldsymbol{\beta} -
    \mathbf{m})^{\top}\mathbf {P}(\boldsymbol{\tau})(\boldsymbol{\beta} -
    \mathbf{m})\right)$}
                      & \\
& Linear interaction $x_1 \cdot x_2 \cdot \beta$ & & \\ \cline{2-4}
& Smooth effect $f(x)$ & \multirow{8}{*}{$\propto |\mathbf{P}(\boldsymbol{\tau})|^{\frac{1}{2}} \exp\left(
    -\frac{1}{2}\boldsymbol{\beta}^\top\mathbf{P}(\boldsymbol{\tau})\boldsymbol{\beta}\right)$} &
    \multirow{2}{*}{IG $\propto (\tau^2)^{-(a + 1)} \exp(-b / \tau^2)$} \\
& Varying coefficient $f(x_2) \cdot x_1$  & & \\
& Smooth effect $f(x_1, \ldots, x_L)$  & & \multirow{2}{*}{HC $\propto (1 + \tau^2/a^2)^{-1}(\tau^2/a^2)^{-1/2}$} \\ \cline{1-1}
\multirow{2}{*}{Grouping variable $s$} & Random intercept $\beta_s$ & &  \\
& Spatial effect $f(s)$ & & \multirow{2}{*}{SD $\propto (\tau^2/\tau)^{-1/2} \exp(-(\tau^2/a)^{1/2})$} \\ \cline{1-1}
\multirow{2}{*}{Grouping and scalar,} & Random slope $x \cdot \beta_s$ & & \\
\multirow{2}{*}{time variable $t$}& Space-time effect $f(s, t)$ & & \multirow{2}{*}{HN $\propto (\tau^2)^{1/2 -1} \exp(-\tau^2/(2a^2))$} \\
& Functional random intercept $f_s(t)$ & & \\
\end{tabular}
\caption{\label{tab:mlegobricks} Commonly used ``Lego bricks'' for model terms in BAMLSS. Priors for
  linear effects assume that the precision matrix $\mathbf{P}(\boldsymbol{\tau})$ is fixed. For smooth
  effects, prior densities are: Inverse Gamma (IG), half-Cauchy (HC), scale-dependent (SD) and
  half-normal (HN).}
\end{sidewaystable}


\section{Strategies for implementation} \label{sec:strategies}

Today, there exist a couple of implementations supporting distributional regression models. The
\proglang{R} packages \pkg{gamlss} and \pkg{VGAM} are probably the best known for flexible modeling
in an empirical Bayes setting. Besides, general purpose Bayesian modeling software like \pkg{WinBUGS},
\pkg{JAGS} and \pkg{Stan} play a prominent role in the applied statistician's toolbox. Whereas the
former emphasize on GAM-type additive models, the latter in principle allow for any type
of model. This huge amount of flexibility is of course not without its drawbacks, e.g., estimation
time is usually quite long especially when using large data sets. One of the most efficient
software packages for Bayesian GAM-type distributional regression models, but with less flexibility on the
model term side, is the standalone software \pkg{BayesX} with corresponding \proglang{R} interface
\pkg{R2BayesX}. More recently, the \proglang{R} package \pkg{mgcv} started to support general
smooth models \citep{bamlss:Wood+Pya+Saefken:2016} and additionally implements an interface to
\pkg{JAGS} for estimating GAMs \citep{bamlss:Wood:2016b}.

While the possibility for implementing new distributions is supported by all of the above software
packages, options for implementing new model term types are more limited. The \pkg{mgcv}
package provides very generic smooth term constructor methods for creating model terms based on
a basis function approach. Package \pkg{gamlss} is slightly more flexible since the user can
implement not only his own smooth terms, but also smooth fitting updating functions. On the other
hand, using general purpose Bayesian modeling software the user can in principle incorporate all
kinds of different model terms, however, the implementation is usually a bit tedious, i.a.

Whereas model term flexibility is a major focus, there is less attention on the algorithmic side.
Usually model fitting algorithms can be chosen only from a small predefined list. In contrast, this
paper aims to contribute to full algorithmic flexibility by presenting iterative estimation
algorithms with exchangeable updating functions. Moreover, the structures exploited in the last
three sections suggest an unified modeling architecture that is useful for building algorithms and
writing interfaces to existing implementations.

In order to unify the multitude of possibilities, a general software system for BAMLSS and beyond
adresses the following:
\begin{enumerate}
\item Unified model description,
\item a generic method for setting up model terms and corresponding prior structures,
\item support of exchangeable updating functions or even model fitting engines,
\item standard post modeling extractor functions.
\end{enumerate}
While points 2--4 are primarily software specific issues, the first point is more general. For
setting up all necessary design and penalty matrices, as well as prior structures, a symbolic
model definition is needed. A concept for symbolic model description is described in the next
section. Section~\ref{sec:architecture} then describes a general architecture for implementing
BAMLSS and beyond.


\subsection{Symbolic descriptions} \label{sec:symdesc}

Based on \citet{bamlss:Wilkinson+Rogers:1973} symbolic descriptions for specifying models have been
implemented for various computer programs. The statistical environment \proglang{R} provides such
a syntax (see also \citealp{bamlss:Chambers+Hastie:1992}), which is familiar to almost any common
\proglang{R} user today. Without such specifications, that in the end translate model formulae into
model frames, the estimation of regression models is very circumstantial, especially in the case of
additive predictors (\ref{eqn:addpred}). Therefore, the \proglang{R} model formula
language is also extensible. The recommended package \pkg{mgcv}~\citep{bamlss:Wood:2016} for
estimating GAMs additionally provides the generic descriptor \code{s()} for smooth terms, amongst
others. However, to conveniently specify the models presented in Section~\ref{sec:model}, a
slightly enhanced syntax is required.

Hereinafter, we follow the notation of the \proglang{R} formula language and denote
smooth and random effect terms with the \code{s()} descriptor. A typical linear regression model 
with a response variable \code{y} and covariates \code{x1} and \code{x2} is then represented by
\begin{center}
\code{y} $\sim$ \code{x1 + x2}
\end{center}
A model with two additional nonlinear modeled terms of covariates \code{z1}, \code{z2} and \code{z3}
is set up with
\begin{center}
\begin{tabular}{l}
\code{y} $\sim$ \code{x1 + x2 + s(z1) + s(z2, z3)}
\end{tabular}
\end{center}
However, in the context of distributional regression we need formula extensions for multiple
parameters. A convenient way to specify, e.g., the parameters of a normal model with
$y~\sim~\mathcal{N}(\mu = \eta_{\mu}, log(\sigma) = \eta_{\sigma})$ is given by
\begin{center}
{ \renewcommand{\arraystretch}{1}
\begin{tabular}{l}
\code{list(} \\
$\quad$ \code{y} $\sim$ \code{x1 + x2 + s(z1) + s(z2),} \\
$\quad$ \code{sigma} $\sim$ \code{x1 + x2 + s(z1)} \\
\code{)}
\end{tabular}
}
\end{center}
i.e., two formulas are provided where the first represents the description of the mean $\mu$
and the second of the scale parameter $\sigma$. Furthermore, the two formulas
are symbolically connected by a list of formulas that is send to the subsequent processor. This way
any number of parameters can be easily specified, e.g., a four parameter example is
\begin{center}
{ \renewcommand{\arraystretch}{1}
\begin{tabular}{l}
\code{list(} \\
$\quad$ \code{y} $\sim$ \code{x1 + x2 + s(z1) + s(z2),} \\
$\quad$ \code{sigma} $\sim$ \code{x1 + x2 + s(z1),} \\
$\quad$ \code{nu} $\sim$ \code{s(z1),} \\
$\quad$ \code{tau} $\sim$ \code{s(z2)} \\
\code{)}
\end{tabular}
}
\end{center}
A convention we make at this point is that the first formula is always the one including the
response variable and all other formulas have the corresponding parameter name on the left hand
side. Hence, a mapping of terms with parameters is provided.

Models with categorical responses can be formulated in a similar fashion. A model
with three categories within the \code{y} variable, e.g., a multinomial model with some
reference category can be defined by
\begin{center}
{ \renewcommand{\arraystretch}{1}
\begin{tabular}{l}
\code{list(} \\
$\quad$ \code{y} $\sim$ \code{x1 + s(z1) + s(z2),} \\
$\quad$ $\sim$ \code{x1 + x2 + s(z1) + s(z3)} \\
\code{)}
\end{tabular}
}
\end{center}
where all subsequent formulas do not need a left hand side. The only additional assumption here is
that the order of the formulas represents the order of the categories. Another option is to specify
the formulas of each category explicitly by
\begin{center}
{ \renewcommand{\arraystretch}{1}
\begin{tabular}{l}
\code{list(} \\
$\quad$ \code{y1} $\sim$ \code{x1 + s(z1) + s(z2),} \\
$\quad$ \code{y2} $\sim$ \code{x1 + x2 + s(z1) + s(z3)} \\
\code{)}
\end{tabular}
}
\end{center}
i.e., variable \code{y1} is a dummy variable indicating whether the $i$-th observation is in
category 1 and \code{y2} in category 2, respectively.

In summary, the described model definition syntax does not restrict to any type of regression
model or number of parameters.


\subsection{Architecture} \label{sec:architecture}

A possible architecture for the BAMLSS framework is illustrated in Figure~\ref{fig:blocks}.
\begin{figure}[ht!]
\centering
\setlength{\unitlength}{1cm}
\setlength{\fboxsep}{0pt}
\begin{picture}(10.53, 5.7)(0, 0)
\put(0, 5){\fcolorbox{black}{heat5}{\framebox(2, 0.7)[c]{\footnotesize \texttt{formula}}}}
\put(2.5, 5){\fcolorbox{black}{heat5}{\framebox(2, 0.7)[c]{\footnotesize \texttt{family}}}}
\put(5, 5){\fcolorbox{black}{heat5}{\framebox(2, 0.7)[c]{\footnotesize \texttt{data}}}}
\put(2.5, 3.5){\fcolorbox{black}{heat4}{\framebox(2, 0.7)[c]{\footnotesize \texttt{model.frame}}}}
\put(2.5, 2.5){\fcolorbox{black}{heat3}{\framebox(2, 0.7)[c]{\footnotesize \texttt{transformer}}}}
\put(6, 3.5){\fcolorbox{black}{heat2}{\framebox(2, 0.7)[c]{\footnotesize \texttt{optimizer}}}}
\put(6, 2.5){\fcolorbox{black}{heat1}{\framebox(2, 0.7)[c]{\footnotesize \texttt{sampler}}}}
\put(6, 1.5){\fcolorbox{black}{heat2}{\framebox(2, 0.7)[c]{\footnotesize \texttt{results}}}}
\put(1, 0){\fcolorbox{black}{heat5}{\framebox(2, 0.7)[c]{\footnotesize \texttt{summary}}}}
\put(3.5, 0){\fcolorbox{black}{heat5}{\framebox(2, 0.7)[c]{\footnotesize \texttt{plot}}}}
\put(6, 0){\fcolorbox{black}{heat5}{\framebox(2, 0.7)[c]{\footnotesize \texttt{select}}}}
\put(8.5, 0){\fcolorbox{black}{heat5}{\framebox(2, 0.7)[c]{\footnotesize \texttt{predict}}}}
\put(3.5, 4.2){\line(0, 1){0.8}}
\put(1, 5){\line(0, -1){0.4}}
\put(6, 5){\line(0, -1){0.4}}
\put(1, 4.605){\line(1, 0){5}}
\put(3.5, 3.2){\line(0, 1){0.3}}
\put(4.51, 2.85){\line(1, 0){0.8}}
\put(5.3, 2.85){\line(0, 1){1}}
\put(5.3, 3.85){\line(1, 0){0.72}}
\put(2, 0.7){\line(0, 1){0.4}}
\put(4.5, 0.7){\line(0, 1){0.4}}
\put(7, 0.7){\line(0, 1){0.8}}
\put(9.5, 0.7){\line(0, 1){0.4}}
\put(2, 1.098){\line(1, 0){7.5}}
\put(7, 2.2){\line(0, 1){0.3}}
\put(7, 3.2){\line(0, 1){0.3}}
\end{picture}
\caption{\label{fig:blocks} Flexible BAMLSS architecture.}
\end{figure}
The concept can be divided into three parts: First, functions that describe distribution families,
formulas, together with the data that is used for modeling. Secondly, functions that actually
compute estimates of parameters, and thirdly, functions for creating results like sampling
statistics i.a., visualization, prediction etc.

To set up the necessary model frame, a parser function translates the model formula
(see Section~\ref{sec:symdesc}) and the data and creates all necessary information that is
needed for subsequent model fitting engines. For some algorithms a modified version of the
model term setup is needed, e.g., when using the mixed model representation (\ref{eqn:mixed}) for
smooth terms, design and penalty matrices are transformed. This can be done by an optional
transformer function applied on the BAMLSS model frame.

The estimation step has two parts. First, an (optional) optimizer function, for example computing
posterior mode estimates. Secondly, a sampler function for full Bayesian inference with MCMC which uses
posterior mode estimates as staring values. Note that each, the optimizer and the sampler function,
can be exchanged in this concept, i.e., this way it is possible to write a number of different
optimizer and sampler functions, e.g., for specific problems.

In summary, the main advantage of the architecture is that the building blocks model frame,
transformer, optimizer, sampler and results are entirely exchangeable and reusable, i.e., it is
relatively easy to incorporate new algorithms or engines, amongst others. The other blocks are
assumed to be more or less stable such that no extra coding is required. An implementation of this
concept together with examples is illustrated in the next section.


\section{Software implementation and examples} \label{sec:softex}

An implementation of the conceptional framework is provided in the \proglang{R} package
\pkg{bamlss} \citep{bamlss:Umlauf+Klein+Zeileis:2016}. Unlike other software packages for regression
modeling, the design substantially focuses on maximum flexibility and easy integration of new code
and/or standalone systems, hence, possible implementations even go beyond the concepts presented in
the above sections.

More specifically, the \pkg{bamlss} package provides infrastructures that build up all necessary
model matrices and information from which it is easy for the user to construct estimation algorithms or
interfaces to existing software packages like \pkg{JAGS} or \pkg{BayesX}. Similarly, the user
usually does not need (want) to take care about computing post-estimation results like effect plots,
summaries, etc., however even this is possible. This way, solutions for new problems can be created
relatively easy, as shown in the examples. Therefore, the \pkg{bamlss} package can also be seen as
an harmonized framework for regression modeling.

The following only gives a brief introduction of the capabilities of the package, for a detailed
overview please visit the manual pages of \pkg{bamlss}.


\subsection{Model fitting with bamlss} \label{sec:bamlss}

The main model fitting function is called \fct{bamlss}. The most important arguments are
\begin{Code}
  bamlss(formula, family = "gaussian", data = NULL,
    weights = NULL, subset = NULL, offset = NULL, na.action = na.omit,
    transform = NULL, optimizer = NULL, sampler = NULL, results = NULL,
    start = NULL, cores = NULL, combine = TRUE, ...)
\end{Code}
where the first two lines basically represent the standard model frame specifications
\citep[see][]{bamlss:Chambers+Hastie:1992}.

Argument \code{formula} is an extended model formula, which can be specified by the
principles described in Section~\ref{sec:symdesc}, i.e., the \code{formula} can be a list of
formulas where each list entry specifies the details of one parameter of the modeled response
distribution. For incorporating smooth terms, the \pkg{bamlss} package makes heavy use of package
\pkg{mgcv} infrastructures and supports all model term constructors implemented in \pkg{mgcv},
such as \fct{s}, \fct{te}, \fct{ti} and so forth.

Estimation is performed by an \code{optimizer} and \code{sampler} function, which can be provided by the user.
The default optimizer function implements an IWLS backfitting algorithm (\ref{eqn:blockbackfit})
with automatic smoothing variance selection, see also Algorithm~\ref{fig:algodesign} and
\ref{fig:umode}. The default sampler function implements derivative based MCMC based on IWLS
proposals, smoothing variances are sampled using slice sampling, see also Section~\ref{sec:postmean}.
For writing new optimizer and sampler functions only a simple general format of function arguments
and return values must be adhered. See the documentation of the \pkg{bamlss} package for more
details.

Moreover, since MCMC sampling is computationally intensive it is also possible to run chains on
several cores using package \pkg{parallel}, which is part of the base \proglang{R} distribution. 

Like other model fitting functions in \proglang{R} the user must specify the distributions to be
modeled, which are commonly represented as so called family objects. Since these objects
are simply passed to the optimizer or sampler functions, the model fitting functions itself must 
know how much information is needed to interpret the model. Therefore, in \pkg{bamlss} the family
objects can hold an arbitrary number of specifications. Family objects may also control, e.g.,
which optimizer or sampler function should be used when calling function \fct{bamlss},
amongst other options. This way, it is easy to write model
fitting functions for specific problems, like for the continuous time Cox-model in example
Section~\ref{sec:coxreg}. At the time of writing, the distributions presented in
Table~\ref{tab:distributions} are already provided within the \pkg{bamlss} package.
\begin{table}[t!]
\centering
\begin{tabular}{ll}
Function & Distribution \\ \hline
\fct{beta.bamlss} & Beta distribution \\
\fct{binomial.bamlss} & Binomial distribution \\
\fct{cnorm.bamlss} & Censored normal distribution \\
\fct{cox.bamlss} & Continuous time Cox-model \\
\fct{gaussian.bamlss} & Gaussian distribution \\
\fct{gamma.bamlss} & Gamma distribution \\
\fct{multinomial.bamlss} & Multinomial distribution \\
\fct{mvnorm.bamlss} & Multivariate normal distribution \\
\fct{poisson.bamlss} & Poisson distribution \\ \hline
\end{tabular}
\caption{\label{tab:distributions} Available distributions in package \pkg{bamlss}.}
\end{table}
In addition, the package supports family objects of the \pkg{gamlss} package.


\subsection{Precepitation climatolgy from daily observations} \label{sec:censreg}

Climatology models are one important component of the meteorological tool set. The accurate
and complete knowledge of precipitation climatologies is especially relevant for problems involving
agriculture, risk assessment, water management, tourism etc. One particular challenge of such models
is the prediction of precipitation into areas without measurement. This is usually accounted for
by multi-step procedures using interpolation methods like Kriging \citep{bamlss:Krige:1951}.
Precipitation climatology models commonly assume a normal distribution and are computed for
monthly aggregated observations. However, because precipitation data is skewed and exhibits high
density at zero observations the assumption of a normal distribution is critical and
models for monthly data may have limited information value on finer temporal resolutions.

To contribute to this topic we analyze precipitation data of Austria taken from the HOMSTART
project
({\small \url{http://www.zamg.ac.at/cms/de/forschung/klima/datensaetze/homstart/}}) \\
conducted at the Zentralanstalt f\"ur Meteorologie und Geodynamik (ZAMG) and funded by the Austrian
Climate Research Programme (ACRP). \citet{bamlss:Umlauf+Mayr+Messner:2012} already investigated the
question if it rains more frequently on weekends than during work days using the HOMSTART data.
The data set consists of daily precipitation time series within 1948--2009. Time series that could
not be homogenized, e.g., due to missing appropriate reference stations or other uncertainties, are
left out of the analysis. Overall, a rather dense net of 57~stations is used for modeling.
In Figure~\ref{fig:rainmodeldata}, left panel, the distribution of the final meteorological stations
is shown.

As precipitation observations are limited to zero, a natural extension to the models that have been
discussed for this type of data is censoring. Censoring assumes that a certain quantity cannot be
observed below or above some threshold value. Here, we assume a ``left-censored'' normal
distribution with a threshold at zero. Moreover, to make positive observations more ``normal'', we
carry out a square root transformation of the daily observations. The censored normal distribution
seems to fit quite well to the data as shown in the right panel of Figure~\ref{fig:rainmodeldata}.

The censored normal model with latent Gaussian variable $y^\star$ and
observed response $y$, the square root of daily precipitation observations, is given by
\begin{eqnarray*}
y^\star &\sim& \mathcal{N}(\mu, \sigma^2) \\
\mu &=& \eta_{\mu} \\
\log(\sigma) &=& \eta_{\sigma} \\
y &=& max(0, y^\star).
\end{eqnarray*}
Modeling precipitation in complex terrain like the Alps naturally involves structured spatial
variation, weather incoming flow is mostly from the north-west such that the northern part of
Austria typically exhibits more rain than the southern part during the year. Moreover, the amount
of precipitation is expected to increase with altitude. To account for these effects, as well as for
potentially pronounced spatially-varying seasonality, we use the following additive predictor for
parameter $\mu$ and $\sigma$:
$$
\eta = \beta_0 + f_1(\texttt{alt}) + f_2(\texttt{day}) +
  f_3(\texttt{lon}, \texttt{lat}) + f_4(\texttt{day}, \texttt{lon}, \texttt{lat}),
$$
here function $f_1$ is an altitude effect, $f_2$ the cyclic seasonal variation, $f_3$ a spatially
correlated effect of longitude and latitude coordinates and $f_4$ a spatially-varying seasonal
effect. Hence, the overall seasonal effect is constructed by the main effect $f_2$ and the
interaction effect $f_4$, where the deviations from the main effect are modeled to sum to zero for
each day of the year, i.e., this can be viewed as a functional ANOVA decomposition. All functions
are estimated using penalized regression splines with multivariate normal priors
(\ref{eqn:mvnormprior}) and inverse gamma priors (\ref{eqn:igprior}) for the corresping variance parameters.

The density function of a left censored Gaussian distribution with the threshold at zero is given by
\begin{equation}
  f(y; \, \mu = \eta_{\mu}, \, \log(\sigma) = \eta_{\sigma}) = \begin{cases} 
    \frac{1}{\sigma} \phi\left(\frac{y-\mu}{\sigma} \right) & y > 0 \\
    \Phi\left(\frac{-\mu}{\sigma} \right) & \mbox{else,}
  \end{cases}
\end{equation}
where $\phi$ is the probability density and $\Phi$ the cumulative distribution function of the
standard normal distribution.

For posterior mode estimation with the IWLS backfitting algorithm (\ref{eqn:blockbackfit}), as well
as for the corresponding derivative based MCMC sampling scheme presented in
Section~\ref{sec:dmh}, the elements of the score vectors $\mathbf{u}_k = \partial \ell(\boldsymbol{\beta}; \mathbf{y}, \mathbf{X}) /
\partial \boldsymbol{\eta}_k$ need to be computed with
\begin{equation*}
\frac{\partial \ell(\boldsymbol{\beta}; y, \mathbf{x})}{
\partial \eta_{\mu}} = \begin{cases}
  \frac{y - \mu}{\sigma^2} & y > 0 \\
  -\frac{1}{\sigma} \frac{\phi\left(\frac{-\mu}{\sigma} \right)}
    {\Phi\left(\frac{-\mu}{\sigma} \right)} & \mbox{else,}
\end{cases}
\end{equation*}
and
\begin{equation*}
\frac{\partial \ell(\boldsymbol{\beta}; y, \mathbf{x})}{
\partial \eta_{\sigma}} = \begin{cases} 
  -1 + \frac{(y - \mu)^2}{\sigma^2} & y > 0 \\ 
  -\frac{-\mu}{\sigma} \frac{\phi\left(\frac{-\mu}{\sigma} \right)}
    {\Phi\left(\frac{-\mu}{\sigma} \right)} & \mbox{else.}
\end{cases}
\end{equation*}
The diagonal elements of the weight matrix
$\mathbf{W}_{kk} = -\mathrm{diag}(\partial^2 \ell(\boldsymbol{\beta}; \mathbf{y}, \mathbf{X}) /
\partial \boldsymbol{\eta}_k \partial \boldsymbol{\eta}_k^\top)$ are derived using
\begin{equation*}
\frac{\partial^2 \ell(\boldsymbol{\beta}; y, \mathbf{x})}{
\partial \eta_{\mu}^2} = \begin{cases} 
  -\frac{1}{\sigma^2} & y > 0 \\
  -\frac{-\mu}{\sigma^3}\frac{\phi\left(\frac{-\mu}{\sigma} \right)}
    {\Phi\left(\frac{-\mu}{\sigma} \right)}-\frac{1}{\sigma^2}
    \frac{\phi\left(\frac{-\mu}{\sigma} \right)^2}
    {\Phi\left(\frac{-\mu}{\sigma} \right)^2}
  & \mbox{else,}
\end{cases}
\end{equation*}
and
\begin{equation*}
\frac{\partial^2 \ell(\boldsymbol{\beta}; y, \mathbf{x})}{
\partial \eta_{\sigma}^2} = \begin{cases} 
  - 2 \frac{(y-\mu)^2}{\sigma^2} & y > 0 \\
  - \frac{-\mu}{\sigma}\frac{\phi\left(\frac{-\mu}{\sigma} \right)}
    {\Phi\left(\frac{-\mu}{\sigma} \right)} - \frac{(-\mu)^3}{\sigma^3}
    \frac{\phi\left(\frac{-\mu}{\sigma} \right)}
    {\Phi\left(\frac{-\mu}{\sigma} \right)} - \frac{(-\mu)^2}{\sigma^2}
    \frac{\phi\left(\frac{-\mu}{\sigma} \right)^2}
    {\Phi\left(\frac{-\mu}{\sigma} \right)^2} & \mbox{else.}
\end{cases}
\end{equation*}
The first and second derivative functions have been implemented in the \pkg{bamlss} family
\fct{cnorm.bamlss}.

Since the HOMSTART data set has over $1.2$ million observations, the resulting design matrices
excessively demands the computer's storage as well as the CPU power. In order to prevent computational
problems associated with very large data sets like HOMSTART, we make use of the fact that, e.g.,
the effect of the day of
year has $365$ unique observations, only. This is much smaller than the total number of observations
of the data set, i.e., the corresponding design matrix has a lot of duplicated rows and is hence inefficient
for the use within model fitting algorithms. Therefore, we implemented updating functions
$U_{jk}( \cdot )$ that support shrinkage of the design matrices based
on unique covariate observations. To make the IWLS algorithms based on unique observations work a
reduced form of the diagonal weight matrix $\mathbf{W}_{kk}$ and the reduced
partial residual vector from $\mathbf{z}_k - \boldsymbol{\eta}^{(t)}_{k, -j}$ has to be computed
additionally. For the computational details of this highly
efficient algorithm see \citet{bamlss:Lang+Umlauf+Wechselberger+Harttgen+Kneib:2014}. Since the default
estimation engines in package \pkg{bamlss} are based on the design of the generic BAMLSS algorithm
presented in Algorithm~\ref{fig:algodesign}, this is relatively straightforward to implement.

With a total of 4000 iterations of the MCMC sampler, on a Linux system with 8
Intel i7-2600 3.40GHz processors running the model takes approximately 17 hours. For computing the
final model output the first $2000$ samples of every core are withdrawn and only every 10th sample
is saved.

The plots of the estimated effects are shown in Figure~\ref{fig:rainmodeleffects}.
The top row illustrates the spatial variation of the seasonal effect (solid lines) together with the
mean effect (dashed lines) for parameters $\mu$ and $\sigma$. The estimates indicate that during
June to August precipitation is highest in the mean effect for $\mu$. However, there is some clear
spatial variation, especially differences between the regions north and south of the Alps. This
is highlighted by the red, gray and blue lines and show that the southern stations have a clear
annual peak while for the northern stations the semiannual pattern is more pronounced. Similarly,
the seasonal effect for parameter $\sigma$ has considerable variation between north and south.
The uncertainty peak is shifting from the middle of summer to fall when going from north to south.

The second row of Figure~\ref{fig:rainmodeleffects} shows the resulting spatial trends.
The spatial effect for parameter $\mu$ indicates that regions with positive effect accumulate in the
north-west part of Austria. The overall importance of the spatial effect is similar to the seasonal
effects, which is depicted by the range of the color legend that is used for plotting.
The spatial effect for parameter $\sigma$ shows that model uncertainty is the highest within the
southern regions and in Vorarlberg.

The bottom plot in Figure~\ref{fig:rainmodeleffects} is an example of the resulting precipitation
climatology for January 10th. The predicted average precipitation is quite low all over Austria,
ranging from $0$ to $1.1$mm. The map indicates that more precipitation can be expected in the
northern parts of the Alps, especially in Vorarlberg and regions around Salzburg. The effect of
elevation is also visible since the valleys exhibit less precipitation than the alpine regions,
however, the effect is not as pronounced as, e.g., the seasonal effect(s), most probably because
the variation of elevation of the meteorological stations used in this data set is relatively small.
\begin{figure}[t!]
\centering
\includegraphics[width=0.46\textwidth]{figures/rainmodel-data-stations}\includegraphics[width=0.46\textwidth]{figures/rainmodel-data-hist}
\caption{\label{fig:rainmodeldata} Distribution of available meteorological stations and
  daily precipitation values.}
\end{figure}

\begin{figure}[t!]
\centering
\includegraphics[width=0.46\textwidth]{figures/rainmodel-effects-season-mu}\includegraphics[width=0.46\textwidth]{figures/rainmodel-effects-season-sigma} \\[0.5cm]
\includegraphics[width=0.46\textwidth]{figures/rainmodel-effects-spatial-mu}\includegraphics[width=0.46\textwidth]{figures/rainmodel-effects-spatial-sigma} \\[0.5cm]
\includegraphics[width=0.92\textwidth]{figures/rainmodel-effects-predict}
\caption{\label{fig:rainmodeleffects} Estimated effects of the precipitation model, 1st and 2nd row,
  predicted average precipitation for January 10th, bottom row.}
\end{figure}


\subsection{Cox-model} \label{sec:coxreg}

This analysis is based on the article of \citet{bamlss:Taylor:2015}. The \emph{London Fire Brigade}
(LFB, \url{http://www.london-fire.gov.uk}) is one of the largest in the world. Each year, the LFB is
called thousands of times, in most cases due to dwelling fires. Since the time it takes until a
fire engine arrives at the scene after an emergency call has been received, an important question is
how to reduce the response times in order to prevent further damage or fatal casualties. For this
reason, the LFB's annual performance target is an average fire engine arrival time of six minutes at
maximum. Clearly, it mostly depends on the distance of the responsible fire station to the site of
how much damage can be prevented, besides, in some cases it may happen that fire engines are already
in use at another fire scenery and therefore it also depends on the number of fire stations in the
area. This is viewed as an optimization problem in \citet{bamlss:Taylor:2015}, i.e., they treat
the question of possible improvements of response times according to space in a probabilistic way.
Therefore, they analyze the effect of fire station closures in 2014 using a parametric proportional
hazards model and identify regions of possible concern about the number of available fire stations.
To contribute to the topic, we apply an extended complex Cox-model for continuous time on the
2015 dwelling fire response time data of London and show how the generic framework presented
in this paper can be utilized to set up new estimation algorithms for this type model.

The data is freely available at the London DataStore (\url{http://data.london.gov.uk/}) with an UK
Open Government Licence (OGL v2). The data set, also containing previous years, can be downloaded
from
\begin{center}
\url{http://data.london.gov.uk/dataset/london-fire-brigade-incident-records}
\end{center}
A precompiled version of the 2015 data is available in the \pkg{bamlss} package.

The final 2015 dwelling fire data set consists of $5838$ fire events that have been received at
the $103$ fire stations. The distribution of dwelling fires and fire stations is shown in
Figure~\ref{fig:londonfiredata}. The top left panel indicates that fire events are distributed
all over London and have higher density in the city center, as shown in the lower left panel where
the number of fires are counted within each of the hexagons. The top right panel then shows that for
some regions the percentage of fire response times greater than six minutes is relatively high (again
measured within each hexagon) especially at the borders of London, which is most probably due to the
lower density of fire stations. Moreover, according the full distribution of fire response times in
2015, see the lower right panel, about 30\% of the response times where greater than six minutes.
\begin{figure}[t!]
\centering
\includegraphics[width=0.92\textwidth]{figures/firemodel-data}
\caption{\label{fig:londonfiredata} Distribution of dwelling fires and fire stations in London (2015).}
\end{figure}

As mentioned above, \citet{bamlss:Taylor:2015} analyze the response times within a
survival context, which seems natural for this kind of time-to-event data. More precisely, they
describe the hazard of an event (the fire engine arrives) at time $t$ with a relative risk model of
the form
$$
\lambda(t) = \exp\left(\eta(t)\right) =  \exp\left( \eta_{\lambda}(t) + \eta_{\gamma} \right),
$$
i.e., a model for the instantaneous arrival rate conditional on the engine having not arrived before
time $t$. Here, the hazard function is assumed to depend on a time-varying predictor
$\eta_{\lambda}(t)$ and a time-constant predictor $\eta_{\gamma}$. In most survival models, the
time-varying part $\eta_{\lambda}(t)$ represents the so called baseline hazard and is an univariate
function of time $t$. Following \citet{bamlss:Taylor:2015}, we set up a similar but extended
model with a time-constant predictor given by
$$
\eta_{\gamma} = \beta_0 + f_1(\texttt{fsintens}) + f_2(\texttt{daytime}) +
  f_3(\texttt{lon}, \texttt{lat}) + f_4(\texttt{daytime}, \texttt{lon}, \texttt{lat}),
$$
where $\beta_0$ is an intercept and function $f_1$ is the effect of fire station intensity,
variable \code{fsintens} computed with a kernel density estimate of all fire stations in London.
I.e., this variable is a proxy for the distance to the next fire station(s), especially suited for
situations when the responsible fire station already send out all fire engines such that help needs
to arrive from another station. Function $f_2$ accounts for the effect that it is more
difficult for a fire engine to arrive at the scene in rush hours, i.e., the risk of waiting longer
than six minutes is expected to depend on the time of the day, variable \code{daytime}. To treat the
question of structured spatially driven hazards, a spatial effect $f_3$ of longitude and
latitude coordinates is included in the model. Moreover, we also treat the \code{daytime} effect in
a spatially correlated context, function $f_4$. For example, we assume that rush hour
peaks may have local hot spots that can be captured by this three dimensional effect. Again, all
functions $f_1, \ldots, f_4$ are assumed to be possibly nonlinear and are modeled using penalized
splines.

In this example, we additionally relax the time-varying predictor $\eta_{\lambda}(t)$ to
$$
\eta_{\lambda}(t) = f_0(t) + \sum_{j = 1}^{J_{\lambda}} f_j(t, \mathbf{x}).
$$
Here, the baseline hazard is represented by $f_0(t)$ and all functions $f_j(t, \mathbf{x})$
are time-varying possibly nonlinear functions of covariates. Hence, our model is a complex
Cox-type additive model as introduced by \citet{bamlss:Kneib+Fahrmeir:2007}. To further investigate
if there is a space-time varying effect, i.e., if the shape of the baseline hazard is dependent on
the location we use the following time-varying additive predictor
$$
\eta_{\lambda}(\texttt{arrivaltime}) = f_0(\texttt{arrivaltime}) + f_1(\texttt{arrivaltime}, \texttt{lon}, \texttt{lat}),
$$
where variable \code{arrivaltime} is the waiting time until the first fire engine arrives after the
received emergency call and function $f_0(\texttt{arrivaltime})$ then represents the baseline
hazard. Function $f_1(\texttt{arrivaltime}, \texttt{lon}, \texttt{lat})$ is a space-time varying
effect modeling the deviations from the baseline, i.e., testing if the risk of waiting longer
than six minutes is driven by other factors that are not available in this analysis. Both functions
are modeled using penalized splines.

The probability that the engine will arrive on the scene after time $t$ is described by the survival
function
$$
S(t) = Prob(T > t) = \exp \, \left( -\int_0^t \lambda(u)du \right),
$$
which is of major interest in this analysis. Based on the survival function the log-likelihood
function of the continuous time Cox-model is given by
$$
\ell(\boldsymbol{\beta}; \mathbf{y}, \mathbf{X}) =
  \sum_{i = 1}^n \left( \delta_i\eta_{i\gamma} - \int_0^{t_i} \exp( \eta_{i\lambda}(u)du ) \right).
$$
where $\delta_i$ is the usual censoring indicator, which equals to $\delta_i = 1$ in this example,
because we focus on real fire events. For implementing an algorithm for posterior mode estimation
as well as for derivative based MCMC simulation, the score vectors and Hessian need to be computed.
Assuming a basis function approach, the score vector of the regression coefficients for the
time-varying part $\eta_{\lambda}(t)$ is
$$
\mathbf{s}\left(\boldsymbol{\beta}_{\lambda}\right) = \boldsymbol{\delta}^{\top}\mathbf{X}_{\lambda}(\mathbf{t}) -
  \sum_{i = 1}^n \exp(\eta_{i\gamma})\left( \int_0^{t_i} \exp( \eta_{i\lambda}(u) )\mathbf{x}_i(u)du \right).
$$
The elements of the Hessian w.r.t.\ $\boldsymbol{\beta}_{\lambda}$ are
$$
\mathbf{H}\left(\boldsymbol{\beta}_{\lambda}\right) =
-\sum_{i = 1}^{n} \exp\left( \eta_{i\gamma} \right) \int_{0}^{t_i} \exp( \eta_{i\lambda}(u) )
  \boldsymbol{x}_{i\lambda}(u)\boldsymbol{x}_{i\lambda}^{\top}(u)du.
$$
Note that the Hessian cannot be simplified further, i.e., in order to construct IWLS updating
functions. The reason is that for the IWLS updating scheme we need to compute the diagonal weight
matrix based on $\partial^2 \ell(\boldsymbol{\beta}; \mathbf{y}, \mathbf{X}) /
\partial \boldsymbol{\eta}_{\lambda}(\mathbf{t}) \partial \boldsymbol{\eta}_{\lambda}(\mathbf{t})^\top$.
This requires a functional derivative like the Hadamard derivative since the predictor depends on time
$t$. However, it turns out that the derivative forms martingale residuals
\citep[see, e.g.,][]{bamlss:Barlow+Prentice:1988} which are incapable for estimating time-varying
effects, see also \citet{bamlss:Hofner:2008} Section~5.2 for a detailed discussion.

Constructing updating functions for the time-constant part $\eta_{\gamma}$ again yields an
IWLS updating scheme with working observations given by
$$
\mathbf{z} = \boldsymbol{\eta}_{\gamma} + \mathbf{W}^{-1}\mathbf{u},
$$
with the weight matrix
$$
\mathbf{W} = \mathrm{diag}(\mathbf{P}\exp( \boldsymbol{\eta}_{\gamma})),
$$
where $\mathbf{P}$ is a diagonal matrix with elements $p_{ii} = \int_0^{t_i}
\exp( \eta_{i\lambda}(u)du )$. The score vector is
$$
\mathbf{u} = \boldsymbol{\delta} - \mathbf{P}\exp(\boldsymbol{\eta}_{\gamma}).
$$
\citep{bamlss:Hennerfeind+Brezger+Fahrmeir:2006}

As a result, applying the generic algorithm presented in Algorithm~\ref{fig:algodesign} to this type
of problem two specific difficulties need to be considered. First, the updating functions for
the time-varying predictor $\eta_{\lambda}(t)$ are different from the time-constant updating
functions for $\eta_{\gamma}$. Secondly, a specific hurdle of the continuous time Cox-model is the
computation of the integrals, because these do not have a closed form solution and need to be
approximated numerically, e.g., by the trapezoidal rule or Gaussian quadrature. Moreover, it
is inefficient to compute the integrals a new for every updating step, since for the
time-constant part the integrals given in $\mathbf{P}$ do not change anymore.

In order to reduce computing time we account for the idiosyncrasy of the Cox-model and
implement an optimizer function \fct{cox.mode} for posterior mode estimation as well as
the sampler function \fct{cox.mcmc} for MCMC simulation. The amount of work to implement this
model using the \pkg{bamlss} infrastructures is moderate, because most of the code of the default
estimation engines can be reused and only need slight adaption. In this example, the
the optimizer and sampler function are part of the corresponding bamlss family object
\fct{cox.bamlss}.

On a Linux system with 8 Intel i7-2600 3.40GHz processors estimation takes
approximately 1.2 days. Note that function \fct{cox.mode} also applies an automated procedure for
smoothing variances selection using information criteria, see also Algorithm~\ref{fig:umode}.

The estimated effects are shown in Figure~\ref{fig:londonfireeffects}. The upper left panel
shows that the average ``risk'' that a fire engine arrives increases steeply until the target time
of six minutes. The space-time varying effect is relatively small compared to the overall size
of the effect, especially until the six minutes target time it seems that the location
does not have a great influence on the relative risk. Only for waiting times above ${\sim}15$
minutes, the space-time varying effect is more pronounced. The effect for fire station intensity is
quite large and bounded, i.e., there is a natural limit for the benefit from opening new fire stations in the area.
The effect of the time of the day then indicates that in the morning hours around $4$-$5$ am, as well
as in the afternoon around $2$-$4$ pm, the risk of waiting longer for the fire engine to arrive
is only slightly increasing. In addition, the spatial deviations from the mean time of day effect are
modest, similar in magnitude as the spacial-varying baseline effects. The largest deviation seems
to be at around $10$ am. In Figure~\ref{fig:londonfiredaytime} the spatial-varying effect is
illustrated on $12$ time points. The maps indicate possible hot-spots of this effect, however, as
mentioned above the overall effect size from $-0.4$ to $0.4$ is not very large such that differences
in risk probabilities are almost negligible. In contrast, the time-constant spatial effect clearly shows
that the average risk of increased waiting times are higher in the city center and some smaller
area in southern London. However, the  estimated probabilities of waiting longer than six minutes
around the center show moderate variation, while the borders of London indicate higher probabilities
as well as in the western parts, most probably because of the lower fire station density in these
areas. In summary, next to the baseline effect, the most important effects on the log risk are the
fire station intensity and the time-constant spatial effect which have an absolute range of about
$4$ on the log-scale.

\begin{figure}[t!]
\centering
\includegraphics[width=0.46\textwidth]{figures/firemodel-effects-baseline}\includegraphics[width=0.46\textwidth]{figures/firemodel-effects-prob} \\[0.5cm]
\includegraphics[width=0.46\textwidth]{figures/firemodel-effects-fsintens}\includegraphics[width=0.46\textwidth]{figures/firemodel-effects-spatial-td} \\[0.5cm]
\includegraphics[width=0.46\textwidth]{figures/firemodel-effects-daytime-curves}\includegraphics[width=0.46\textwidth]{figures/firemodel-effects-spatial-tc}
\caption{\label{fig:londonfireeffects} Estimated effects of the fire emergency response times
  survival model. Top left panel shows the mean baseline effect, red line, together with the
  spatially-varying effects, black lines. The six minutes target waiting time is represented by
  the blue dashed vertical line. The upper right panel shows the estimated probability
  of waiting longer than six minutes until the first engine arrives at 8:30 am. The space-time varying
  effect is illustrated at six minutes waiting time in the second row, right panel.
  The time of day effect again shows the mean effect as red lines and spatial deviations by black lines.}
\end{figure}


\begin{figure}[t!]
\centering
\includegraphics[width=0.32\textwidth]{figures/firemodel-daytime-t1}\includegraphics[width=0.32\textwidth]{figures/firemodel-daytime-t2}\includegraphics[width=0.32\textwidth]{figures/firemodel-daytime-t3} \\[0.5cm]
\includegraphics[width=0.32\textwidth]{figures/firemodel-daytime-t4}\includegraphics[width=0.32\textwidth]{figures/firemodel-daytime-t5}\includegraphics[width=0.32\textwidth]{figures/firemodel-daytime-t6} \\[0.5cm]
\includegraphics[width=0.32\textwidth]{figures/firemodel-daytime-t7}\includegraphics[width=0.32\textwidth]{figures/firemodel-daytime-t8}\includegraphics[width=0.32\textwidth]{figures/firemodel-daytime-t9} \\[0.5cm]
\includegraphics[width=0.32\textwidth]{figures/firemodel-daytime-t10}\includegraphics[width=0.32\textwidth]{figures/firemodel-daytime-t11}\includegraphics[width=0.32\textwidth]{figures/firemodel-daytime-t12} \\[0.5cm]
\caption{\label{fig:londonfiredaytime} Estimated spatial-varying time of the day effect.}
\end{figure}


\section{Summary}\label{sec:conclusion}

This paper summarizes frequently used algorithms for the estimation of flexible Bayesian
distributional regression models, a.k.a.\ Bayesian additive models for location, scale and shape
(BAMLSS), and beyond. We highlight the similarities between optimization and sampling concepts and
outline a generic architecture that utilizes repeated ``Lego bricks'' in order to facilitate solutions
for new problems or the integration of existing code. An implementation of the conceptional framework
is provided in the \proglang{R} package \pkg{bamlss} \citep{bamlss:Umlauf+Klein+Zeileis:2016}.
The usefulness of the approach is illustrated by two complex, difficult to estimate, models.


%% \section*{Acknowledgments}


\bibliography{bamlss}


\clearpage


\begin{appendix}

\section{Posterior mode updating based on IWLS} \label{appendix:pmodeiwls}

The following shows the steps needed to derive the iterative updating scheme based on IWLS in
Section~\ref{sec:bricksmodelfit}. Focusing on the $j$-th row of (\ref{eqn:blocknewton}) gives
\begin{eqnarray*}
(\mathbf{X}_{jk}^\top\mathbf{W}_{kk}\mathbf{X}_{jk} + \mathbf{G}_{jk}(\boldsymbol{\tau}_{jk}))\boldsymbol{\beta}_{jk}^{(t+1)} +
\ldots + \mathbf{X}_{jk}^\top\mathbf{W}_{kk}\mathbf{X}_{J_kk}\boldsymbol{\beta}_{J_kk}^{(t + 1)} - \\
(\mathbf{X}_{jk}^\top\mathbf{W}_{kk}\mathbf{X}_{jk} + \mathbf{G}_{jk}(\boldsymbol{\tau}_{jk}))\boldsymbol{\beta}_{jk}^{(t)} -
\ldots - \mathbf{X}_{jk}^\top\mathbf{W}_{kk}\mathbf{X}_{J_kk}\boldsymbol{\beta}_{J_kk}^{(t)}
  &=& \mathbf{X}_{jk}^\top \mathbf{u}_k^{(t)} - \mathbf{G}_{jk}(\boldsymbol{\tau}_{jk})\boldsymbol{\beta}_{jk}^{(t)}
\end{eqnarray*}
\begin{eqnarray*}
\mathbf{G}_{jk}(\boldsymbol{\tau}_{jk})(\boldsymbol{\beta}_{jk}^{(t+1)} - \boldsymbol{\beta}_{jk}^{(t)}) +
  \mathbf{X}_{jk}^\top\mathbf{W}_{kk}\mathbf{X}_{jk}\boldsymbol{\beta}_{jk}^{(t+1)} + \ldots +
  \mathbf{X}_{jk}^\top\mathbf{W}_{kk}\mathbf{X}_{J_kk}\boldsymbol{\beta}_{J_kk}^{(t+1)} - \\
  \mathbf{X}_{jk}^\top\mathbf{W}_{kk}\mathbf{X}_{jk}\boldsymbol{\beta}_{jk}^{(t)} - \ldots -
  \mathbf{X}_{jk}^\top\mathbf{W}_{kk}\mathbf{X}_{J_kk}\boldsymbol{\beta}_{J_kk}^{(t)}
  &=& \mathbf{X}_{jk}^\top \mathbf{u}_k^{(t)} - \mathbf{G}_{jk}(\boldsymbol{\tau}_{jk})\boldsymbol{\beta}_{jk}^{(t)}
\end{eqnarray*}
\begin{eqnarray*}
\mathbf{G}_{jk}(\boldsymbol{\tau}_{jk})\boldsymbol{\beta}_{jk}^{(t)} +
  \mathbf{G}_{jk}(\boldsymbol{\tau}_{jk})(\boldsymbol{\beta}_{jk}^{(t+1)} - \boldsymbol{\beta}_{jk}^{(t)}) +
  \mathbf{X}_{jk}^\top\mathbf{W}_{kk}\mathbf{X}_{jk}\boldsymbol{\beta}_{jk}^{(t+1)} + \ldots +
  \mathbf{X}_{jk}^\top\mathbf{W}_{kk}\mathbf{X}_{J_kk}\boldsymbol{\beta}_{J_kk}^{(t+1)} - \\
  \mathbf{X}_{jk}^\top\mathbf{W}_{kk}\mathbf{X}_{jk}\boldsymbol{\beta}_{jk}^{(t)} - \ldots -
  \mathbf{X}_{jk}^\top\mathbf{W}_{kk}\mathbf{X}_{J_kk}\boldsymbol{\beta}_{J_kk}^{(t)}
  &=& \mathbf{X}_{jk}^\top \mathbf{u}_k^{(t)}
\end{eqnarray*}
\begin{eqnarray*}
\mathbf{G}_{jk}(\boldsymbol{\tau}_{jk})\boldsymbol{\beta}_{jk}^{(t+1)} +
  \mathbf{X}_{jk}^\top\mathbf{W}_{kk}\mathbf{X}_{jk}\boldsymbol{\beta}_{jk}^{(t+1)} + \ldots +
  \mathbf{X}_{jk}^\top\mathbf{W}_{kk}\mathbf{X}_{J_kk}\boldsymbol{\beta}_{J_kk}^{(t+1)} - \\
  \mathbf{X}_{jk}^\top\mathbf{W}_{kk}\mathbf{X}_{jk}\boldsymbol{\beta}_{jk}^{(t)} - \ldots -
  \mathbf{X}_{jk}^\top\mathbf{W}_{kk}\mathbf{X}_{J_kk}\boldsymbol{\beta}_{J_kk}^{(t)}
  &=& \mathbf{X}_{jk}^\top \mathbf{u}_k^{(t)}
\end{eqnarray*}
\begin{eqnarray*}
\mathbf{G}_{jk}(\boldsymbol{\tau}_{jk})\boldsymbol{\beta}_{jk}^{(t+1)} +
  \mathbf{X}_{jk}^\top\mathbf{W}_{kk}\mathbf{X}_{jk}\boldsymbol{\beta}_{jk}^{(t+1)} + 
  \mathbf{X}_{jk}^\top\mathbf{W}_{kk}\boldsymbol{\eta}_{k, -j}^{(t+1)} -
  \mathbf{X}_{jk}^\top\mathbf{W}_{kk}\boldsymbol{\eta}_{k}^{(t)}
  &=& \mathbf{X}_{jk}^\top \mathbf{u}_k^{(t)}
\end{eqnarray*}
\begin{eqnarray*}
(\mathbf{X}_{jk}^\top\mathbf{W}_{kk}\mathbf{X}_{jk} + \mathbf{G}_{jk}(\boldsymbol{\tau}_{jk}))\boldsymbol{\beta}_{jk}^{(t+1)}
  &=& \mathbf{X}_{jk}^\top \mathbf{u}_k^{(t)} + \mathbf{X}_{jk}^\top\mathbf{W}_{kk}\boldsymbol{\eta}_{k}^{(t)} -
    \mathbf{X}_{jk}^\top\mathbf{W}_{kk}\boldsymbol{\eta}_{k, -j}^{(t+1)}
\end{eqnarray*}
\begin{eqnarray*}
  \boldsymbol{\beta}_{jk}^{(t+1)}
  &=& (\mathbf{X}_{jk}^\top\mathbf{W}_{kk}\mathbf{X}_{jk} + \mathbf{G}_{jk}(\boldsymbol{\tau}_{jk}))^{-1}(\mathbf{X}_{jk}^\top \mathbf{u}_k^{(t)} +
    \mathbf{X}_{jk}^\top\mathbf{W}_{kk}\boldsymbol{\eta}_{k}^{(t)} -
    \mathbf{X}_{jk}^\top\mathbf{W}_{kk}\boldsymbol{\eta}_{k, -j}^{(t+1)})
\end{eqnarray*}
\begin{eqnarray*}
  \boldsymbol{\beta}_{jk}^{(t+1)}
  &=& (\mathbf{X}_{jk}^\top\mathbf{W}_{kk}\mathbf{X}_{jk} + \mathbf{G}_{jk}(\boldsymbol{\tau}_{jk}))^{-1}\mathbf{X}_{jk}^\top(\mathbf{u}_k^{(t)} +
    \mathbf{W}_{kk}\boldsymbol{\eta}_{k}^{(t)} -
    \mathbf{W}_{kk}\boldsymbol{\eta}_{k, -j}^{(t+1)})
\end{eqnarray*}
\begin{eqnarray*}
  \boldsymbol{\beta}_{jk}^{(t+1)}
  &=& (\mathbf{X}_{jk}^\top\mathbf{W}_{kk}\mathbf{X}_{jk} + \mathbf{G}_{jk}(\boldsymbol{\tau}_{jk}))^{-1}\mathbf{X}_{jk}^\top(
    \mathbf{W}_{kk}\mathbf{W}_{kk}^{-1}\mathbf{u}_k^{(t)} +
    \mathbf{W}_{kk}\boldsymbol{\eta}_{k}^{(t)} -
    \mathbf{W}_{kk}\boldsymbol{\eta}_{k, -j}^{(t+1)})
\end{eqnarray*}
\begin{eqnarray*}
  \boldsymbol{\beta}_{jk}^{(t+1)}
  &=& (\mathbf{X}_{jk}^\top\mathbf{W}_{kk}\mathbf{X}_{jk} + \mathbf{G}_{jk}(\boldsymbol{\tau}_{jk}))^{-1}\mathbf{X}_{jk}^\top\mathbf{W}_{kk}(
    \mathbf{W}_{kk}^{-1}\mathbf{u}_k^{(t)} +
    \boldsymbol{\eta}_{k}^{(t)} -
    \boldsymbol{\eta}_{k, -j}^{(t+1)})
\end{eqnarray*}
This yields the updating function $U_{jk}( \cdot )$ shown in (\ref{eqn:blockbackfit}).

\newpage

\section{Full conditionals for derivative based MCMC} \label{appendix:fullcond1}
The following shows the steps to derive a multivariate normal jumping distribution based
on a second order Taylor series expansion of the log-posterior centered at the last state of
$\boldsymbol{\beta}_{jk}$.
\begin{eqnarray*}
  \pi(\boldsymbol{\beta}_{jk}^\star | \cdot) &\propto& \exp\left[
    \log\,p\left(\boldsymbol{\beta}_{jk}^{(t)} | \cdot\right) +
    \left(\boldsymbol{\beta}_{jk}^\star - \boldsymbol{\beta}_{jk}^{(t)}\right)^\top \mathbf{s}\left(\boldsymbol{\beta}_{jk}^{(t)}\right) + \right.\\
  && \left. \qquad\qquad \frac{1}{2}\left(\boldsymbol{\beta}_{jk}^\star - \boldsymbol{\beta}_{jk}^{(t)}\right)^\top
    \mathbf{H}_{kk}\left(\boldsymbol{\beta}_{jk}^{(t)}\right)
    \left(\boldsymbol{\beta}_{jk}^\star - \boldsymbol{\beta}_{jk}^{(t)}\right)\right] \\
  &\propto& \exp\left[(\boldsymbol{\beta}_{jk}^\star)^\top \mathbf{s}\left(\boldsymbol{\beta}_{jk}^{(t)}\right) +
    \left(\frac{1}{2}(\boldsymbol{\beta}_{jk}^\star)^\top\mathbf{H}_{kk}\left(\boldsymbol{\beta}_{jk}^{(t)}\right) - \right.\right. \\
  && \qquad\qquad\left.\left. \frac{1}{2}(\boldsymbol{\beta}_{jk}^{(t)})^\top\mathbf{H}_{kk}\left(\boldsymbol{\beta}_{jk}^{(t)}\right) \right)\left(\boldsymbol{\beta}_{jk}^\star - \boldsymbol{\beta}_{jk}^{(t)}\right)\right] \\
  &\propto& \exp\left[(\boldsymbol{\beta}_{jk}^\star)^\top \mathbf{s}\left(\boldsymbol{\beta}_{jk}^{(t)}\right) +
    \frac{1}{2}(\boldsymbol{\beta}_{jk}^\star)^\top\mathbf{H}_{kk}\left(\boldsymbol{\beta}_{jk}^{(t)}\right)\boldsymbol{\beta}_{jk}^\star - \right. \\
  && \left. \qquad\qquad\, \frac{1}{2}(\boldsymbol{\beta}_{jk}^\star)^\top\mathbf{H}_{kk}\left(\boldsymbol{\beta}_{jk}^{(t)}\right)\boldsymbol{\beta}_{jk}^{(t)} -
    \frac{1}{2}(\boldsymbol{\beta}_{jk}^{(t)})^\top\mathbf{H}_{kk}\left(\boldsymbol{\beta}_{jk}^{(t)}\right)\boldsymbol{\beta}_{jk}^\star \right] \\
  &=& \exp\left[
    \frac{1}{2}(\boldsymbol{\beta}_{jk}^\star)^\top\mathbf{H}_{kk}\left(\boldsymbol{\beta}_{jk}^{(t)}\right)\boldsymbol{\beta}_{jk}^\star +
    (\boldsymbol{\beta}_{jk}^\star)^\top \mathbf{s}\left(\boldsymbol{\beta}_{jk}^{(t)}\right) -
    (\boldsymbol{\beta}_{jk}^\star)^\top\mathbf{H}_{kk}\left(\boldsymbol{\beta}_{jk}^{(t)}\right)\boldsymbol{\beta}_{jk}^{(t)} \right] \\
  &=& \exp\left[
    -\frac{1}{2}(\boldsymbol{\beta}_{jk}^\star)^\top-\mathbf{H}_{kk}\left(\boldsymbol{\beta}_{jk}^{(t)}\right)\boldsymbol{\beta}_{jk}^\star +
    (\boldsymbol{\beta}_{jk}^\star)^\top \left(\mathbf{s}\left(\boldsymbol{\beta}_{jk}^{(t)}\right) - \mathbf{H}_{kk}\left(\boldsymbol{\beta}_{jk}^{(t)}\right)\boldsymbol{\beta}_{jk}^{(t)}\right) \right]
\end{eqnarray*}
Which leads to the proposal density
$q(\boldsymbol{\beta}_{jk}^\star | \, \boldsymbol{\beta}_{jk}^{(t)}) =
  \mathcal{N}(\boldsymbol{\mu}_{jk}^{(t)}, \boldsymbol{\Sigma}_{jk}^{(t)})$ with precision matrix
$$
\left(\boldsymbol{\Sigma}_{jk}^{(t)}\right)^{-1} = -\mathbf{H}_{kk}\left(\boldsymbol{\beta}_{jk}^{(t)}\right)
$$
and mean
\begin{eqnarray*}
\boldsymbol{\mu}_{jk}^{(t)} &=& \boldsymbol{\Sigma}_{jk}^{(t)}\left[
  s\left(\boldsymbol{\beta}_{jk}^{(t)}\right) -
  \mathbf{H}_{kk}\left(\boldsymbol{\beta}_{jk}^{(t)}\right)\boldsymbol{\beta}_{jk}^{(t)} \right] \\
&=& \boldsymbol{\beta}_{jk}^{(t)} -
  \mathbf{H}_{kk}\left(\boldsymbol{\beta}_{jk}^{(t)}\right)^{-1}
  \mathbf{s}\left(\boldsymbol{\beta}_{jk}^{(t)}\right) \\
 &=& \boldsymbol{\beta}_{jk}^{(t)} -
  \left[\mathbf{F}_{kk}\left( \boldsymbol{\beta}_{jk}^{(t)} \right) +
    \mathbf{G}_{jk}(\boldsymbol{\tau}_{jk}))\right]^{-1}\mathbf{s}\left( \boldsymbol{\beta}_{jk}^{(t)} \right).
\end{eqnarray*}
Using a linear basis function representation of functions $f_{jk}( \cdot )$ the precision matrix is
$$
\left(\boldsymbol{\Sigma}_{jk}^{(t)}\right)^{-1} = \mathbf{X}_{jk}^\top\mathbf{W}_{kk}\mathbf{X}_{jk} + \mathbf{G}_{jk}(\boldsymbol{\tau}_{jk}),
$$
with weights $\mathbf{W}_{kk} = -\mathrm{diag}(\partial^2 \ell(\boldsymbol{\beta}; \mathbf{y}, \mathbf{X}) /
\partial \boldsymbol{\eta}_k \partial \boldsymbol{\eta}_k^\top)$ and the mean can be written as
\begin{eqnarray*}
  \boldsymbol{\mu}_{jk}^{(t)} &=& \boldsymbol{\Sigma}_{jk}^{(t)}\left[
  \mathbf{X}_{jk}^\top\mathbf{u}_k^{(t)} - \mathbf{G}_{jk}(\boldsymbol{\tau}_{jk})\boldsymbol{\beta}_{jk}^{(t)} +
  (\mathbf{X}_{jk}^\top\mathbf{W}_{kk}\mathbf{X}_{jk} + \mathbf{G}_{jk}(\boldsymbol{\tau}_{jk}))\boldsymbol{\beta}_{jk}^{(t)}\right] \\
  &=& \boldsymbol{\Sigma}_{jk}^{(t)}\left[
  \mathbf{X}_{jk}^\top\mathbf{u}_k^{(t)} +
  \mathbf{X}_{jk}^\top\mathbf{W}_{kk}\mathbf{X}_{jk}\boldsymbol{\beta}_{jk}^{(t)}\right] \\
  &=& \boldsymbol{\Sigma}_{jk}^{(t)}\left[
  \mathbf{X}_{jk}^\top\mathbf{u}_k^{(t)} +
  \mathbf{X}_{jk}^\top\mathbf{W}_{kk}\left(\boldsymbol{\eta}_k^{(t)} - \boldsymbol{\eta}^{(t)}_{k,-j}\right)\right] \\
  &=& \boldsymbol{\Sigma}_{jk}^{(t)}\mathbf{X}_{jk}^\top\left[
  \mathbf{u}_k^{(t)} + \mathbf{W}_{kk}\left(\boldsymbol{\eta}_k^{(t)} - \boldsymbol{\eta}^{(t)}_{k,-j}\right)\right] \\
  &=& \boldsymbol{\Sigma}_{jk}^{(t)}\mathbf{X}_{jk}^\top\mathbf{W}_{kk}\left[
  \boldsymbol{\eta}_k^{(t)} + \mathbf{W}_{kk}^{-1}\mathbf{u}_k^{(t)}  - \boldsymbol{\eta}^{(t)}_{k,-j}\right] \\
  &=& \boldsymbol{\Sigma}_{jk}^{(t)}\mathbf{X}_{jk}^\top\mathbf{W}_{kk}\left[\mathbf{z}_k  - \boldsymbol{\eta}^{(t)}_{k,-j}\right]
\end{eqnarray*}
with working observations
$\mathbf{z}_k = \boldsymbol{\eta}_k^{(t)} + \mathbf{W}_{kk}^{-1}\mathbf{u}_k^{(t)}$.

\end{appendix}


\end{document}

