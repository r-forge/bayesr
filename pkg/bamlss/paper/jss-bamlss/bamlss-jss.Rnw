%\documentclass[article]{jss}
\documentclass[nojss]{jss}
\usepackage{amsmath,amssymb,amsfonts,thumbpdf}
\usepackage{multirow,longtable}

%% additional commands
\newcommand{\squote}[1]{`{#1}'}
\newcommand{\dquote}[1]{``{#1}''}
\newcommand{\fct}[1]{{\texttt{#1()}\index{#1@\texttt{#1()}}}}
\newcommand{\class}[1]{\dquote{\texttt{#1}}}
%% for internal use
\newcommand{\fixme}[1]{\emph{\marginpar{FIXME} (#1)}}
\newcommand{\readme}[1]{\emph{\marginpar{README} (#1)}}

%% Authors: NU + rest in alphabetical order
\author{Nikolaus Umlauf\\Universit\"at Innsbruck \And
        Achim Zeileis\\Universit\"at Innsbruck \AND
        Nadja Klein\\Humboldt-University of Berlin \And
        Thorsten Simon\\Universit\"at Innsbruck}
\Plainauthor{Nikolaus Umlauf, Achim Zeileis, Nadja Klein, Thorsten Simon}

\title{\pkg{bamlss}: A Lego Toolbox for {B}ayesian Additive Models for
  Location Scale and Shape (and Beyond)}
\Plaintitle{bamlss: A Lego Toolbox for Bayesian Additive
  Models for Location Scale and Shape (and Beyond)}
\Shorttitle{A Lego Toolbox for BAMLSS (and Beyond)}
	
\Keywords{GAMLSS, distributional regression, backfitting, gradient boosting, LASSO,
  MCMC, neural networks, \proglang{R}}
\Plainkeywords{GAMLSS, distributional regression, backfitting, gradient boosting, LASSO,
  MCMC, neural networks, R}

\Abstract{
During the last decades there has been an increasing interest in distributional regression 
models that allow to model all distributional parameters, such as location, scale and shape 
and thereby the entire data distribution conditional on covariates. In particular, the 
framework of structured additive distributional regression models enables to specify 
different types of effects such as linear, non-linear or interaction effects on all the 
distribution parameters hence providing a very flexible and generic framework suited for 
many complex real data problems. However, the implementation of new models is usually time-consuming
and complex, especially using Bayesian estimation algorithms. We propose an unified modeling
architecture, which is implemented in the \proglang{R} package \pkg{bamlss}, that makes it possible
to embed many different approaches suggested in literature and software. We show that
implementing (new) algorithms, non-standard distributions or effect types, or the integration of
already existing software, is relatively straightforward in this setting. We illustrate the
usefulness of the approach by implementing highly efficient algorithms for fully Bayesian inference
based on MCMC simulation, backfitting algorithms, gradient boosting, LASSO-type penalized models as
well as neural network distributional regression models.
}

\Address{
  Nikolaus Umlauf, Achim Zeileis, Thorsten Simon\\
  Department of Statistics\\
  Faculty of Economics and Statistics\\
  Universit\"at Innsbruck\\
  Universit\"atsstr.~15\\
  6020 Innsbruck, Austria\\
  E-mail: \email{Nikolaus.Umlauf@uibk.ac.at},\\
  \phantom{E-mail: }\email{Achim.Zeileis@R-project.org}\\
  \phantom{E-mail: }\email{Thorsten.Simon@uibk.ac.at}\\
  URL: \url{http://eeecon.uibk.ac.at/~umlauf/},\\
  \phantom{URL: }\url{http://eeecon.uibk.ac.at/~zeileis/}\\

  Nadja Klein\\
  Humboldt-University of Berlin\\
  School of Business and Economics\\
  Applied Statistics\\
  Unter den Linden 6\\
  10099 Berlin, Germany\\
  E-mail: \email{nadja.klein@hu-berlin.de}\\
  URL: \url{https://hu.berlin/NK}
}

\SweaveOpts{engine = R, eps = FALSE, keep.source = TRUE}

<<preliminaries, echo=FALSE, results=hide>>=
options(width = 70, prompt = "R> ", continue = "+  ",
  SweaveHooks = list(fig = function() par(mar = c(4.1, 4.1, 1, 1))))
library("bamlss")
@

\input{defs}

\begin{document}
\SweaveOpts{concordance=TRUE}

\section{Introduction}

Generalized additive models for location, scale and shape
(GAMLSS,~\citealp{bamlss:Rigby+Stasinopoulos:2005}), also known as distributional regression models
\citep{bamlss:Klein+Kneib+Lang+Sohn:2015, bamlss:Klein+Kneib+Klasen+Lang:2015}, provide a very
flexible framework that enables modeling all parameters of a response distribution in terms of
covariates. While most common model specifications (linear models, generalized linear models [GLMs],
generalized additive models [GAMs], etc.) ignore the fact that the distribution of any quantity is
not well characterized by the mean alone, GAMLSS has the clear advantage to minimize the risk of
model misspecification and therefore provides much more reliable probabilistic forecasts.

The \proglang{R} package \pkg{gamlss}
\citep{bamlss:Stasinopoulos:Rigby:2007, bamlss:Stasinopoulos+Rigby:2019} and \pkg{VGAM}
\citep{bamlss:Yee:2009, bamlss:Yee:2019} were the first packages
to follow the distributional regression approach and comprise a quite flexible framework for
implementing response distributions. Variable selection algorithms for GAMLSS based on gradient
boosting were then introduced by the \pkg{gamboostLSS} package
\citep{bamlss:Hofner+Mayr+Schmid:2014, bamlss:Hofner:2018} and more recently, the contributed
base \proglang{R} package \pkg{mgcv} \citep{bamlss:Wood:2017, bamlss:Wood:2019} provides a
fitting routine for the estimation of general smooth models \citep{bamlss:Wood+Pya+Saefken:2016}.
The inferential framework of the packages mentioned is based on maximum likelihood, however, if
taken into account at all. For complex models, e.g., with response
distributions outside the exponential family or when multiple predictors contain several smooth
effects \citep{bamlss:Klein+Kneib+Lang:2015}, the maximum likelihood estimators based on
asymptotic properties might fail. In such situations the fully Bayesian approach using Markov chain
Monte Carlo (MCMC) simulation techniques is particularly attractive since valid credible intervals
are easily obtained from the posterior samples. While Bayesian distributional regression models
can in principle be estimated using general purpose MCMC software like
\pkg{JAGS}~\citep{bamlss:Plummer:2013}, \pkg{Stan}~\citep{bamlss:stan-software:2016} or
\pkg{WinBUGS}~\citep{bamlss:Lunn+Thomas+Best+Spiegelhalter:2000}, they all have the drawback
that sampling time is usually quite long, or it is even impossible to estimate distributional
regression models with GAM-type predictors, e.g., when using large data sets, modeling spatial
effects or complex higher-order interactions. The first software capable of estimating such
GAM-type distributional regression models using MCMC is the standalone package
\pkg{BayesX}~\citep{bamlss:Brezger+Kneib+Lang:2005, bamlss:Umlauf:2015, bamlss:Belitz+Brezger+Kneib+Lang+Umlauf:2017},
which provides highly efficient sampling schemes for very large
data sets as well as (spatial) multilevel models. However, the implementation of new response
distributions (models) in \pkg{BayesX} is not straightforward and users usually have to ask the
maintainers to do so.

In this paper, we present the \proglang{R} package \pkg{bamlss}, a flexible ``Lego toolbox''
for Bayesian distributional regression models (and beyond). The main contributions of \pkg{bamlss}
are the following:
%
\begin{itemize}
\item Estimate distributional regression models with usual \proglang{R} modeling ``look \& feel'',
\item easy implementation of (new) models and algorithms (Bayesian or frequentist),
\item comparison of existing optimization algorithms and samplers,
\item or integration of existing implementations.
\end{itemize}
%
The package builds on the well-established \pkg{mgcv} infrastructures using
\proglang{R}'s formula syntax for model specification. Moreover, the package provides
commonly used extractor functions like \fct{summary}, \fct{plot}, \fct{predict}, etc., such
that users interested in developing new models or algorithms (Bayesian or frequentist) can
really concentrate on it.
The package is made available from the Comprehensive
\proglang{R} Archive Network (CRAN) at \url{http://CRAN.R-project.org/package=bamlss}.

The remainder of this paper is as follows. In Section~\ref{sec:motivation},
three motivating examples illustrate the first steps using \pkg{bamlss}. Section~\ref{sec:distreg}
introduces distributional regression models in more detail. A thorough introduction of the
\proglang{R} package \pkg{bamlss}, describing the most important infrastructures and building blocks,
is then given in Section~\ref{sec:package}. In Section~\ref{sec:application} we highlight the
unified modeling approach using a complex distributional regression model for predicting
flash counts.


\section{Motivating examples} \label{sec:motivation}

This section gives a first quick overview of the basic functionality of the package.
Readers more interested in advanced examples using \pkg{bamlss} could skip this
section and move to Section~\ref{sec:application} directly. The first example
demonstrates that the usual ``look \& feel'' when using well-established model fitting 
functions like \fct{glm} is an elementary part of \pkg{bamlss}, i.e., first steps and
basic handling of the package should be relatively simple. The second example then
explains how full distributional regression models can be estimated and show
cases the flexibility of the provided modeling infrastructures. The last example shows that the
package can also be used to use completely different estimation methods, in this case
a LASSO-type estimation engine.

\subsection{Logit model} \label{sec:logitmodel}

This example is taken from the \pkg{AER} package \citep{bamlss:Kleiber+Zeileis:2008} and is about
labor force participation (yes/no) of women in Switzerland 1981 \citep{bamlss:Gerfin:1996}.
The data can be loaded with
<<>>=
data("SwissLabor", package = "AER")
@
The data frame contains of 872 observations on 7 variables, where some of them might have
a nonlinear influence on the response labor \code{participation}. Now, a standard Bayesian 
binomial logit model using the default MCMC algorithm can be fitted with
<<echo = FALSE, results = hide>>=
if(!file.exists("SwissLaborModel.rda")) {
  f <- participation ~ income + age + education + youngkids + oldkids + foreign + I(age^2)
  set.seed(123)
  b <- bamlss(f, family = "binomial", data = SwissLabor,
    n.iter = 12000, burnin = 2000, thin = 10)
  save(b, file = "SwissLaborModel.rda")
} else {
  load("SwissLaborModel.rda")
}
@
<<eval = FALSE>>=
library("bamlss")

## Model formula.
f <- participation ~ income + age + education +
  youngkids + oldkids + foreign + I(age^2)

## First, set the seed for reproducibility.
set.seed(123)

## Estimate model.
b <- bamlss(f, family = "binomial", data = SwissLabor,
  n.iter = 12000, burnin = 2000, thin = 10)
@
using 12000 iterations with a burnin-phase of 2000 and a thinning parameter of 10. Prior
the MCMC sampling a backfitting algorithm for posterior mode estimation is performed. The
posterior mode estimates are then used as starting values for MCMC. Using
the main model fitting function \fct{bamlss} all model fitting engines can be exchanged, 
which is explained in detail in Section~\ref{sec:package} and the application
Section~\ref{sec:application}. The default model fitting engines use family objects
(see Section~\ref{sec:package}), similar to the families that can be used
with the \fct{glm} function, which enables easy implementation of new distributions (models).

Note, to capture nonlinearities, a quadratic term for variable \code{age} is added to
the model. The resulting object \code{b} is of class \code{"bamlss"} for which standard
extractor functions like \fct{summary}, \fct{coef}, \fct{plot}, \fct{predict}, etc.\
are available. The model summary output is printed by~\label{logitmodel_summary}
<<>>=
summary(b)
@
and is based on MCMC samples, which suggest ``significant'' effects for all covariates, 
except for variable \code{education}, since the 95\% credible interval contains zero.
In addition, the acceptance probabilities \code{alpha} are reported and indicate
proper behavior of the MCMC algorithm.
The column \code{parameters} shows respective posterior mode estimates of the regression
coefficients, which are calculated by the upstream backfitting algorithm. Before 
proceeding the analysis, users usually perform additional convergence checks of the
MCMC chains by looking at traceplots and auto-correlation.
<<eval = FALSE>>=
plot(b, which = c("samples", "max-acf"))
@
\begin{figure}[!ht]
\centering
\setkeys{Gin}{width=1\textwidth}
<<echo = FALSE, fig = TRUE, height = 3, width = 8>>=
par(mfrow = c(1, 3), mar = c(4.1, 4.1, 4.1, 1.1))
bsp <- b
bsp$samples <- bsp$samples[, c("pi.p.(Intercept)"), drop = FALSE]
plot(bsp, which = "samples", spar = FALSE, ask = FALSE)
plot(b, which = "max-acf", spar = FALSE, ask = FALSE)
@
\caption{\label{fig:logit_traceplots} Logit model, MCMC trace (left panel),
  auto-correlation for the intercept (middle panel), maximum auto-correlation for all
  parameters (right panel).}
\end{figure}
These are visualized in Figure~\ref{fig:logit_traceplots} and reveal convergence of
the MCMC chains, i.e., there is no visible trend and the very low
auto-correlation shown for the intercept and the maximum auto-correlation of all
parameters suggests i.i.d.\ samples from the posterior distribution.
Note that the function call would compute all trace- and auto-correlation plots, however,
for convenience we only show plots for the intercept. In addition, samples can also be
extracted using function \fct{samples}, which returns an object of class \code{"mcmc"},
a class provided by the \pkg{coda} package \citep{bamlss:Plummer+Best+Cowles+Vines:2006}.
This package includes a rich infrastructure for further convergence diagnostic checks,
e.g., Gelman and Rubin's convergence diagnostic
\citep{bamlss:Gelman+Rubin:1992, bamlss:Brooks+Gelman:1998} or
Heidelberger and Welch's convergence diagnostic
\citep{bamlss:Heidelberger+Welch:1981, bamlss:Heidelberger+Welch:1983}.

Model predictions on the probability scale can be obtained by the predict method, e.g.,
to visualize the effect of covariate \code{age} on the probability we can do the following:
<<>>=
## Create new data for prediction.
nd <- data.frame(income = 11, age = seq(2, 6.2, length = 100),
  education = 12, youngkids = 1, oldkids = 1, foreign = "no")

## Predict for both cases of variable foreign.
nd$p.no <- predict(b, newdata = nd, type = "parameter", FUN = c95)
nd$foreign <- "yes"
nd$p.yes <- predict(b, newdata = nd, type = "parameter", FUN = c95)
@
The predict method is applied on all MCMC samples and argument \code{FUN} specifies a function
that can be applied on the predictor or distribution parameter samples. The default is the \fct{mean} function,
however, in this case we additionally extract the empirical 2.5\% and 97.5\% quantiles
using function \fct{c95} to obtain credible intervals (note, individual samples can be
extracted by passing \code{FUN = function(x) \{ return(x) \}}, i.e., this way users can
easily generate their own statistics). Then, the estimated effect can be visualized with
<<eval = FALSE>>=
plot2d(p.no ~ age, data = nd, ylab = "participation",
  ylim = range(c(nd$p.no, nd$p.yes)), lty = c(2, 1, 2))
plot2d(p.yes ~ age, data = nd, col.lines = "blue", add = TRUE,
  lty = c(2, 1, 2))
@
\begin{figure}[!ht]
\centering
\setkeys{Gin}{width=0.4\textwidth}
<<echo = FALSE, fig = TRUE, width = 4.5, height = 3.8>>=
par(mar = c(4.1, 4.1, 0.1, 1.1))
plot2d(p.no ~ age, data = nd, ylab = "participation",
  ylim = range(c(nd$p.no, nd$p.yes)), lty = c(2, 1, 2))
plot2d(p.yes ~ age, data = nd, col.lines = "blue", add = TRUE,
  lty = c(2, 1, 2))
legend("topright", c("foreign yes", "foreign no"), lwd = 1,
  col = c("blue", "black"), box.col = NA, bg = NA)
@
\caption{\label{fig:logit_effects} Effect of covariate \code{age} on estimated
  probabilities for both cases, \code{foreign} \code{"yes"} and \code{"no"}. The solid lines
  represent mean estimates, dashed lines represent the 95\% credible interval.}
\end{figure}
The estimates are shown in Figure~\ref{fig:logit_effects} and suggest a clear difference 
for the effect of \code{age} between both cases of factor variable \code{foreign}.

\subsection{Normal location-scale model}

In this example we will now extend the framework and estimate a complete distributional regression
model using a small textbook example of the well-known simulated motorcycle accident data
\citep{bamlss:Silverman:1985}.
<<>>=
data("mcycle", package = "MASS")
@
The data set contains measurements of the head acceleration
(in $g$, variable \code{accel}) in a simulated motorcycle accident, recorded in milliseconds after
impact (variable \code{times}). To estimate a normal location-scale model with
$$
\texttt{accel} \sim \mathcal{N}(\mu = f_{\mu}(\texttt{times}),
  \log(\sigma) = f_{\sigma}(\texttt{times}))
$$
where functions $f_{\mu}( \cdot )$ and $f_{\sigma}( \cdot )$ are unspecified smooth
functions, which are estimated using regression splines. The log-link for parameter
$\sigma$ ensures positivity. We can use the following model formula for estimation
<<>>=
f <- list(accel ~ s(times, k = 20), sigma ~ s(times, k = 20))
@
where function \fct{s} is the smooth term constructor from the \pkg{mgcv} package
\citep{bamlss:Wood:2019}, the default of \fct{s} are thin-plate regression splines.
Note that model formulae are provided as lists of formulae, i.e., each list entry
represents one parameter of the response distribution. Moreover, note that all smooth
terms, i.e., \fct{te}, \fct{ti}, etc., are supported by \pkg{bamlss}. This way, it is
also possible to incorporate user defined model terms. A full Bayesian semi-parametric 
distributional regression model can be estimated with
<<echo = FALSE, results = hide>>=
if(!file.exists("toymodel.rda")) {
  set.seed(456)
  b <- bamlss(f, data = mcycle, family = "gaussian",
    n.iter = 12000, burnin = 2000, thin = 10)
  save(b, file = "toymodel.rda")
} else {
  load("toymodel.rda")
}
@
<<eval = FALSE>>=
## Set the seed for reproducibility.
set.seed(456)

## Estimate normal location-scale model.
b <- bamlss(f, data = mcycle, family = "gaussian",
  n.iter = 12000, burnin = 2000, thin = 10)
@
again using 12000 iterations for the MCMC chain, a burnin of 2000 and a thinning
of parameter of 10. After the estimation algorithms are finished, the estimated effects can be
visualized instantly using the plotting method.
<<eval = FALSE>>=
plot(b, model = c("mu", "sigma"))
@
\begin{figure}[!ht]
\centering
\setkeys{Gin}{width=0.4\textwidth}
<<echo = FALSE, fig = TRUE, width = 4.5, height = 3.8>>=
par(mar = c(4.1, 4.1, 0.1, 1.1))
plot(b, model = "mu", spar = FALSE, scheme = 2, grid = 200)
@
<<echo = FALSE, fig = TRUE, width = 4.5, height = 3.8>>=
par(mar = c(4.1, 4.1, 0.1, 1.1))
plot(b, model = "sigma", spar = FALSE, scheme = 2, grid = 200)
@
\caption{\label{fig:toy_effects} Estimated effects of \code{times} on parameter
  $\mu$ and $\sigma$ of the normal location-scale model. The grey shaded areas represent
  95\% credible intervals.}
\end{figure}
The estimated effects are shown in Figure~\ref{fig:toy_effects} depicting a clear nonlinear
relationship for parameter $\mu$ and $\sigma$.

For judging how well the model fits to the data the user can inspect randomized quantile
residuals \citep{bamlss:Dunn:Smyth:1996} using histograms or quantile-quantile plots. 
Residuals can be extracted using function \fct{residuals} and has a plotting method. 
Alternatively, residuals can be investigated with
<<eval = FALSE>>=
plot(b, which = c("hist-resid", "qq-resid"))
@
\begin{figure}[!ht]
\centering
\setkeys{Gin}{width=0.4\textwidth}
<<echo = FALSE, fig = TRUE, width = 4.5, height = 4.7>>=
par(mar = c(4.1, 4.1, 4.1, 1.1))
plot(b, which = "hist-resid", spar = FALSE, col = "lightgray")
@
<<echo = FALSE, fig = TRUE, width = 4.5, height = 4.7>>=
par(mar = c(4.1, 4.1, 4.1, 1.1))
plot(b, which = "qq-resid", spar = FALSE)
@
\caption{\label{fig:toy_resids} Histogram and quantile-quantile plot of the
  resulting randomized quantile residuals of the normal location-scale model.}
\end{figure}
According the histogram and the quantile-quantile plot of the resulting randomized
quantile residuals in Figure~\ref{fig:toy_resids}, the model seems to fit relatively well. 
Only for very low and very high values of \code{accel} the fitted distributions seem
to be less appropriate.

Besides residuals, users can evaluate the model performance, e.g., for model selection 
based on the deviance information criterion (DIC), which can be extracted using function 
\fct{DIC}
<<>>=
DIC(b)
@
and is also reported in the model summary output. Furthermore, statistical calibration
of fitted models can be assessed by scoring rules
\citep{bamlss:Gneiting+Balabdaoui+Raftery:2007}. For example, 
the \proglang{R} package \pkg{scoringRules} \citep{bamlss:Jordan+Krueger+Lerch:2018} 
provides easy evaluation of the continuous rank probability score (CRPS) for a couple of 
distributions.

\subsection{LASSO-type penalization} \label{sec:lassointro}

<<echo = FALSE>>=
SwissLabor$c.income <- cut(SwissLabor$income,
  breaks = quantile(SwissLabor$income, prob = seq(0, 1, length = 10)),
  include.lowest = TRUE)

SwissLabor$c.age <- cut(SwissLabor$age,
  breaks = quantile(SwissLabor$age, prob = seq(0, 1, length = 10)),
  include.lowest = TRUE)

SwissLabor$c.education <- with(SwissLabor, cut(education,
  unique(quantile(SwissLabor$education, prob = seq(0, 1, length = 10))),
  include.lowest = TRUE))

SwissLabor$youngkids <- ordered(SwissLabor$youngkids)
SwissLabor$oldkids[SwissLabor$oldkids > 4] <- 5
SwissLabor$oldkids <- ordered(SwissLabor$oldkids)

if(!file.exists("LassoSL.rda")) {
  f <- participation ~ la(c.income,fuse=2) + la(c.age,fuse=2) + la(c.education,fuse=2) +
    la(youngkids,fuse=2) + la(oldkids,fuse=2) + la(foreign,fuse=2)
    
  b <- bamlss(f, family = "binomial", data = SwissLabor,
    optimizer = lasso, sampler = FALSE, upper = 500, lower = 2,
    criterion = "BIC", nlambda = 1000)

  set.seed(111)

  m <- bamlss(f, family = "binomial", data = SwissLabor,
    optimizer = FALSE, sampler = GMCMC,
    n.iter = 12000, burnin = 2000, thin = 10,
    start = coef(b, mstop = lasso.stop(b)))

  save(b, m, file = "LassoSL.rda")
} else {
  load("LassoSL.rda")
}
@

This section illustrates that estimation engines can easily exchanged using the flexible
infrastructure provided in \pkg{bamlss}. Again, the \code{SwissLabor} data and binomial logit model
of Section~\ref{sec:logitmodel} is used, however, in this example a fused LASSO algorithm is applied
for estimation.
The algorithm performs variable selection in combination with factor fusion (clustering) and can also
be used to identify interpretable nonlinearities. Methodological details on LASSO-type penalization
using \pkg{bamlss} are provided in \citet{bamlss:Groll+Hambuckers+Kneib+Umlauf:2019}.
To apply the fused LASSO and to capture possible nonlinearities, the numeric variables \code{age},
\code{income} and \code{education} are categorized using empirical quantiles. E.g., for variable
\code{age} an ordered factor variable can be obtained by
<<eval = FALSE>>=
SwissLabor$c.age <- cut(SwissLabor$age,
  breaks = quantile(SwissLabor$age, prob = seq(0, 1, length = 10)),
  include.lowest = TRUE)
@
Variables \code{income} and \code{education} are transformed in the same way. Similarly,
variable \code{youngkids} and \code{oldkids} are transformed to ordered factors using
function \fct{ordered}. The formula for the fused LASSO model is then specified with the
special \fct{la} model term constructor function provided in \pkg{bamlss}
<<>>=
f <- participation ~ la(c.income,fuse=2) + la(c.age,fuse=2) +
  la(c.education,fuse=2) + la(youngkids,fuse=2) + la(oldkids,fuse=2) +
  la(foreign,fuse=2)
@
where argument \code{fuse} specifies the type of fusion (nominal fusion \code{fuse=1},
ordered fusion \code{fuse=2}). To estimate the fused LASSO model only the default \code{optimizer}
function in the \fct{bamlss} wrapper function call needs to exchanged
<<eval = FALSE>>=
b <- bamlss(f, family = "binomial", data = SwissLabor,
  optimizer = lasso, sampler = FALSE,
  criterion = "BIC")
@
The optimum shrinkage parameter $\lambda$ is selected by the BIC.
Note that no MCMC sampling is used after the \fct{lasso} estimation engine is applied,
argument \code{sampler = FALSE} in the \fct{bamlss} call.

The BIC curve and the coefficient paths including the optimum shrinkage parameter $\lambda$ can
be visualized with
<<eval = FALSE>>=
lasso.plot(b)
@
and are shown in Figure~\ref{fig:lasso}.
\begin{figure}[!ht]
\centering
\setkeys{Gin}{width=0.32\textwidth}
<<echo = FALSE, fig = TRUE, width = 4, height = 5>>=
par(mar = c(4.1, 4.1, 4.1, 1.1))
lasso.plot(b, 1, spar = FALSE, main = "criterion")
@
<<echo = FALSE, fig = TRUE, width = 4, height = 5, results = hide>>=
par(mar = c(4.1, 4.1, 4.1, 4.1))
lasso.plot(b, 2, spar = FALSE, name = "pi.s.la(c.income).c.income", main = "income")
@
<<echo = FALSE, fig = TRUE, width = 4, height = 5, results = hide>>=
par(mar = c(4.1, 4.1, 4.1, 4.1))
lasso.plot(b, 2, spar = FALSE, name = "pi.s.la(c.age).c.age", main = "age")
@
\newline
<<echo = FALSE, fig = TRUE, width = 4, height = 5, results = hide>>=
par(mar = c(4.1, 4.1, 4.1, 4.1))
lasso.plot(b, 2, spar = FALSE, name = "pi.s.la(c.education).c.education", main = "education")
@
<<echo = FALSE, fig = TRUE, width = 4, height = 5, results = hide>>=
par(mar = c(4.1, 4.1, 4.1, 4.1))
lasso.plot(b, 2, spar = FALSE,
  name = c("pi.s.la(youngkids).youngkids", "pi.s.la(oldkids).oldkids", "pi.s.la(foreign).foreign"),
  main = "kids & foreign")
legend("bottomleft", c("youngkids", "oldkids", "foreign"), lwd = 1, col = rev(rainbow_hcl(3)),
  box.col = NA, bg = NA)
@
\caption{\label{fig:lasso} Top row left panel, BIC curve with optimum shrinkage parameter $\lambda$
  of the LASSO example model. All other panels show the coefficient paths of categorized variables
  and true factor covariates.}
\end{figure}
The BIC curves shows a clear minimum. The coefficient paths obviously depict that the algorithm
can either shrink categories out of the model (shrink to zero), or even fuses them, e.g.,
as shown for variable \code{youngkids} where categories \code{1}, \code{2} and \code{3} are
all fused and have the same negative effect.

\begin{figure}[!ht]
\centering
\setkeys{Gin}{width=0.32\textwidth}
<<echo = FALSE, fig = TRUE, width = 4, height = 4.3, results = hide>>=
pincome <- predict(b, term = "c.income", intercept = FALSE, mstop = lasso.stop(b))
page <- predict(b, term = "c.age", intercept = FALSE, mstop = lasso.stop(b))
peducation <- predict(b, term = "c.education", intercept = FALSE, mstop = lasso.stop(b))

yr <- range(c(pincome, page, peducation))

plot2d(pincome ~ income, data = SwissLabor, rug = TRUE, ylim = yr)
@
<<echo = FALSE, fig = TRUE, width = 4, height = 4.3>>=
plot2d(page ~ age, data = SwissLabor, rug = TRUE, ylim = yr)
@
<<echo = FALSE, fig = TRUE, width = 4, height = 4.3, results = hide>>=
plot2d(peducation ~ education, data = SwissLabor, rug = TRUE, ylim = yr)
@
\caption{\label{fig:lasso_effects} Fused LASSO, estimated nonlinear effects of categorized variables
  \code{age}, \code{income} and \code{education} with added \fct{rug} representation
  of the data (small bottom vertical lines).}
\end{figure}
In Figure~\ref{fig:lasso_effects}, the estimated effects of the categorized variables \code{age}, 
\code{income} and \code{education} are shown. The effects are computed by predicting without
intercept using the optimum stopping iteration, which is selected by BIC and can be extracted with
function \fct{lasso.stop}. The stopping iteration is passed to the \fct{predict} method by specifying
the \code{mstop} argument.
<<eval = FALSE>>=
p.age <- predict(b, term = "c.age", intercept = FALSE,
  mstop = lasso.stop(b))
@
The figure is then generated using the untransformed original covariate
<<eval = FALSE>>=
plot2d(p.age ~ age, data = SwissLabor, rug = TRUE)
@
Using the fused LASSO estimation some nonlinearities
can be identified, moreover, compared to the insignificant effect
of variable \code{education} in the summary output of Section~\ref{sec:logitmodel} on
page~\pageref{logitmodel_summary}, the fused LASSO identifies a positive effect on labor
\code{participation} for years of education above 12. The effect size for covariate \code{age} is
the largest according the y-axis, indicating that \code{age} has the highest influence on
labor \code{participation}.

\section{Distributional regression} \label{sec:distreg}

\subsection{Model structure}

Within the framework of GAMLSS \citep{bamlss:Rigby+Stasinopoulos:2005} or
distributional regression models \citep{bamlss:Klein+Kneib+Lang+Sohn:2015}
all parameters of the response distribution can be modeled by
explanatory variables such that
\begin{equation} \label{eqn:dreg}
y \sim \mathbf{\mathcal{D}}\left(h_{1}(\theta_{1}) = \eta_{1}, \,\,
  h_{2}(\theta_{2}) = \eta_{2}, \dots, \,\, h_{K}(\theta_{K}) =
  \eta_{K}\right),
\end{equation}
where $\mathbf{\mathcal{D}}$ denotes a parametric distribution for the response
variable $y$ with $K$ parameters $\theta_k$, $k = 1, \ldots, K$, that are linked to 
additive predictors using known monotonic and twice differentiable functions
$h_{k}(\cdot)$. Note that the response may also be a
$q$-dimensional vector $\mathbf{y} = (y_{1}, \ldots, y_{q})^\top$, e.g., when
$\mathbf{\mathcal{D}}$ is a multivariate distribution
(see, e.g., \citealp{bamlss:Klein+Kneib+Klasen+Lang:2015}).
The additive predictor for the $k$-th parameter is given by
\begin{equation} \label{eqn:addpred}
\boldsymbol{\eta}_k = \eta_k(\mathbf{X}; \boldsymbol{\beta}_k) =
  f_{1k}(\mathbf{X}; \boldsymbol{\beta}_{1k}) + \ldots + f_{J_kk}(\mathbf{X}; \boldsymbol{\beta}_{J_kk}),
\end{equation}
based on $j = 1, \ldots, J_k$ unspecified (possibly nonlinear) functions $f_{jk}(\cdot)$, 
applied to each row of the generic data matrix $\mathbf{X}$, encompassing all available 
covariate information. The corresponding parameters
$\boldsymbol{\beta}_k = (\boldsymbol{\beta}_{1k}, \ldots, \boldsymbol{\beta}_{J_kk})^\top$ 
are typically regression coefficients pertaining to model matrices
$\mathbf{X}_k = (\mathbf{X}_{1k}, \ldots, \mathbf{X}_{J_kk})^\top$,
whose structure only depend on the type of covariate(s) and prior assumptions about
$f_{jk}( \cdot )$.

Usually, functions $f_{jk}( \cdot )$ are based on a basis function approach, where $\eta_k$ then
is a typical GAM-type or so-called structured additive predictor
(STAR,~\citealp{bamlss:Fahrmeir+Kneib+Lang:2004}). \citet{bamlss:Umlauf+Klein+Zeileis:2018}
relax this assumption and let $f_{jk}(\cdot)$ be an unspecified composition of covariate data
and regression coefficients. For example, functions $f_{jk}(\cdot)$ could also represent nonlinear
growth curves, a regression tree, a neural network or LASSO-penalized model terms as shown
in Section~\ref{sec:lassointro}.

For full Bayesian inference, priors need to be assigned to the regression coefficients
$\boldsymbol{\beta}_{jk}$. To be as flexible as possible, \citet{bamlss:Umlauf+Klein+Zeileis:2018}
use the rather general prior $
p_{jk}(\boldsymbol{\beta}_{jk}; \boldsymbol{\tau}_{jk}, \boldsymbol{\alpha}_{jk}) 
$ for the $j$-th model term of the $k$-th parameter, where the form of $p_{jk}( \cdot )$ depends
on the type of function $f_{jk}( \cdot )$. Here,
$\boldsymbol{\tau} =
(\boldsymbol{\tau}_{11}^\top, \ldots, \boldsymbol{\tau}_{J_11}^\top, \ldots,
\boldsymbol{\tau}_{1K}^\top, \ldots, \boldsymbol{\tau}_{J_KK}^\top)^\top$ is
the vector of all assigned hyper-parameters, e.g., representing smoothing variances (shrinkage parameters).
Similarly, $\boldsymbol{\alpha}_{jk}$ is the set of all fixed prior specifications. In most
situations the prior $p_{jk}(\boldsymbol{\beta}_{jk}; \boldsymbol{\tau}_{jk}, \boldsymbol{\alpha}_{jk})$
is based on a multivariate normal kernel for $\boldsymbol{\beta}_{jk}$ and on inverse gamma
distributions for each $\boldsymbol{\tau}_{jk} = (\tau_{1jk}, \ldots, \tau_{L_{jk}jk})^\top$,
but as indicated previously, in principle any type of prior can be used
(please see \citealp{bamlss:Gelman:2006, bamlss:Polson+Scott:2012, bamlss:Klein+Kneib:2016,
bamlss:Umlauf+Klein+Zeileis:2018} for
more detailed discussions on priors for $\boldsymbol{\beta}_{jk}$ and $\boldsymbol{\tau}_{jk}$).

Examples of distributional models that fit well in this framework are the ones for:
\begin{itemize}
  \item Univariate responses of any type, e.g.~counts with zero-inflation and or overdispersion as
    proposed in \citet{bamlss:Klein+Kneib+Lang:2015, bamlss:Herwartz:Klein+Strumann:2016},
    continuous responses with spikes, skewness, heavy tails or bounded support as in
    \citet{bamlss:Klein+Kneib+Lang+Sohn:2015,bamlss:Klein+Denuit+Kneib+Lang:2014}, as well as
    responses for extreme events \citep{bamlss:Umlauf+Kneib:2018}.
 \item Multivariate responses such as multivariate normal, multivariate t or Dirichlet regression
   (for analyzing compositional data, \citealp{bamlss:Klein+Kneib+Klasen+Lang:2015}).
 \item Multivariate responses with more complex dependence structures modeled through copulas
   \citet{bamlss:Klein+Kneib:2016b}.
 \item Survival data and joint modeling~\citep{bamlss:Koehler+Umlauf+Greven:2016, bamlss:Koehler+Umlauf+Greven:2018}.
\end{itemize}

\subsection{Posterior estimation} \label{sec:algos}

Although the types of models that can be fitted within the flexible BAMLSS framework
can be quite complex, \citet{bamlss:Umlauf+Klein+Zeileis:2018} show that there are a number
of similarities between optimization and sampling concepts. Fortunately, and albeit the different
model term complexity, algorithms for posterior mode and
mean estimation can be summarized into a partitioned updating scheme with
separate updating equations using leapfrog or zigzag iteration \citep{bamlss:Smyth:1996}, e.g.,
with updating equations
\begin{equation} \label{eqn:blockblockupdate}
(\boldsymbol{\beta}_{jk}^{(t + 1)}, \boldsymbol{\tau}_{jk}^{(t + 1)}) =
  U_{jk}(\boldsymbol{\beta}_{jk}^{(t)}, \boldsymbol{\tau}_{jk}^{(t)}; \, \cdot \,) \qquad
    j = 1, \ldots, J_k, \quad k = 1, \ldots, K,
\end{equation}
where function $U_{jk}( \cdot )$ is an updating function, e.g., for generating one Newton-Raphson
step or for getting the next step in an MCMC simulation, a.o.

Using a basis function approach, the updating functions $U_{jk}( \cdot )$ for posterior mode
(frequentist penalized likelihood) estimation or MCMC for $\boldsymbol{\beta}_{jk}$ share an iteratively
weighted least squares updating step (IWLS,~\citealp{bamlss:Gamerman:1997})
\begin{equation} \label{eqn:blockbackfit}
  \boldsymbol{\beta}_{jk}^{(t+1)}
  = U_{jk}(\boldsymbol{\beta}_{jk}^{(t)}; \, \cdot \,) =
    (\mathbf{X}_{jk}^\top\mathbf{W}_{kk}\mathbf{X}_{jk} +
      \mathbf{G}_{jk}(\boldsymbol{\tau}_{jk}))^{-1}\mathbf{X}_{jk}^\top\mathbf{W}_{kk}(
      \mathbf{z}_k - \boldsymbol{\eta}_{k, -j}^{(t+1)}),
\end{equation}
with weight matrices $\mathbf{W}_{kk}$ and working responses $\mathbf{z}_k$, similarly to the
well-known IWLS updating scheme for generalized linear models (GLM,~\citealp{bamlss:Nelder+Wedderburn:1972}).
The matrices $\mathbf{G}_{jk}(\boldsymbol{\tau}_{jk})$ are derivative matrices of the priors
$p_{jk}(\boldsymbol{\beta}_{jk}; \boldsymbol{\tau}_{jk}, \boldsymbol{\alpha}_{jk})$ w.r.t.\ the
regression coefficients $\boldsymbol{\beta}_{jk}$, e.g.,
$\mathbf{G}_{jk}(\boldsymbol{\tau}_{jk})$ can be a penalty matrix that penalizes the complexity of
$f_{jk}( \cdot )$ using a P-spline representation \citep{bamlss:Eilers+Marx:1996}.

Even if the functions $f_{jk}( \cdot )$ are not based on a basis function approach, the
updating scheme (\ref{eqn:blockbackfit}) can be further generalized with
$$
\boldsymbol{\beta}_{jk}^{(t + 1)} = U_{jk}\left(\boldsymbol{\beta}_{jk}^{(t)},
  \mathbf{z}_k - \boldsymbol{\eta}_{k, -j}^{(t+1)}; \, \cdot \,\right),
$$
i.e., theoretically any updating function applied on the ``partial residuals''
$\mathbf{z}_k - \boldsymbol{\eta}_{k, -j}^{(t+1)}$ can be used. (For detailed derivations see
\citealp{bamlss:Umlauf+Klein+Zeileis:2018}.)

The great advantage of this modular architecture is, that the concept does not limit to modeling
of the distributional parameters $\theta_k$ in (\ref{eqn:dreg}), e.g.\ as mentioned above,
based on the survival function,
\citet{bamlss:Koehler+Umlauf+Greven:2016} and \citet{bamlss:Koehler+Umlauf+Greven:2018} implement
Bayesian joint models for survival and longitudinal data. Moreover, the updating schemes do not
restrict to any particular estimation engine, e.g.,
\citet{bamlss:Groll+Hambuckers+Kneib+Umlauf:2019} use the framework to implement LASSO-type
penalization for GAMLSS and \citet{bamlss:Simon+Fabsic+Mayr+Umlauf+Zeileis:2018}
investigate gradient boosting with stability selection algorithms
(see also Section~\ref{sec:application}). Very recently, \citet{bamlss:Umlauf+Klein:2019}
implement neural network distributional regression models.

\subsection{Model Choice and Evaluation}

\subsubsection{Measures of performance}

Model choice and variable selection is important in distributional regression due to the large number of candidate models. \texttt{bamlss} offers a number of possible options to do so.

\begin{itemize}
\item {Information criteria} can be used to compare different model specifications.  
For posterior mode estimation, the Akaike information criterion (AIC), or the corrected
AIC, as well as the Bayesian information criterion (BIC), are implemented in \texttt{bamlss}.  Estimation of model complexity
is based on the so-called equivalent degrees of freedom (EDF). 

For MCMC based estimation, 
\texttt{bamlss} mainly relies on the deviance information criterion (DIC, \cite{SpiBesCarLin2002}) and widely applicable information criterion (WAIC, \cite{watanabe2010asymptotic}). 
\item Quantile residuals:
Quantile residuals \cite{bamlss:Dunn:Smyth:1996} can be used to evaluate the model fit.
%defined as
%$\hat{r}_i = \Phi^{-1}(\mathbf{\mathcal{F}}(y_i | \, \hat{\boldsymbol{\theta}}_{i}))$ with the inverse 
%cumulative distribution function (CDF) of a standard normal distribution $\Phi^{-1}$ and
% $\mathbf{\mathcal{F}}( \cdot )$ the CDF of the modeled distribution $\mathcal{D}( \cdot )$ with estimated
%parameters $\hat{\boldsymbol{\theta}}_{i} = (\hat{\theta}_{i1}, \ldots, \hat{\theta}_{iK})^\top$
%plugged in, should at least approximately be standard normally distributed if the correct model has
%been specified. Resulting residuals can
%be assessed by quantile-quantile-plots or
%probability integral transforms which consider
%$u_i = \mathbf{\mathcal{F}}(y_i | \, \hat{\boldsymbol{\theta}}_i)$. If the estimated model is a good approximation to the true data generating 
%process, the $u_i$ will then approximately follow a uniform distribution on $[0, 1]$.
%Graphically, histograms of the $u_i$ can be used for this purpose.
\item Scoring rules:
Sometimes it is helpful to evaluate the performance on a test data set (or for instance based on cross validation). For this, \texttt{bamlss} implements proper scoring rules \cite{bamlss:Gneiting+Balabdaoui+Raftery:2007}. %Implementation of the log-score (i.e.~the can for instance be done via \textbf{CODE}
\end{itemize}

\subsubsection{Evaluation and Interpretation}
 \begin{itemize}
\item Plotting: The default plotting function of a \texttt{bamlss} object \texttt{plot(b)} will return all effects centered around their mean. However, it can sometimes be useful in distributional regression to look at transformation of the original model parameters such as expected value or variance of the repsonse variable $\yvec$, we give examples for both in our application.
\item Predictions:
For obtaining such transformations the \texttt{predict} function is used. 
 \end{itemize}


\section{The bamlss package} \label{sec:package}

\section{Application} \label{sec:application}

This section illustrates the workflow with \pkg{bamlss} along an example application.
We want to build a statistical model linking positive counts of cloud-to-ground
lightning discharges to atmospheric quantities from a reanalysis dataset.

The region we focus on are the European Eastern Alps. Cloud-to-ground lightning
discharges---detected by the Austrian Lightning Detection and Information System
\citep[ALDIS, ][]{schulz2005}---are counted on grids with a mesh size of $32~km$.
The lightning observations are available for the period 2010--2018.
The reanalysis data comes from the fifth generation of the ECMWF (European
Centre for Medium-Range Weather Forecasts) atmosphheric reanalyses of the
global climate \citep{era5}. ERA5 provides a globally complete and consistent
pseudo-observations of the atmosphere using the laws of physics. The horizontal
resolution is approx.\ $32~km$. The temporal resolution is hourly. The temporal
coverage is from 1979 to present.
In this example application we work only with a small subset of the data, which
can be assessed from the accompanying \proglang{R} package \pkg{FlashAustria}
\citep{FlashAustria}.

<<application-load-model, echo=FALSE>>=
## --- data ---
# data("flash_austria", package = "FlashAustria")
data("fitted_model", package = "FlashAustria")
b <- flash_austria_model

## --- specify formula ---
f <- list(
  mu = counts ~ s(d2m, bs = "ps") + s(q_prof_PC1, bs = "ps") +
    s(cswc_prof_PC4, bs = "ps") + s(t_prof_PC1, bs = "ps") +
    s(v_prof_PC2, bs = "ps") + s(sqrt_cape, bs = "ps"),
  theta = ~s(sqrt_lsp, bs = "ps")
)
@

<<application-data>>=
## --- data ---
data("flash_austria", package = "FlashAustria")
head(flash_austria_train)
nrow(flash_austria_train)
@

The motivation for this application is as follows: Lightning counts are not
modelled within the atmospheric reanalyses. Lightning observations are only
available for the period 2010--2018. With a statistical model on hand one could
predict lightning counts for the time before 2010 and thus analyze lightning
events in the past for which no observations are available.

The response of our statistical model are positive counts, with a mean of
\Sexpr{round(mean(flash_austria_train$counts), 2)}, and a variance of
\Sexpr{round(var(flash_austria_train$counts), 2)}. Thus, we are facing a
truncated count data distribution which is highly overdispersive.  In order to
capture the tuncation of the data and its overdispersion we employ a
zero-truncated negative binomial distribution \citep{cameron2013count}, which
is specified by two parameters $\mu > 0$ and $\theta > 0$. $\mu$ is the
expectation of the underlying untruncated negative binomial, and $\theta$
modifies the variance of the untruncated negative binomial by
$\mathrm{VAR}(\tilde{Y}) = \mu + \mu^2/\theta$, where $\tilde{Y}$ is a latent
random variable following the underlying untruncated negative binomial
distribution.

The zero-truncated negative binomial distribution is implemented as
\code{ztnbinom_bamlss()} within \pkg{bamlss}.  In order to specify smooth terms
form both distributional parameter, the formula has to be a \code{list}. The
first element is named \code{mu} and specifies terms for the paramter $\mu$,
but also specifies that \code{counts} is the response of the regression model.
The second element is named \code{theta} and specifies a smooth term for the
paramter $\theta$. Hence well known for their sampling properties, we are
applying P-splines \citep{bamlss:Eilers+Marx:1996} for all terms. Specifying
smooth terms within \pkg{bamlss} formulae builds on the \pkg{mgcv} infrastructure
\citep{bamlss:Wood:2019} provided by \code{s()}, which leads to the following
specification of the model:

<<application-model1, eval=FALSE>>=
## --- specify formula ---
f <- list(
  mu = counts ~ s(d2m, bs = "ps") + s(q_prof_PC1, bs = "ps") +
    s(cswc_prof_PC4, bs = "ps") + s(t_prof_PC1, bs = "ps") +
    s(v_prof_PC2, bs = "ps") + s(sqrt_cape, bs = "ps"), 
  theta = ~ s(sqrt_lsp, bs = "ps")
)
@

Now we have all ingedients on hand to feed the standard interface for
statistical models in \proglang{R}: A formula \code{f}, a family
\code{ztnbinom_bamlss()}, and a data set \code{flash_austria_train}. Within the
\code{bamlss()} call we also provide arguments which are passed forward to the
optimizer and the sampler. We choose the gradient boosting optimizer
\code{boost()} in order to find initial values for the default sampler
\code{GMCMC()}. Gradient boosting proved to offer a very stable method for
finding regression coefficients that serve as initial values for a MCMC sampler
\citep{bamlss:Simon+Mayr+Umlauf+Zeileis:2019}.  We set the number of iteration
to $1000$. For the sampling we allow another $1000$ iterations as burn-in
phase, and apply a thinning of the resulting chain of $5$.  Running
\code{n.iter = 6000} iterations in total leads to $1000$ MCMC samples in the
end:

%%% <<application-model2, eval=FALSE>>=
%%% ## --- fit model ---
%%% b <- bamlss(f, family = "ztnbinom", data = flash_austria_train,
%%%   optimizer = boost, maxit = 1000,                  # boosting arguments
%%%   thin = 5, burnin = 1000, n.iter = 6000            # sampler arguments
%%% )
%%% @
%%% 
\begin{Schunk}
\begin{Sinput}
R> ## --- fit model ---
R> b <- bamlss(f, family = "ztnbinom", data = flash_austria_train,
+    optimizer = boost, maxit = 1000,                  # boosting arguments
+    thin = 5, burnin = 1000, n.iter = 6000            # sampler arguments
+  )
\end{Sinput}
\begin{Soutput}
logLik -36924.3 eps 0.0007 iteration 1000 qsel 7
elapsed time:  5.43min
Starting the sampler...
|********************| 100%  0.00sec 27.73min
\end{Soutput}
\end{Schunk}

The model was fitted on a single core Intel i7-7700 CPU with 3.60GHz and 16~GB memory,
on which the boosting took less than 6~minutes and the MCMC sampling took less the
30~minutes. As a first diagnostic we check the loglikelihood contributions of the
individual terms during the boosting optimization (Fig.~\ref{fig:appboost}).

<<app-plot-boost, eval = FALSE>>=
boost.plot(b, which = "loglik.contrib", intercept = FALSE)
@

\setkeys{Gin}{width=.8\textwidth}
\begin{figure}
\begin{center}
<<app-plot-boost, fig = TRUE, height = 4, width = 6, echo = FALSE>>=
boost.plot(b, c("loglik.contrib"), intercept = FALSE)
# layout(matrix(c(rep(1,2), rep(2,3)), nrow = 1))
# boost.plot(b, "loglik")
# boost.plot(b, "loglik.contrib")
@
\end{center}
\caption{Contribution to the log-likelihood of individual terms during
gradient boosting.}
\label{fig:appboost}
\end{figure}

After 1000~iterations the term \code{s(sqrt_cape).mu} has the highest contribution
to the loglikelihood with %
\Sexpr{round(flash_austria_model$model.stats$optimizer$boost.summary$summary$mu["s(sqrt_cape)",2])} %
followed by \code{s(q_prof_PC1).mu} with %
\Sexpr{round(flash_austria_model$model.stats$optimizer$boost.summary$summary$mu["s(q_prof_PC1)",2])}.
The term of the parameter $\theta$ \code{s(sqrt_lsp).theta} has a relatively small
contribution with %
\Sexpr{round(flash_austria_model$model.stats$optimizer$boost.summary$summary$theta["s(sqrt_lsp)",2])}.
The overall message of this diagnostic is that the contributions to the
loglikelihood at the end of the boosting procedure are very small and that
the algorithm approached a stable state, which suggest that we retrieve
reasonable initial values for the MCMC sampling.

The MCMC chains are investigated by looking directly at the traces of the chains
and with the auto-correlation function of the chains.

<<app-plot-trace, fig = FALSE, eval = FALSE>>=
plot(b, model = "mu", term = "s(sqrt_cape)", which = "samples")
@

Figure \ref{fig:apptrace} shows the traces and the auto-correlation functions
for two regression coefficients of the term \code{s(sqrt_cape)}. The traces
reveal samples around stables means. This suggests that the 1000 boosting
iterations and the 1000 burn-in samples were sufficient in order to approach
reasonable starting values for the sampling. The auto-correlation functions
reveal that after the thinning hardly any correlation remains between
consecutive samples.

\begin{figure}
\begin{center}
<<echo = FALSE, fig = TRUE>>=
b2 <- b
b2$samples[[1]] <- b2$samples[[1]][, grep("s(sqrt_cape)", colnames(b2$samples[[1]]), fixed = TRUE)][, 1:2]

par(mar = c(4.1, 4.1, 4.1, 1.1))
plot(b2, which = "samples", ask = FALSE)
@
\end{center}
\caption{MCMC trace (left panels) and auto-correlation (right panels) for two
splines from the term \code{s(sqrt\_cape)} of the model \code{mu}.}
\label{fig:apptrace}
\end{figure}

As these diagnostics suggest that a reasonable initial state for the sampling
has been found and the samples are independent draws from the posterior, one
can go further and investigate the estimated effects. The boosting summary
(Fig.~\ref{fig:appboost}) revealed that the terms \code{s(sqrt_cape)} and
\code{s(q_prof_PC1)} had a large contribution for improving the fit. 
Looking at these effects illustrate how the atmospheric parameters
of the reanalyses are related to lightning events (Fig.~\ref{fig:appeffect}),
and thus help to understand the physics associted with lightning events.
The effects are presented on the scale of the linear predictor, i.e., the
log scale.

<<app-plot-effect1, fig = FALSE, eval = FALSE>>=
plot(b, term = c("s(sqrt_cape)", "s(q_prof_PC1)", "s(sqrt_lsp)"))
@

\begin{figure}
\begin{center}
\setkeys{Gin}{width=1\textwidth}
<<echo = FALSE, fig = TRUE, height = 3, width = 6>>=
par(mfrow = c(1, 3))
plot(b, term = c("s(sqrt_cape)", "s(q_prof_PC1)", "s(sqrt_lsp)"),
  ask = FALSE, spar = FALSE, scheme = 2, grid = 200,
  rug = TRUE, col.rug = "#39393919")
@
\end{center}
\caption{Effect of the terms \code{s(sqrt\_cape)} and \code{s(q\_prof\_PC1)}
from the model \code{mu} and term \code{s(sqrt\_lsp)} from model \code{theta}.
Credible intervals derived from MCMC samples.}
\label{fig:appeffect}
\end{figure}

\code{s(sqrt_cape)} reveals a monotonic increasing shape. In the
range from $0$--$30$ the effect increases linearly with small credible intervals.
For higher values the effect flattens and shows large credibe intervals which are
associated with the samll amount of data in that range. Physically the shape of the
effect is meaningful as more convective available potential energy has the
potential to lead to heavier lightning events.
\code{s(q_prof_PC1)} reveals areas of large credible intervals at the left and
right bounds of the range due to small amount of data. In the mid-range a
increasing effect is identified. As \code{q_prof_PC1} is the leading principal
component of the vertical profile of specific humidity, one has to consider the
corresponding spatial mode (not shown) for interpretation: Higher values of
\code{q_prof_PC1} are linked to more moisture in the lower atmosphere,
which is also available as a source of \emph{latent energy}, i.e., energy that
becomes free when water transfers from the gas to the liquid phase.

Finally it is interesting to look at the effect acting on the link scale of
the parameter $\theta$, \code{s(sqrt_lsp)} (right panel in Fig.~\ref{fig:appeffect}).
\code{sqrt_lsp} is the square root of large scale precipitation, i.e.,
precipitation that is not linked to convective processes and thus it is
not related to strong lightning events. The effect shows following relationship:
Higher values of \code{sqrt_lsp} lead to smaller $\theta$, which increases the
variance of the distribution.

Before applying the model, i.e., predicting lightning cases before 2010, we
check the marginal calibration of the distribution by hanging rootogram, a tool
popular for the evaluation of count data regression models
\citep{kleiber2016visualizing}. First we predict the distributional parameter
on out-of-sample data \code{flash_austria_eval} for which lightning
observations are on hand.

<<app-rootogram1>>=
## --- out-of-sample prediciton ---
fit <- predict(b, newdata = flash_austria_eval, type = "parameter")
str(fit)
@

\code{predict()} returns a \code{list}, of which each element is named
as a distributional parameter and contains by default a vector of
predictions. Each prediction is the average of the preditions obtained
by all MCMC samples. The resulting \code{list} can be used to
derive further quantities by employing the functions of the \pkg{bamlss}
family that can be extracted using \code{family()},

<<app-rootogram2>>=
## --- extract family ---
fam <- family(b)
fam
@

The family contains functions to map the predictors to the parameter
scale, density, cumulative distribution function, log-likelihood, and
scores and hessian. We apply the density to compute the expected
frequencies of the positive counts. The function \code{...$d} takes the
quantile as first argument, and the \code{list} with the parameters,
as returned by \code{predict()}, as a second argument. 

<<app-rootogram3>>=
## --- expected frequencies ---
expect <- sapply(1:50, function(j) sum(fam$d(j, fit)))
@

In order to plot the rootogram, we have to name the vector and
coerce it to an object of class \code{table}. The verifying
observed frequencies can be directly obtained by \code{table}.

<<app-rootogram4>>=
names(expect) <- 1:50
expect <- as.table(expect)

## --- observed frequencies ---
obsrvd <- table(flash_austria_eval$counts)[1:50]
@

The observed and expected frequencies can be plugged into the
default method of \code{rootogram} from the \pkg{countreg}
package \citep{zeileis2008regression}.

<<app-plot-rootogram1, eval = FALSE>>=
## --- plot rootogram ---
library("countreg")
rootogram(obsrvd, expect, xlab = "# Counts", main = "Rootogram")
@

\setkeys{Gin}{width=.7\textwidth}
\begin{figure}
\begin{center}
<<app-plot-rootogram2, fig = TRUE, height = 5, width = 6, echo = FALSE>>=
## --- plot rootogram ---
library("countreg")
rootogram(obsrvd, expect, xlab = "# Counts", main = "Rootogram")
@
\end{center}
\caption{Hanging rootogram for evaluating calibration of count data model
on out-of-sample data. Red line indicates the expected frequencies on the
square root scale. Gray bars indicate observed frequencies on square root
scale hanging from the red line.}
\label{fig:approoto}
\end{figure}

The rootogram reveals reasonable calibration of the model though it is slightly
underestimating the number of events with a single lightning discharge.
Now given good convergence and sample characteristics of the gradient
boosting optimizer and MCMC sampler, physically interpretable effects, and
good out-of-sample calibration, we can take our model and predict a case
for the period before 2010, for which no lightning data are available.
The case of interest is a front moving from the West to the East on the
Northern side of the Alps on 2001-09-15 and 2001-09-16.
The case data \code{flash_austria_case} contains additional columns containing
time and space information, and is of class \code{sf} \citep{pebesma2018sf}.
We predict the parameters for this case, and derive the probability of
observing 10 or more flashes within a grid box conditioned on a thunderstorm activity,
by applying the cumulative distribution function \code{...$p} of the family.

<<app-plot-case1>>=
## --- predict case ---
library("sf")
fit <- predict(b, newdata = flash_austria_case, type = "parameter")
flash_austria_case$P10 <- 1 - fam$p(9, fit)
@

We visualize this case by employing \code{ggplot()} \citep{wickham2016gg},
and the Inferno color scale from the \pkg{colorspace} package
\citep{zeileis2019colorspace}. The country borders \code{world} are retrieved
from the \pkg{rnaturalearth} package \citep{south2017rnaturalearth}. 

<<app-plot-case2, eval = FALSE>>=
## --- plot case ---
library("ggplot2")
library("colorspace")

ggplot() + geom_sf(aes(fill = P10), data = flash_austria_case) +
  scale_fill_continuous_sequential("Inferno", rev = TRUE) +
  geom_sf(data = world, col = "white", fill = NA) +
  coord_sf(xlim = c(8, 17), ylim = c(45.5, 50), expand = FALSE) +
  facet_wrap(~time) + theme_minimal()
@

\setkeys{Gin}{width=1\textwidth}
\begin{figure}
\begin{center}
<<app-plot-case-eval, fig = TRUE, echo = FALSE>>=
library("ggplot2")
library("colorspace")

library("rnaturalearth")
library("rnaturalearthdata")
world <- ne_countries(scale = "medium", returnclass = "sf")

ggplot() + geom_sf(aes(fill = P10), data = flash_austria_case) +
  scale_fill_continuous_sequential("Inferno", rev = TRUE) +
  geom_sf(data = world, col = "white", fill = NA) +
  coord_sf(xlim = c(7.95, 17), ylim = c(45.45, 50), expand = FALSE) +
  facet_wrap(~time, nrow = 2) + theme_minimal() # +
#  theme(legend.position = "bottom")
@
\end{center}
\caption{A probabilistic reconstruction of lightning counts occured on
September 15 2001 at 6~UTC, 17~UTC and 23~UTC and on September 16 2001 at
13~UTC, i.e., the probability of having observed $10$ or more counts within one
grid box.}
\label{fig:appcase}
\end{figure}

The figure reveals that the probability for strong lightning events increases
during 2001-09-15 between 6 and 17~UTC. During night time the front occurs,
which can be nicely seen at 23~UTC. The propagation of the front is blocked
by the main Alpine ridge located at $47^\circ~N$. On the subsequent day
2001-09-16 one can see that the probability on the downwind side of the
Alps has increased.

\section{Achknowledgements}
T.~Simon acknowledges the funding by the Austrian Science Fund (FWF, grant no.~P31836)

\bibliography{bamlss}

\end{document}

