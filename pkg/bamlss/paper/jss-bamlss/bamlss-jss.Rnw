%\documentclass[article]{jss}
\documentclass[nojss]{jss}
\usepackage{amsmath,amssymb,amsfonts,thumbpdf}
\usepackage{multirow,longtable}

%% additional commands
\newcommand{\squote}[1]{`{#1}'}
\newcommand{\dquote}[1]{``{#1}''}
\newcommand{\fct}[1]{{\texttt{#1()}\index{#1@\texttt{#1()}}}}
\newcommand{\class}[1]{\dquote{\texttt{#1}}}
%% for internal use
\newcommand{\fixme}[1]{\emph{\marginpar{FIXME} (#1)}}
\newcommand{\readme}[1]{\emph{\marginpar{README} (#1)}}

%% Authors: NU + rest in alphabetical order
\author{Nikolaus Umlauf\\Universit\"at Innsbruck \And
        Achim Zeileis\\Universit\"at Innsbruck \AND
        Nadja Klein\\Humboldt-University of Berlin \And
        Thorsten Simon\\Universit\"at Innsbruck}
\Plainauthor{Nikolaus Umlauf, Achim Zeileis, Nadja Klein, Thorsten Simon}

\title{\pkg{bamlss}: A Lego Toolbox for {B}ayesian Additive Models for
  Location Scale and Shape (and Beyond)}
\Plaintitle{bamlss: A Lego Toolbox for Bayesian Additive
  Models for Location Scale and Shape (and Beyond)}
\Shorttitle{A Lego Toolbox for BAMLSS (and Beyond)}
	
\Keywords{GAMLSS, distributional regression, backfitting, gradient boosting, LASSO,
  MCMC, neural networks, \proglang{R}}
\Plainkeywords{GAMLSS, distributional regression, backfitting, gradient boosting, LASSO,
  MCMC, neural networks, R}

\Abstract{
During the last decades there has been an increasing interest in distributional regression 
models that allow to model all distributional parameters, such as location, scale and shape 
and thereby the entire data distribution conditional on covariates. In particular, the 
framework of structured additive distributional regression models enables to specify 
different types of effects such as linear, non-linear or interaction effects on all the 
distribution parameters hence providing a very flexible and generic framework suited for 
many complex real data problems. However, the implementation of new models is usually time-consuming
and complex, especially using Bayesian estimation algorithms. We propose an unified modeling
architecture, which is implemented in the \proglang{R} package \pkg{bamlss}, that makes it possible
to embed many different approaches suggested in literature and software. We show that
implementing (new) algorithms, non-standard distributions or effect types, or the integration of
already existing software, is relatively straightforward in this setting. We illustrate the
usefulness of the approach by implementing highly efficient algorithms for fully Bayesian inference
based on MCMC simulation, backfitting algorithms, gradient boosting, LASSO-type penalized models as
well as neural network distributional regression models.
}

\Address{
  Nikolaus Umlauf, Achim Zeileis, Thorsten Simon\\
  Department of Statistics\\
  Faculty of Economics and Statistics\\
  Universit\"at Innsbruck\\
  Universit\"atsstr.~15\\
  6020 Innsbruck, Austria\\
  E-mail: \email{Nikolaus.Umlauf@uibk.ac.at},\\
  \phantom{E-mail: }\email{Achim.Zeileis@R-project.org}\\
  \phantom{E-mail: }\email{Thorsten.Simon@uibk.ac.at}\\
  URL: \url{http://eeecon.uibk.ac.at/~umlauf/},\\
  \phantom{URL: }\url{http://eeecon.uibk.ac.at/~zeileis/}\\

  Nadja Klein\\
  Humboldt-University of Berlin\\
  School of Business and Economics\\
  Applied Statistics\\
  Unter den Linden 6\\
  10099 Berlin, Germany\\
  E-mail: \email{nadja.klein@hu-berlin.de}\\
  URL: \url{https://hu.berlin/NK}
}

\SweaveOpts{engine = R, eps = FALSE, keep.source = TRUE}

\input{defs}

\begin{document}
\SweaveOpts{concordance=TRUE}

\section{Introduction}

Generalized additive models for location, scale and shape
(GAMLSS,~\citealp{bamlss:Rigby+Stasinopoulos:2005}), also known as distributional regression models
\citep{bamlss:Klein+Kneib+Lang+Sohn:2015, bamlss:Klein+Kneib+Klasen+Lang:2015}, provide a very
flexible framework that enables modeling all parameters of a response distribution in terms of
covariates. While most common model specifications (linear models, generalized linear models [GLMs],
generalized additive models [GAMs], etc.) ignore the fact that the distribution of any quantity is
not well characterized by the mean alone, GAMLSS has the clear advantage to minimize the risk of
model misspecification and therefore provides much more reliable probabilistic forecasts.

The \proglang{R} package \pkg{gamlss}
\citep{bamlss:Stasinopoulos:Rigby:2007, bamlss:Stasinopoulos+Rigby:2019} and \pkg{VGAM}
\citep{bamlss:Yee:2009, bamlss:Yee:2019} were the first packages
to follow the distributional regression approach and comprise a quite flexible framework for
implementing response distributions. Variable selection algorithms for GAMLSS based on gradient
boosting were then introduced by the \pkg{gamboostLSS} package
\citep{bamlss:Hofner+Mayr+Schmid:2014, bamlss:Hofner:2018} and more recently, the contributed
base \proglang{R} package \pkg{mgcv} \citep{bamlss:Wood:2017, bamlss:Wood:2018} provides a
fitting routine for the estimation of general smooth models \citep{bamlss:Wood+Pya+Saefken:2016}.
The inferential framework of the packages mentioned is based on maximum likelihood, however, if
taken into account at all. For complex models, e.g., with response
distributions outside the exponential family or when multiple predictors contain several smooth
effects \citep{bamlss:Klein+Kneib+Lang:2015}, the maximum likelihood estimators based on
asymptotic properties might fail. In such situations the fully Bayesian approach using Markov chain
Monte Carlo (MCMC) simulation techniques is particularly attractive since valid credible intervals
are easily obtained from the posterior samples. While Bayesian distributional regression models
can in principle be estimated using general purpose MCMC software like
\pkg{JAGS}~\citep{bamlss:Plummer:2013}, \pkg{Stan}~\citep{bamlss:stan-software:2016} or
\pkg{WinBUGS}~\citep{bamlss:Lunn+Thomas+Best+Spiegelhalter:2000}, they all have the drawback
that sampling time is usually quite long, or it is even impossible to estimate distributional
regression models with GAM-type predictors, e.g., when using large data sets, modeling spatial
effects or complex higher-order interactions. The first software capable of estimating such
GAM-type distributional regression models using MCMC is the standalone package
\pkg{BayesX}~\citep{bamlss:Belitz+Brezger+Kneib+Lang+Umlauf:2017}, which provides highly
efficient sampling schemes for very large data sets as well as (spatial) multilevel models. However,
the implementation of new response distributions (models) in \pkg{BayesX} is not straightforward and users
usually have to ask the maintainers to do so.

In this paper, we present the \proglang{R} package \pkg{bamlss}, a flexible ``Lego toolbox''
for Bayesian distributional regression models (and beyond). The main contributions of \pkg{bamlss}
are the following:
%
\begin{itemize}
\item Easy implementation of (new) models and algorithms,
\item comparison of existing optimization algorithms and samplers,
\item or integration of existing implementations.
\end{itemize}
%
The package builds on the well-established \pkg{mgcv} infrastructures using
\proglang{R}'s formula syntax for model specification. Moreover, the package provides
commonly used extractor functions like \fct{summary}, \fct{plot}, \fct{predict}, etc., such
that users interested in developing new models or algorithms (Bayesian or frequentist) can
really concentrate on it.
The package is made available from the Comprehensive
\proglang{R} Archive Network (CRAN) at \url{http://CRAN.R-project.org/package=bamlss}.

The remainder of this paper is as follows. In Section~\ref{sec:motivation} a
motivating example illustrates the first steps using \pkg{bamlss}. Section~\ref{sec:distreg}
introduces distributional regression models in more detail. A thorough introduction of the
\proglang{R} package \pkg{bamlss}, describing the most important infrastructures and building blocks,
is then given in Section~\ref{sec:package}. In Section~\ref{sec:application} we highlight the
unified modeling approach using a complex distributional regression model for predicting
flash counts.

\section{Motivating example} \label{sec:motivation}

\section{Distributional regression} \label{sec:distreg}

In structured additive distributional regression we assume that the distribution of a $D$-dimensional random variable $(y_{i1},\ldots,y_{iD})'$, $i=1,\ldots,n$ given some covariates $\xvec_i$  has a parametric densit

$$p(y_{i1},\ldots,y_{iD}|\vartheta_{i1},\ldots,\vartheta_{iK}\equiv p_i.$$ 

\paragraph{Examples of distributions}

\paragraph{How does this work?}
The basic idea to implicitely model the entire distribution is to link each parameter $\vartheta_{ik}$ $k=1,\ldots,K$ of $p_i$ to a semiparametric structured additive predictor $\eta_{ik}$ formed of the covariates with the help of monotone, twice differentiable response functions $h_k$, such that $\vartheta_{ik}=h_k(\eta_{ik})$ and $\eta_{ik}=h_k^{-1}(\vartheta_{ik})$. Note, that  each predictor and each distribution parameter can be summarised in vectors of length $n$, $\etavec_k=(\eta_{1k},\ldots,\eta_{nk})'$, $\varthetavec_k=(\vartheta_{1k},\ldots\vartheta_{nk})'$, and that the subscript $k$ in the predictor is a notation to indicate which parameter the predictor belongs to.  The response function is usually chosen to maintain restrictions on the parameter space, like the exponential function $\vartheta_{ik}=\exp(\eta_{ik})$ to ensure a parameter with values on the positive real half axis, the identity function if the parameter space is unrestricted or $\vartheta_{ik}=\frac{\eta_{ik}}{\sqrt{1+(\eta_{ik})^2}}$ if $\vartheta_{ik}\in[-1,1]$, which is e.g.~the case for the correlation between two variables.

For any parameter of the multivariate distributions discussed in the previous section, the semiparametric predictor has the general form
\begin{equation}\label{eq:predictor}
 \eta_{i}^{\vartheta_k} = \sum_{j=1}^{J_k}f_{j}^{\vartheta_k}(\nuvec_i)
\end{equation}
comprising various functions $f_j^{\vartheta_k}(\nuvec_i)$ defined on the complete covariate information $\nuvec_i$. Specific components may for instance be given by
\begin{itemize}
\item linear functions $f_{j}^{\vartheta_k}(\nuvec_i)=\xvec'_{i}\beta_j^{\vartheta_k}$, including the overall level of the predictor as an intercept $\beta_{0j}^{\vartheta_k}$  and  $\xvec_i$ is a subvector of  $\nuvec_i$. Note that $\xvec_i$ may be chosen specifically for each parameter $\vartheta_k$ but we suppress this potential dependency in our notation.
\item continuous functions $f_{j}^{\vartheta_k}(\nuvec_i)=f_{j}^{\vartheta_k}(x_i)$, where $x_i$ is a single element of $\nuvec_i$ and $f_j$ is an appropriate smooth function to represent the effect of $x_i$ on $\vartheta_{ik}$.
\item spatial variations $f_{j}^{\vartheta_k}(\nuvec_i)=f_{j}^{\vartheta_k}(s_i)$, where $s_i$ represents spatial information, e.g. coordinate information in terms of longitude and latitude or discrete spatial information representing a fixed set of (administrative) geographical units.
\item random effects $f_{j}^{\vartheta_k}(\nuvec_i)=\beta_{j,g_i}^{\vartheta_k}$, where $g_i$ is a cluster variable that groups the observations.
\end{itemize}


We represent smooth terms in the models using basis functions, as a result of which the predictors can always be written in the generic matrix notation
\[
 \etavec^{\vartheta_k} = \sum_{j=1}^{J_k}\mZ_{j}^{\vartheta_k}\betavec_j^{\vartheta_k}
\]
where the design matrices $\mZ_{j}^{\vartheta_k}$ are obtained by evaluating the basis functions at observed covariate values  and  $\betavec_j^{\vartheta_k}$ are the vectors of basis coefficients to be estimated.

Specific properties of the basis coefficients such as smoothness are regularised by assuming possibly improper Gaussian priors
\begin{eqnarray}\label{eq:priorspec}
p\left(\betavec_j^{\vartheta_k}\bigg|(\tau_j^{\vartheta_k})^2\right) \propto \left(\frac{1}{(\tau_j^{\vartheta_k})^2}\right)^{\rank\left(\mK_j^{\vartheta_k}\right)/2}\exp\left(-\frac{1}{2(\tau_j^{\vartheta_k})^2}\left(\betavec_j^{\vartheta_k}\right)'\mK_j^{\vartheta_k}\betavec_j^{\vartheta_k}\right)
\end{eqnarray}
where $\mK_j^{\vartheta_k}$ is a prior precision matrix and $(\tau_j^{\vartheta_k})^2$ are smoothing variances. The latter are supplemented with inverse gamma hyperpriors, i.e. $(\tau_j^{\vartheta_k})^2~\sim\IGD(a_j^{\vartheta_k},b_j^{\vartheta_k})$ in order to obtain a data-driven amount of smoothness with $a_j^{\vartheta_k}=b_j^{\vartheta_k}=0.001$ as default values for practical analyses.
%Specific examples will be given in Section~\ref{sec:CM}.

As a result, each term $\fvec_j^{\vartheta_k}=(f_j^{\vartheta_k}(\nuvec_1),\ldots,f_j^{\vartheta_k}(\nuvec_n))'=\mZ_j^{\vartheta_k}\betavec_j^{\vartheta_k}$ is determined by a design matrix $\mZ_j^{\vartheta_k}$ and a prior precision or penalty matrix $\mK_j^{\vartheta_k}$. We give specific examples in the following (suppressing the index $j$ and superscript $\vartheta_k$):

\paragraph*{Continuous Covariates}
To approximate potentially nonlinear effects, we use Bayesian P-splines, compare \citet{eilers} and \citet{BreLan2006} for detailed explanations. The $n\times S$ design matrix $\mZ$ in this setting is composed of $S$ B-spline basis functions evaluated at observed covariates $x_i$. Assuming equidistant knots for the spline expansion, a first or second order random walk is a sensible choice for the prior of $\betavec$, i.e.
\begin{eqnarray*}
\beta_s|\beta_{s-1},\tau^2&\sim&\ND\left(\beta_{s-1},\tau^2\right),\quad s=2,\ldots,S\\
\text{or}\quad\quad\quad\qquad\qquad\quad\qquad\\
\beta_s|\beta_{s-1},\beta_{s-2},\tau^2&\sim&\ND\left(2\beta_{s-1}-\beta_{s-2},\tau^2\right),\quad s=3,\ldots,S
\end{eqnarray*}
with noninformative priors for initial values. This prior structure yields the penalty matrix $\mK = \mD'\mD$ where $\mD$ is a difference matrix of first or second order.

\paragraph*{Spatial Effects}
For discrete spatial effects observed on a lattice or regions, we consider Markov random fields, see \citet{RueHel2005}. Let $s_i\in\lbrace 1,\ldots,S\rbrace$ denote the index or region observation $i$ belongs to. Then $f(s_i) = \beta_{s_i}$ is assumed such that we estimate separate parameters $\beta_1,\ldots,\beta_S$ for each region. As a consequence, the $n\times S$ design matrix is an incidence matrix, i.e. $\mZ[i,s]=1$ if observation $i$ belongs to location $s$ and zero otherwise. The simplest Markov random field prior for the coefficients $\beta_s$ is defined by
\begin{equation*}
\beta_{s}|\beta_{r},\;r\neq s,\;\tau^2\sim\ND\left(\sum_{r\in\partial_s}\frac{1}{N_s}\beta_{r},\frac{\tau^2}{N_s}\right),
\end{equation*}
where $\partial_s$ denotes the set of neighbours of region $s$ and $N_s$ is the number of regions in $\partial_s$. The penalty matrix is then given by
\begin{eqnarray*}
 \mK[s,r] &=& \begin{cases}
 -1 & s\neq r,\quad r\in\partial_s\\
 0 & s\neq r,\quad r\notin\partial_s\\
 N_s& s=r.
 \end{cases}
\end{eqnarray*}

For detailed explanations on structured additive regression with further examples we refer the reader to~\citet{fahkne13}.


\section{The bamlss package} \label{sec:package}

\section{Application} \label{sec:application}

\bibliography{bamlss}

\end{document}

