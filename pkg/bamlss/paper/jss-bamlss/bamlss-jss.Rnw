%\documentclass[article]{jss}
\documentclass[nojss]{jss}
\usepackage{amsmath,amssymb,amsfonts,thumbpdf}
\usepackage{multirow,longtable}

%% additional commands
\newcommand{\squote}[1]{`{#1}'}
\newcommand{\dquote}[1]{``{#1}''}
\newcommand{\fct}[1]{{\texttt{#1()}\index{#1@\texttt{#1()}}}}
\newcommand{\class}[1]{\dquote{\texttt{#1}}}
%% for internal use
\newcommand{\fixme}[1]{\emph{\marginpar{FIXME} (#1)}}
\newcommand{\readme}[1]{\emph{\marginpar{README} (#1)}}

%% Authors: NU + rest in alphabetical order
\author{Nikolaus Umlauf\\Universit\"at Innsbruck \And
        Achim Zeileis\\Universit\"at Innsbruck \AND
        Nadja Klein\\Humboldt-University of Berlin \And
        Thorsten Simon\\Universit\"at Innsbruck}
\Plainauthor{Nikolaus Umlauf, Achim Zeileis, Nadja Klein, Thorsten Simon}

\title{\pkg{bamlss}: A Lego Toolbox for {B}ayesian Additive Models for
  Location Scale and Shape (and Beyond)}
\Plaintitle{bamlss: A Lego Toolbox for Bayesian Additive
  Models for Location Scale and Shape (and Beyond)}
\Shorttitle{A Lego Toolbox for BAMLSS (and Beyond)}
	
\Keywords{GAMLSS, distributional regression, backfitting, gradient boosting, LASSO,
  MCMC, neural networks, \proglang{R}}
\Plainkeywords{GAMLSS, distributional regression, backfitting, gradient boosting, LASSO,
  MCMC, neural networks, R}

\Abstract{
During the last decades there has been an increasing interest in distributional regression 
models that allow to model all distributional parameters, such as location, scale and shape 
and thereby the entire data distribution conditional on covariates. In particular, the 
framework of structured additive distributional regression models enables to specify 
different types of effects such as linear, non-linear or interaction effects on all the 
distribution parameters hence providing a very flexible and generic framework suited for 
many complex real data problems. However, the implementation of new models is usually time-consuming
and complex, especially using Bayesian estimation algorithms. We propose an unified modeling
architecture, which is implemented in the \proglang{R} package \pkg{bamlss}, that makes it possible
to embed many different approaches suggested in literature and software. We show that
implementing (new) algorithms, non-standard distributions or effect types, or the integration of
already existing software, is relatively straightforward in this setting. We illustrate the
usefulness of the approach by implementing highly efficient algorithms for fully Bayesian inference
based on MCMC simulation, backfitting algorithms, gradient boosting, LASSO-type penalized models as
well as neural network distributional regression models.
}

\Address{
  Nikolaus Umlauf, Achim Zeileis, Thorsten Simon\\
  Department of Statistics\\
  Faculty of Economics and Statistics\\
  Universit\"at Innsbruck\\
  Universit\"atsstr.~15\\
  6020 Innsbruck, Austria\\
  E-mail: \email{Nikolaus.Umlauf@uibk.ac.at},\\
  \phantom{E-mail: }\email{Achim.Zeileis@R-project.org}\\
  \phantom{E-mail: }\email{Thorsten.Simon@uibk.ac.at}\\
  URL: \url{http://eeecon.uibk.ac.at/~umlauf/},\\
  \phantom{URL: }\url{http://eeecon.uibk.ac.at/~zeileis/}\\

  Nadja Klein\\
  Humboldt-University of Berlin\\
  School of Business and Economics\\
  Applied Statistics\\
  Unter den Linden 6\\
  10099 Berlin, Germany\\
  E-mail: \email{nadja.klein@hu-berlin.de}\\
  URL: \url{https://hu.berlin/NK}
}

\SweaveOpts{engine = R, eps = FALSE, keep.source = TRUE}

<<preliminaries, echo=FALSE, results=hide>>=
options(width = 70, prompt = "R> ", continue = "+  ",
  SweaveHooks = list(fig = function() par(mar = c(4.1, 4.1, 1, 1))))
library("bamlss")
@

\input{defs}

\begin{document}
\SweaveOpts{concordance=TRUE}

\section{Introduction}

Generalized additive models for location, scale and shape
(GAMLSS,~\citealp{bamlss:Rigby+Stasinopoulos:2005}), also known as distributional regression models
\citep{bamlss:Klein+Kneib+Lang+Sohn:2015, bamlss:Klein+Kneib+Klasen+Lang:2015}, provide a very
flexible framework that enables modeling all parameters of a response distribution in terms of
covariates. While most common model specifications (linear models, generalized linear models [GLMs],
generalized additive models [GAMs], etc.) ignore the fact that the distribution of any quantity is
not well characterized by the mean alone, GAMLSS has the clear advantage to minimize the risk of
model misspecification and therefore provides much more reliable probabilistic forecasts.

The \proglang{R} package \pkg{gamlss}
\citep{bamlss:Stasinopoulos:Rigby:2007, bamlss:Stasinopoulos+Rigby:2019} and \pkg{VGAM}
\citep{bamlss:Yee:2009, bamlss:Yee:2019} were the first packages
to follow the distributional regression approach and comprise a quite flexible framework for
implementing response distributions. Variable selection algorithms for GAMLSS based on gradient
boosting were then introduced by the \pkg{gamboostLSS} package
\citep{bamlss:Hofner+Mayr+Schmid:2014, bamlss:Hofner:2018} and more recently, the contributed
base \proglang{R} package \pkg{mgcv} \citep{bamlss:Wood:2017, bamlss:Wood:2019} provides a
fitting routine for the estimation of general smooth models \citep{bamlss:Wood+Pya+Saefken:2016}.
The inferential framework of the packages mentioned is based on maximum likelihood, however, if
taken into account at all. For complex models, e.g., with response
distributions outside the exponential family or when multiple predictors contain several smooth
effects \citep{bamlss:Klein+Kneib+Lang:2015}, the maximum likelihood estimators based on
asymptotic properties might fail. In such situations the fully Bayesian approach using Markov chain
Monte Carlo (MCMC) simulation techniques is particularly attractive since valid credible intervals
are easily obtained from the posterior samples. While Bayesian distributional regression models
can in principle be estimated using general purpose MCMC software like
\pkg{JAGS}~\citep{bamlss:Plummer:2013}, \pkg{Stan}~\citep{bamlss:stan-software:2016} or
\pkg{WinBUGS}~\citep{bamlss:Lunn+Thomas+Best+Spiegelhalter:2000}, they all have the drawback
that sampling time is usually quite long, or it is even impossible to estimate distributional
regression models with GAM-type predictors, e.g., when using large data sets, modeling spatial
effects or complex higher-order interactions. The first software capable of estimating such
GAM-type distributional regression models using MCMC is the standalone package
\pkg{BayesX}~\citep{bamlss:Belitz+Brezger+Kneib+Lang+Umlauf:2017}, which provides highly
efficient sampling schemes for very large data sets as well as (spatial) multilevel models. However,
the implementation of new response distributions (models) in \pkg{BayesX} is not straightforward and users
usually have to ask the maintainers to do so.

In this paper, we present the \proglang{R} package \pkg{bamlss}, a flexible ``Lego toolbox''
for Bayesian distributional regression models (and beyond). The main contributions of \pkg{bamlss}
are the following:
%
\begin{itemize}
\item Estimate distributional regression models with usual \proglang{R} modeling ``look \& feel'',
\item easy implementation of (new) models and algorithms (Bayesian or frequentist),
\item comparison of existing optimization algorithms and samplers,
\item or integration of existing implementations.
\end{itemize}
%
The package builds on the well-established \pkg{mgcv} infrastructures using
\proglang{R}'s formula syntax for model specification. Moreover, the package provides
commonly used extractor functions like \fct{summary}, \fct{plot}, \fct{predict}, etc., such
that users interested in developing new models or algorithms (Bayesian or frequentist) can
really concentrate on it.
The package is made available from the Comprehensive
\proglang{R} Archive Network (CRAN) at \url{http://CRAN.R-project.org/package=bamlss}.

The remainder of this paper is as follows. In Section~\ref{sec:motivation},
two motivating examples illustrate the first steps using \pkg{bamlss}. Section~\ref{sec:distreg}
introduces distributional regression models in more detail. A thorough introduction of the
\proglang{R} package \pkg{bamlss}, describing the most important infrastructures and building blocks,
is then given in Section~\ref{sec:package}. In Section~\ref{sec:application} we highlight the
unified modeling approach using a complex distributional regression model for predicting
flash counts.


\section{Motivating examples} \label{sec:motivation}

This section gives a first quick overview of the basic functionality of the package.
Readers more interested in advanced examples using \pkg{bamlss} could skip this
section and move to Section~\ref{sec:application} directly. The first example
demonstrates that the usual ``look \& feel'' when using well-established model fitting 
functions like \fct{glm} is an elementary part of \pkg{bamlss}, i.e., first steps and
basic handling of the package should be relatively simple. The second example then
explains how full distributional regression models can be estimated and show
cases the flexibility of the provided modeling infrastructures.

\subsection{Logit model}

This example is taken from the \pkg{AER} package \citep{bamlss:Kleiber+Zeileis:2008} and is about
labor force participation (yes/no) of women in Switzerland 1981 \citep{bamlss:Gerfin:1996}.
The data can be loaded with
<<>>=
data("SwissLabor", package = "AER")
@
The data frame contains of 872 observations on 7 variables, where some of them might have
a nonlinear influence on the response labor \code{participation}. Now, a standard Bayesian 
binomial logit model using the default MCMC algorithm can be fitted with
<<echo = FALSE, results = hide>>=
if(!file.exists("SwissLaborModel.rda")) {
  f <- participation ~ income + age + education + youngkids + oldkids + foreign + I(age^2)
  set.seed(123)
  b <- bamlss(f, family = "binomial", data = SwissLabor,
    n.iter = 12000, burnin = 2000, thin = 10)
  save(b, file = "SwissLaborModel.rda")
} else {
  load("SwissLaborModel.rda")
}
@
<<eval = FALSE>>=
library("bamlss")

## Model formula.
f <- participation ~ income + age + education +
  youngkids + oldkids + foreign + I(age^2)

## First, set the seed for reproducibility.
set.seed(123)

## Estimate model.
b <- bamlss(f, family = "binomial", data = SwissLabor,
  n.iter = 12000, burnin = 2000, thin = 10)
@
using 12000 iterations with a burnin-phase of 2000 and a thinning parameter of 10. Prior
the MCMC sampling a backfitting algorithm for posterior mode estimation is performed. The
posterior mode estimates are then used as starting values for MCMC. Using
the main model fitting function \fct{bamlss} all model fitting engines can be exchanged, 
which is explained in detail in Section~\ref{sec:package} and the application
Section~\ref{sec:application}. The default model fitting engines use family objects
(see Section~\ref{sec:package}), similar to the families that can be used
with the \fct{glm} function, which enables easy implementation of new distributions (models).

Note, to capture nonlinearities, a quadratic term for variable \code{age} is added to
the model. The resulting object \code{b} is of class \code{"bamlss"} for which standard
extractor functions like \fct{summary}, \fct{coef}, \fct{plot}, \fct{predict}, etc.\
are available. The model summary output is printed by
<<>>=
summary(b)
@
and is based on MCMC samples, which suggest ``significant'' effects for all covariates, 
except for variable \code{education}, since the 95\% credible interval contains zero.
In addition, the acceptance probabilities \code{alpha} are reported and indicate
proper behavior of the MCMC algorithm.
The column \code{parameters} shows respective posterior mode estimates of the regression
coefficients, which are calculated by the upstream backfitting algorithm. Before 
proceeding the analysis, users usually perform additional convergence checks of the
MCMC chains by looking at traceplots and auto-correlation.
<<eval = FALSE>>=
plot(b, which = c("samples", "max-acf"))
@
\begin{figure}[!ht]
\centering
\setkeys{Gin}{width=1\textwidth}
<<echo = FALSE, fig = TRUE, height = 3, width = 8>>=
par(mfrow = c(1, 3), mar = c(4.1, 4.1, 4.1, 1.1))
bsp <- b
bsp$samples <- bsp$samples[, c("pi.p.(Intercept)"), drop = FALSE]
plot(bsp, which = "samples", spar = FALSE, ask = FALSE)
plot(b, which = "max-acf", spar = FALSE, ask = FALSE)
@
\caption{\label{fig:logit_traceplots} Logit model, MCMC trace (left panel),
  auto-correlation for the intercept (middle panel), maximum auto-correlation for all
  parameters (right panel).}
\end{figure}
These are visualized in Figure~\ref{fig:logit_traceplots} and reveal convergence of
the MCMC chains, i.e., there is no visible trend and the very low
auto-correlation shown for the intercept and the maximum auto-correlation of all
parameters suggests i.i.d.\ samples from the posterior distribution.
Note that the function call would compute all trace- and auto-correlation plots, however,
for convenience we only show plots for the intercept. In addition, samples can also be
extracted using function \fct{samples}, which returns an object of class \code{"mcmc"},
a class provided by the \pkg{coda} package \citep{bamlss:Plummer+Best+Cowles+Vines:2006}.
This package includes a rich infrastructure for further convergence diagnostic checks,
e.g., Gelman and Rubin's convergence diagnostic
\citep{bamlss:Gelman+Rubin:1992, bamlss:Brooks+Gelman:1998} or
Heidelberger and Welch's convergence diagnostic
\citep{bamlss:Heidelberger+Welch:1981, bamlss:Heidelberger+Welch:1983}.

Model predictions on the probability scale can be obtained by the predict method, e.g.,
to visualize the effect of covariate \code{age} on the probability we can do the following:
<<>>=
## Create new data for prediction.
nd <- data.frame(income = 11, age = seq(2, 6.2, length = 100),
  education = 12, youngkids = 1, oldkids = 1, foreign = "no")

## Predict for both cases of variable foreign.
nd$p.no <- predict(b, newdata = nd, type = "parameter", FUN = c95)
nd$foreign <- "yes"
nd$p.yes <- predict(b, newdata = nd, type = "parameter", FUN = c95)
@
The predict method is applied on all MCMC samples and argument \code{FUN} specifies a function
that can be applied on the predictor or distribution parameter samples. The default is the \fct{mean} function,
however, in this case we additionally extract the empirical 2.5\% and 97.5\% quantiles
using function \fct{c95} to obtain credible intervals (note, individual samples can be
extracted by passing \code{FUN = function(x) \{ return(x) \}}, i.e., this way users can
easily generate their own statistics). Then, the estimated effect can be visualized with
<<eval = FALSE>>=
plot2d(p.no ~ age, data = nd, ylab = "participation",
  ylim = range(c(nd$p.no, nd$p.yes)), lty = c(2, 1, 2))
plot2d(p.yes ~ age, data = nd, col.lines = "blue", add = TRUE,
  lty = c(2, 1, 2))
@
\begin{figure}[!ht]
\centering
\setkeys{Gin}{width=0.4\textwidth}
<<echo = FALSE, fig = TRUE, width = 4.5, height = 3.8>>=
par(mar = c(4.1, 4.1, 0.1, 1.1))
plot2d(p.no ~ age, data = nd, ylab = "participation",
  ylim = range(c(nd$p.no, nd$p.yes)), lty = c(2, 1, 2))
plot2d(p.yes ~ age, data = nd, col.lines = "blue", add = TRUE,
  lty = c(2, 1, 2))
legend("topright", c("foreign yes", "foreign no"), lwd = 1,
  col = c("blue", "black"), box.col = NA, bg = NA)
@
\caption{\label{fig:logit_effects} Effect of covariate \code{age} on estimated
  probabilities for both cases, \code{foreign} \code{"yes"} and \code{"no"}. The solid lines
  represent mean estimates, dashed lines represent the 95\% credible interval.}
\end{figure}
The estimates are shown in Figure~\ref{fig:logit_effects} and suggest a clear difference 
for the effect of \code{age} between both cases of factor variable \code{foreign}.

\subsection{Normal location-scale model}

In this example we will now extend the framework and estimate a complete distributional regression
model using a small textbook example of the well-known simulated motorcycle accident data
\citep{bamlss:Silverman:1985}.
<<>>=
data("mcycle", package = "MASS")
@
The data set contains measurements of the head acceleration
(in $g$, variable \code{accel}) in a simulated motorcycle accident, recorded in milliseconds after
impact (variable \code{times}). To estimate a normal location-scale model with
$$
\texttt{accel} \sim \mathcal{N}(\mu = f_{\mu}(\texttt{times}),
  \log(\sigma) = f_{\sigma}(\texttt{times}))
$$
where functions $f_{\mu}( \cdot )$ and $f_{\sigma}( \cdot )$ are unspecified smooth
functions, which are estimated using regression splines. The log-link for parameter
$\sigma$ ensures positivity. We can use the following model formula for estimation
<<>>=
f <- list(accel ~ s(times, k = 20), sigma ~ s(times, k = 20))
@
where function \fct{s} is the smooth term constructor from the \pkg{mgcv} package
\citep{bamlss:Wood:2019}, the default of \fct{s} are thin-plate regression splines.
Note that model formulae are provided as lists of formulae, i.e., each list entry
represents one parameter of the response distribution. Moreover, note that all smooth
terms, i.e., \fct{te}, \fct{ti}, etc., are supported by \pkg{bamlss}. This way, it is
also possible to incorporate user defined model terms. A full Bayesian semi-parametric 
distributional regression model can be estimated with
<<echo = FALSE, results = hide>>=
if(!file.exists("toymodel.rda")) {
  set.seed(456)
  b <- bamlss(f, data = mcycle, family = "gaussian",
    n.iter = 12000, burnin = 2000, thin = 10)
  save(b, file = "toymodel.rda")
} else {
  load("toymodel.rda")
}
@
<<eval = FALSE>>=
## Set the seed for reproducibility.
set.seed(456)

## Estimate normal location-scale model.
b <- bamlss(f, data = mcycle, family = "gaussian",
  n.iter = 12000, burnin = 2000, thin = 10)
@
again using 12000 iterations for the MCMC chain, a burnin of 2000 and a thinning
of parameter of 10. After the estimation algorithms are finished, the estimated effects can be
visualized instantly using the plotting method.
<<eval = FALSE>>=
plot(b, model = c("mu", "sigma"))
@
\begin{figure}[!ht]
\centering
\setkeys{Gin}{width=0.4\textwidth}
<<echo = FALSE, fig = TRUE, width = 4.5, height = 3.8>>=
par(mar = c(4.1, 4.1, 0.1, 1.1))
plot(b, model = "mu", spar = FALSE, scheme = 2, grid = 200)
@
<<echo = FALSE, fig = TRUE, width = 4.5, height = 3.8>>=
par(mar = c(4.1, 4.1, 0.1, 1.1))
plot(b, model = "sigma", spar = FALSE, scheme = 2, grid = 200)
@
\caption{\label{fig:toy_effects} Estimated effects of \code{times} on parameter
  $\mu$ and $\sigma$ of the normal location-scale model. The grey shaded areas represent
  95\% credible intervals.}
\end{figure}
The estimated effects are shown in Figure~\ref{fig:toy_effects} depicting a clear nonlinear
relationship for parameter $\mu$ and $\sigma$.

For judging how well the model fits to the data the user can inspect randomized quantile
residuals \citep{bamlss:Dunn:Smyth:1996} using histograms or quantile-quantile plots. 
Residuals can be extracted using function \fct{residuals} and has a plotting method. 
Alternatively, residuals can be investigated with
<<eval = FALSE>>=
plot(b, which = c("hist-resid", "qq-resid"))
@
\begin{figure}[!ht]
\centering
\setkeys{Gin}{width=0.4\textwidth}
<<echo = FALSE, fig = TRUE, width = 4.5, height = 4.7>>=
par(mar = c(4.1, 4.1, 4.1, 1.1))
plot(b, which = "hist-resid", spar = FALSE, col = "lightgray")
@
<<echo = FALSE, fig = TRUE, width = 4.5, height = 4.7>>=
par(mar = c(4.1, 4.1, 4.1, 1.1))
plot(b, which = "qq-resid", spar = FALSE)
@
\caption{\label{fig:toy_resids} Histogram and quantile-quantile plot of the
  resulting randomized quantile residuals of the normal location-scale model.}
\end{figure}
According the histogram and the quantile-quantile plot of the resulting randomized
quantile residuals in Figure~\ref{fig:toy_resids}, the model seems to fit relatively well. 
Only for very low and very high values of \code{accel} the fitted distributions seem
to be less appropriate.

Besides residuals, users can evaluate the model performance, e.g., for model selection 
based on the deviance information criterion (DIC), which can be extracted using function 
\fct{DIC}
<<>>=
DIC(b)
@
and is also reported in the model summary output. Furthermore, statistical calibration
of fitted models can be assessed by scoring rules
\citep{bamlss:Gneiting+Balabdaoui+Raftery:2007}. For example, 
the \proglang{R} package \pkg{scoringRules} \citep{bamlss:Jordan+Krueger+Lerch:2018} 
provides easy evaluation of the continuous rank probability score (CRPS) for a couple of 
distributions.


\section{Distributional regression} \label{sec:distreg}

\subsection{Model structure}

\subsubsection{Response distribution}
In structured additive distributional regression we assume that the distribution of a $D$-dimensional random variable $(y_{i1},\ldots,y_{iD})'$, $i=1,\ldots,n$ given some covariates $\xvec_i$  has a parametric density

$$p_Y(y_{i1},\ldots,y_{iD}|\vartheta_{i1},\ldots,\vartheta_{iK})\equiv p_{Y_i}.$$ 
There is a hugh number of avaiable families in \texttt{bamlss} and it is also possible to define new families as described later.
% \paragraph{Examples:}
% 
% \textbf{maybe add here the distribution from the intro and introduce \texttt{familyname_bamlss()}?}
% 

The basic idea to implicitely model the entire distribution is to link each parameter $\vartheta_{ik}$ $k=1,\ldots,K$ of $p_i$ to a semiparametric structured additive predictor $\eta_{ik}$ formed of the covariates with the help of monotone, twice differentiable response functions $h_k$, such that 
$$\vartheta_{ik}=h_k(\eta_{ik})\mbox{ and }\eta_{ik}=h_k^{-1}(\vartheta_{ik}).$$ 
Note, that  each predictor and each distribution parameter can be summarised in vectors of length $n$, $\etavec_k=(\eta_{1k},\ldots,\eta_{nk})'$, $\varthetavec_k=(\vartheta_{1k},\ldots\vartheta_{nk})'$, and that the subscript $k$ in the predictor is a notation to indicate which parameter the predictor belongs to.

% \paragraph{Example:}
% 
% \textbf{something in R code here?}

 \paragraph{Examples:} Examples of distributional models that fit well in this framework are the ones for
 \begin{itemize}
 \item Univariate responses of any type, e.g.~counts with zero-inflation and or overdispersion as proposed in \citet{bamlss:Klein+Kneib+Lang:2015,bamlss:Herwartz:Klein+Strumann:2016}, continuous responses with spikes, skewness, heavy tails or bounded support as in \citet{bamlss:Klein+Kneib+Lang+Sohn:2015,bamlss:Klein+Denuit+Kneib+Lang:2014}, as well as responses for extreme events \citep{bamlss:Umlauf+Kneib:2018}
 \item multivariate responses such as multivariate normal, multivariate t or Dirichlet regression (for analysing compositional data) \citet{bamlss:Klein+Kneib+Klasen+Lang:2015}
 \item multivariate responses with more complex dependence structures modelled through copulas \citet{bamlss:Klein+Kneib:2016b}
 \item survival data and joint modelling~\citep{bamlss:Koehler+Umlauf+Greven:2016}.
 \item \textbf{Anything else here?}
\end{itemize}

The response function $h_k$ is usually chosen to maintain restrictions on the parameter space, like 
\begin{itemize}
\item the identity function $\vartheta_{ik}=\exp(\eta_{ik})$ if $\vartheta_{ik}\in\mathds{R}$,
\item the exponential function $\vartheta_{ik}=\exp(\eta_{ik})$ to ensure a parameter with values on the positive real half axis (such as variances), 
\item the identity function if the parameter space is unrestricted or $\vartheta_{ik}=\frac{\eta_{ik}}{\sqrt{1+(\eta_{ik})^2}}$ if $\vartheta_{ik}\in[-1,1]$ (for correlation parameters for instance),
\item the logit or probit link for $\vartheta_{ik}\in [0,1]$ for probabilities.
\end{itemize}



\subsubsection{Structured predictors}
For any parameter of the distribution $p_i$, the semiparametric predictor has the general form
\begin{equation}\label{eq:predictor}
 \eta_{ik} = \sum_{j=1}^{J_k}f_{jk}(\nuvec_i)
\end{equation}
comprising various functions $f_{jk}(\nuvec_i)$ defined on the complete covariate information $\nuvec_i$. Specific components may for instance be given by
\begin{itemize}
\item linear functions $f_{jk}(\nuvec_i)=\xvec'_{i}\beta_{jk}$, including the overall level of the predictor as an intercept $\beta_{0jk}$  and  $\xvec_i$ is a subvector of  $\nuvec_i$. Note that $\xvec_i$ may be chosen specifically for each parameter $\vartheta_k$ but we suppress this potential dependency in our notation.
\item continuous functions $f_{jk}=f_{jk}(x_i)$, where $x_i$ is a single element of $\nuvec_i$ and $f_j$ is an appropriate smooth function to represent the effect of $x_i$ on $\vartheta_{ik}$.
\item spatial variations $f_{jk}(\nuvec_i)=f_{jk}(s_i)$, where $s_i$ represents spatial information, e.g. coordinate information in terms of longitude and latitude or discrete spatial information representing a fixed set of (administrative) geographical units.
\item random effects $f_{jk}(\nuvec_i)=\beta_{jk,g_i}$, where $g_i$ is a cluster variable that groups the observations.
\end{itemize}

For detailed explanations on structured additive regression with further examples we refer the reader to~\citet{bamlss:Fahrmeir+Kneib+Lang+Marx:2013,Woo2017} and \citet{bamlss:Kneib+Klein+Lang+Umlauf:2019} for a special focus on general tensor product interactions.


We represent smooth terms in the models using basis functions, as a result of which the predictors can always be written in the generic matrix notation
\[
 \etavec_k = \sum_{j=1}^{J_k}\mZ_{jk}\betavec_{jk}
\]
where the design matrices $\mZ_{jk}$ are obtained by evaluating the basis functions at observed covariate values  and  $\betavec_{jk}$ are the vectors of basis coefficients to be estimated.



Specific properties of the basis coefficients such as smoothness are regularised by assuming possibly improper  priors
$$
p(\betavec_{jk}|\tauvec_{jk}) \propto \left(\frac{1}{\tau_{jk}^2}\right)^{\rank\left(\mK_{jk}\right)/2}\exp\left(-\frac{1}{2\tau_{jk}^2}\betavec_{jk}'\mK_{jk}\betavec_{jk}\right)
$$
with prior precision matrices $\mK_{jk}$ and hyperparameters $\tauvec_{jk}$ which we can also supplement with a hyperprior distribution $p(\tauvec_{jk}|\alphavec_{jk})$, and where we assume $\alphavec_{jk}$ to be fixed or elicited. The default in distributional regression is multivariate Gaussian priors for $\betavec_{jk}|\tau_{jk}$ and inverse gamma priors for the smoothing variances $\tau_{jk}^2$. 
However, there is many more advanced options to include variable selection, shrinkage and prior elicitation. Specifically, we provide
\begin{itemize}
\item improved priors for the smoothing variances~\citep{bamlss:Klein+Kneib:2016},
\item Lasso type regularization \citep{bamlss:Groll+Kneib+Umlauf:2019}
\item Bayesian effect selection through spike-and-slab priors \citep{bamlss:Klein+Carlan+Kneib+Lang+Wagner:2019:arxiv}
\end{itemize}


For the examples above, possible choices for $\mK_{jk}$ would be
\begin{itemize}
\item linear functions: $\mK_{jk}=\nullvec$ or $\mK_{jk}=\mI$, corresponding to flat/ridge type priors.
\item continuous functions: $\mK_{jk}=\mD_l'\mD_l$, where $\mD_l$ is a $l$-th order difference matrix.
\item spatial variations: a Markov random field prior
\item random effects: $\mK_{jk}=\mI$
\end{itemize}



%where $\mK_{jk}$ is a prior precision matrix and $\tau_{jk}^2$ are smoothing variances. \textbf{discuss different choices of hyperpriors here?} %The latter are supplemented with inverse gamma hyperpriors, i.e. $(\tau_j^{\vartheta_k})^2~\sim\IGD(a_j^{\vartheta_k},b_j^{\vartheta_k})$ in order to obtain a data-driven amount of smoothness with $a_j^{\vartheta_k}=b_j^{\vartheta_k}=0.001$ as default values for practical analyses.
%Specific examples will be given in Section~\ref{sec:CM}.

%As a result, each term $\fvec_{jk}=(f_{jk}(\nuvec_1),\ldots,f_{jk}(\nuvec_n))'=\mZ_{jk}\betavec_{jk}$ is determined by a design matrix $\mZ_{jk}$ and a prior precision or penalty matrix $\mK_{jk}$. We give two specific examples in the following (suppressing the index $j$ and superscript $\vartheta_k$):

% \begin{itemize}
% \item {Continuous Covariates:}
% \item {Continuous Covariates:}
% To approximate potentially nonlinear effects, we use Bayesian P-splines, compare \citet{eilers} and \citet{BreLan2006} for detailed explanations. The $n\times S$ design matrix $\mZ$ in this setting is composed of $S$ B-spline basis functions evaluated at observed covariates $x_i$. Assuming equidistant knots for the spline expansion, a first or second order random walk is a sensible choice for the prior of $\betavec$, i.e.
% \begin{eqnarray*}
% \beta_s|\beta_{s-1},\tau^2&\sim&\ND\left(\beta_{s-1},\tau^2\right),\quad s=2,\ldots,S\\
% \text{or}\quad\quad\quad\qquad\qquad\quad\qquad\\
% \beta_s|\beta_{s-1},\beta_{s-2},\tau^2&\sim&\ND\left(2\beta_{s-1}-\beta_{s-2},\tau^2\right),\quad s=3,\ldots,S
% \end{eqnarray*}
% with noninformative priors for initial values. This prior structure yields the penalty matrix $\mK = \mD'\mD$ where $\mD$ is a difference matrix of first or second order.
% 
% \item {Spatial Effects:}
% For discrete spatial effects observed on a lattice or regions, we consider Markov random fields, see \citet{RueHel2005}. Let $s_i\in\lbrace 1,\ldots,S\rbrace$ denote the index or region observation $i$ belongs to. Then $f(s_i) = \beta_{s_i}$ is assumed such that we estimate separate parameters $\beta_1,\ldots,\beta_S$ for each region. As a consequence, the $n\times S$ design matrix is an incidence matrix, i.e. $\mZ[i,s]=1$ if observation $i$ belongs to location $s$ and zero otherwise. The simplest Markov random field prior for the coefficients $\beta_s$ is defined by
% \begin{equation*}
% \beta_{s}|\beta_{r},\;r\neq s,\;\tau^2\sim\ND\left(\sum_{r\in\partial_s}\frac{1}{N_s}\beta_{r},\frac{\tau^2}{N_s}\right),
% \end{equation*}
% where $\partial_s$ denotes the set of neighbours of region $s$ and $N_s$ is the number of regions in $\partial_s$. The penalty matrix is then given by
% \begin{eqnarray*}
%  \mK[s,r] &=& \begin{cases}
%  -1 & s\neq r,\quad r\in\partial_s\\
%  0 & s\neq r,\quad r\notin\partial_s\\
%  N_s& s=r.
%  \end{cases}
% \end{eqnarray*}
% \end{itemize}


\subsection{Posterior Estimation}

Posterior estimation is implemented in \texttt{bamlss} based on different estimation procedures. The main focus is on Bayesian estimation which we describe below, while outlining further possiblities at the end of this section.

\subsubsection{The posterior distribution}

All of these rely on the posterior distribution which is proptional to the likelihood time the priors of all involved model parameters. Let therefore be $\betavec=(\betavec_1',\ldots,\betavec_K')'$, where $\betavec_k=(\betavec_{1k}',\ldots,\betavec_{J_k k}')'$ all unknown regression coefficients and $\tauvec=(\tauvec_1',\ldots,\tauvec_K')'$, $\tauvec_k=(\tauvec_{1k},\ldots,\tauvec_{J_k k})'$ the hyperparameters from the previous section. Then, the posterior is up to constants gives by
\begin{equation}\label{eq:post}
\pi(\betavec,\tauvec|\yvec)\propto \prod_{i=1}^n p_{Y_i}\prod_{k=1}^K\prod_{j=1}^{J_k}p(\betavec_{jk},\tauvec_{jk}).
\end{equation}


\subsubsection{Maximising the posterior mode}

The first option to obtain point estimates for $\betavec,\tauvec$ is to compute the mode of Eq.~\eqref{eq:post} which is a solution of 
\[
\mbox{argmax}_{\betavec,\tauvec} \pi(\betavec,\tauvec|\yvec).
\]
In case of flat priors for all involved parameters the latter optimization problem is equivalent to maximising the log-likelihood function. Typcially, Newton Raphson type algorithms or backfitting algorithms are used to solve the problem iteratvely and \texttt{bamlss} offers several optins. However, direct maximisation with respect to $\tauvec$ is usually hard such that these hyperparameters are determined base on information criteria such as BIC or AIC. 

\subsubsection{Posterior sampling}

The mean of the posterior distribution is
\[
E(\betavec,\tauvec|\yvec) = \int (\betavec,\tauvec)' \pi(\betavec,\tauvec|\yvec) d(\betavec,\tauvec) 
\]
which can be
rarely solved analytically. \texttt{bamlss} used MCMC simulation
as it provides an extensible
framework that can adapt to almost any type of problem in a modular system. In cases where possible. \texttt{bamlss} uses Gibbss-sampling, but in most cases the conditional posteriors are not of closed form and the package then implements 
\emph{Derivative-based Metropolis-Hastings}
 and 
%   Probably the most important algorithm, because of its generality and ease of implementation, is
%   random-walk Metropolis. The sampler proceeds by drawing a candidate
%   $\boldsymbol{\beta}_{jk}^{\star}$ from a symmetric jumping
%   distribution $q(\boldsymbol{\beta}_{jk}^{\star}| \, \boldsymbol{\beta}_{jk}^{(t)})$ which is 
%   commonly a normal distribution
%   $\mathcal{N}(\boldsymbol{\beta}_{jk}^{(t)}, \boldsymbol{\Sigma}_{jk})$ centered at the current 
%   iterate. However, in complex settings this sampling scheme is usually not efficient and tuning
%   the covariance matrix $\boldsymbol{\Sigma}_{jk}$ in an adaptive phase is difficult and does not
%   necessarily result in i.i.d.\ behavior of the Markov chain. Therefore, a commonly-used
%   alternative for the covariance matrix of the jumping distribution is to use the local curvature
%   information
%   $\boldsymbol{\Sigma}_{jk} = -\left( \partial^2 \pi(\boldsymbol{\vartheta}; \mathbf{y}, \mathbf{X}) /
%     \partial \boldsymbol{\beta}_{jk}\boldsymbol{\beta}_{jk}^\top \right)^{-1}$,
%   or its expectation, computed at the posterior mode estimate $\hat{\boldsymbol{\beta}}_{jk}$,
%   requiring building blocks \ref{item:7a} and \ref{item:8}.
%   However, fixing $\boldsymbol{\Sigma}_{jk}$ during MCMC simulation might still lead to undesired
%   behavior of the Markov chain especially when parameter samples move into regions with low
%   probability mass of the posterior distribution. A solution with good mixing properties is to
%   construct approximate full conditionals $\pi(\boldsymbol{\beta}_{jk} | \cdot)$ that are based on a
%   second order Taylor series expansion of the log-posterior centered at the last state 
%   \citep{bamlss:Gamerman:1997, bamlss:Klein+Kneib+Lang:2015}.
%   The resulting proposal density $q(\boldsymbol{\beta}_{jk}^\star | \boldsymbol{\beta}_{jk}^{(t)})$
%   is again normal (see supplements Section~4) with precision matrix and mean given by
%   $$
%   \left(\boldsymbol{\Sigma}_{jk}^{(t)}\right)^{-1} = -\mathbf{H}_{kk}\left(\boldsymbol{\beta}_{jk}^{(t)}\right) \qquad
%   \boldsymbol{\mu}_{jk}^{(t)} =  \boldsymbol{\beta}_{jk}^{(t)} -
%   \left[\mathbf{J}_{kk}\left( \boldsymbol{\beta}_{jk}^{(t)} \right) +
%     \mathbf{G}_{jk}(\boldsymbol{\tau}_{jk})\right]^{-1}\mathbf{s}\left( \boldsymbol{\beta}_{jk}^{(t)} \right),
%   $$
%   which is equivalent to the updating function given in (\ref{eqn:blockblocknewton}) and can again
%   be build using blocks \ref{item:7a} and \ref{item:8}.
%   Hence, the mean is simply one Newton or Fisher scoring iteration towards the posterior mode at
%   the current step.
% 
%   Again, assuming a basis function approach for $f_{jk}( \cdot )$ the precision and mean are
%   $$
%   \left(\boldsymbol{\Sigma}_{jk}^{(t)}\right)^{-1} =
%     \mathbf{X}_{jk}^\top\mathbf{W}_{kk}\mathbf{X}_{jk} + \mathbf{G}_{jk}(\boldsymbol{\tau}_{jk})
%   \qquad
%   \boldsymbol{\mu}_{jk}^{(t)} =
%     \boldsymbol{\Sigma}_{jk}^{(t)}\mathbf{X}_{jk}^\top\mathbf{W}_{kk}\left(\mathbf{z}_k  -
%       \boldsymbol{\eta}^{(t)}_{k,-j}\right)
%   $$
%   with weights $\mathbf{W}_{kk} = -\mathrm{diag}(\partial^2 \ell(\boldsymbol{\beta}; \mathbf{y}, \mathbf{X}) /
% \partial \boldsymbol{\eta}_k \partial \boldsymbol{\eta}_k^\top)$, or the corresponding expectation,
%   as in posterior mode updating using building blocks \ref{item:7b} and \ref{item:8}, with
%   working observations
%   $\mathbf{z}_k = \boldsymbol{\eta}_k^{(t)} + \mathbf{W}_{kk}^{-1}\mathbf{u}_k^{(t)}$
%   (see also the supplemental material Section~4 for a detailed derivation). Note again,
%   the computation of the mean is equivalent to a full Newton step as given in updating function
%   (\ref{eqn:blockblocknewton}), or Fisher scoring when using
%   $-E(\partial^2 \ell(\boldsymbol{\beta}; \mathbf{y}, \mathbf{X}) /
%     \partial \boldsymbol{\eta}_k \partial \boldsymbol{\eta}_k^\top)$,
%   in each iteration of the MCMC sampler using iteratively weighted least squares (IWLS). If
%   the computation of the weights is expensive, one simple strategy is to update
%   $\mathbf{W}_{kk}$ only after samples of all parameters of $\boldsymbol{\theta}_k$ are drawn.
% 
and \emph{slice sampling}, see~\cite{bamlss:Umlauf+Klein+Zeileis:2018} for details.
%   Slice sampling \citep{bamlss:Neal:2003} is a gradient free MCMC sampling scheme that produces
%   samples with $100\%$ acceptance rate. Therefore, and because of the simplicity of the algorithm,
%   slice sampling is especially useful for automated general purpose MCMC implementations that allow
%   for sampling from many distributions.  Updates for the $i$-th parameter in $\boldsymbol{\beta}_{jk}$ are
%   generated by:
%   \begin{enumerate}
%   \item Sample $h \sim \mathcal{U}(0, \pi(\beta_{ijk}^{(t)} | \, \cdot))$.
%   \item Sample $\beta_{ijk}^{(t+1)} \sim \mathcal{U}(S)$ from the horizontal slice
%     $S = \{\beta_{ijk}: h < \pi(\beta_{ijk} | \, \cdot)\}$.
%   \end{enumerate}
%\end{itemize}

In addition, non-Bayesian procedures for estimating distributional models are available in the package, in particular backfitting~\cite{RigSta2005} for GAMLSS and component-wise gradient boosting~\cite{MayFenHofKneSch2012} for the same model class.

\subsection{Model Choice and Evaluation}

\subsubsection{Measures of performance}

Model choice and variable selection is important in distributional regression due to the large number of candidate models. \texttt{bamlss} offers a number of possible options to do so.

\begin{itemize}
\item {Information criteria} can be used to compare different model specifications.  
For posterior mode estimation, the Akaike information criterion (AIC), or the corrected
AIC, as well as the Bayesian information criterion (BIC), are implemented in \texttt{bamlss}.  Estimation of model complexity
is based on the so-called equivalent degrees of freedom (EDF). 

For MCMC based estimation, 
\texttt{bamlss} mainly relies on the deviance information criterion (DIC, \cite{SpiBesCarLin2002}) and widely applicable information criterion (WAIC, \cite{watanabe2010asymptotic}). 
\item Quantile residuals:
Quantile residuals \cite{bamlss:Dunn:Smyth:1996} can be used to evaluate the model fit.
%defined as
%$\hat{r}_i = \Phi^{-1}(\mathbf{\mathcal{F}}(y_i | \, \hat{\boldsymbol{\theta}}_{i}))$ with the inverse 
%cumulative distribution function (CDF) of a standard normal distribution $\Phi^{-1}$ and
% $\mathbf{\mathcal{F}}( \cdot )$ the CDF of the modeled distribution $\mathcal{D}( \cdot )$ with estimated
%parameters $\hat{\boldsymbol{\theta}}_{i} = (\hat{\theta}_{i1}, \ldots, \hat{\theta}_{iK})^\top$
%plugged in, should at least approximately be standard normally distributed if the correct model has
%been specified. Resulting residuals can
%be assessed by quantile-quantile-plots or
%probability integral transforms which consider
%$u_i = \mathbf{\mathcal{F}}(y_i | \, \hat{\boldsymbol{\theta}}_i)$. If the estimated model is a good approximation to the true data generating 
%process, the $u_i$ will then approximately follow a uniform distribution on $[0, 1]$.
%Graphically, histograms of the $u_i$ can be used for this purpose.
\item Scoring rules:
Sometimes it is helpful to evaluate the performance on a test data set (or for instance based on cross validation). For this, \texttt{bamlss} implements proper scoring rules \cite{bamlss:Gneiting+Balabdaoui+Raftery:2007}. %Implementation of the log-score (i.e.~the can for instance be done via \textbf{CODE}
\end{itemize}

\subsubsection{Evaluation and Interpretation}
 \begin{itemize}
\item Plotting: The default plotting function of a \texttt{bamlss} object \texttt{plot(b)} will return all effects centered around their mean. However, it can sometimes be useful in distributional regression to look at transformation of the original model parameters such as expected value or variance of the repsonse variable $\yvec$, we give examples for both in our application.
\item Predictions:
For obtaining such transformations the \texttt{predict} function is used. 
 \end{itemize}


\section{The bamlss package} \label{sec:package}

\section{Application} \label{sec:application}

This section illustrates the workflow with \pkg{bamlss} along an example application.
We want to build a statistical model linking positive counts of cloud-to-ground
lightning discharges to atmospheric quantities from a reanalysis dataset.

The region we focus on are the European Eastern Alps. Cloud-to-ground lightning
discharges---detected by the Austrian Lightning Detection and Information System
\citep[ALDIS, ][]{schulz2005}---are counted on grids with a mesh size of $32~km$.
The lightning observations are available for the period 2010--2018.
The reanalysis data comes from the fifth generation of the ECMWF (European
Centre for Medium-Range Weather Forecasts) atmosphheric reanalyses of the
global climate \citep{era5}. ERA5 provides a globally complete and consistent
pseudo-observations of the atmosphere using the laws of physics. The horizontal
resolution is approx.\ $32~km$. The temporal resolution is hourly. The temporal
coverage is from 1979 to present.
In this example application we work only with a small subset of the data, which
can be assessed from the accompanying \proglang{R} package \pkg{FlashAustria}
\citep{FlashAustria}.

<<application-load-model, echo=FALSE>>=
## --- data ---
# data("flash_austria", package = "FlashAustria")
data("fitted_model", package = "FlashAustria")
b <- flash_austria_model

## --- specify formula ---
f <- list(
  mu = counts ~ s(d2m, bs = "ps") + s(q_prof_PC1, bs = "ps") +
    s(cswc_prof_PC4, bs = "ps") + s(t_prof_PC1, bs = "ps") +
    s(v_prof_PC2, bs = "ps") + s(sqrt_cape, bs = "ps"),
  theta = ~s(sqrt_lsp, bs = "ps")
)
@

<<application-data>>=
## --- data ---
data("flash_austria", package = "FlashAustria")
head(flash_austria_train)
nrow(flash_austria_train)
@

The motivation for this application is as follows: Lightning counts are not
modelled within the atmospheric reanalyses. Lightning observations are only
available for the period 2010--2018. With a statistical model on hand one could
predict lightning counts for the time before 2010 and thus analyze lightning
events in the past for which no observations are available.

The response of our statistical model are positive counts, with a mean of
\Sexpr{round(mean(flash_austria_train$counts), 2)}, and a variance of
\Sexpr{round(var(flash_austria_train$counts), 2)}. Thus, we are facing a
truncated count data distribution which is highly overdispersive.  In order to
capture the tuncation of the data and its overdispersion we employ a
zero-truncated negative binomial distribution \citep{cameron2013count}, which
is specified by two parameters $\mu > 0$ and $\theta > 0$. $\mu$ is the
expectation of the underlying untruncated negative binomial, and $\theta$
modifies the variance of the untruncated negative binomial by
$\mathrm{VAR}(\tilde{Y}) = \mu + \mu^2/\theta$, where $\tilde{Y}$ is a latent
random variable following the underlying untruncated negative binomial
distribution.

The zero-truncated negative binomial distribution is implemented as
\code{ztnbinom_bamlss()} within \pkg{bamlss}.  In order to specify smooth terms
form both distributional parameter, the formula has to be a \code{list}. The
first element is named \code{mu} and specifies terms for the paramter $\mu$,
but also specifies that \code{counts} is the response of the regression model.
The second element is named \code{theta} and specifies a smooth term for the
paramter $\theta$. Hence well known for their sampling properties, we are
applying P-splines \citep{bamlss:Eilers+Marx:1996} for all terms. Specifying
smooth terms within \pkg{bamlss} formulae builds on the \pkg{mgcv} infrastructure
\citep{bamlss:Wood:2019} provided by \code{s()}, which leads to the following
specification of the model:

<<application-model1, eval=FALSE>>=
## --- specify formula ---
f <- list(
  mu = counts ~ s(d2m, bs = "ps") + s(q_prof_PC1, bs = "ps") +
    s(cswc_prof_PC4, bs = "ps") + s(t_prof_PC1, bs = "ps") +
    s(v_prof_PC2, bs = "ps") + s(sqrt_cape, bs = "ps"), 
  theta = ~ s(sqrt_lsp, bs = "ps")
)
@

Now we have all ingedients on hand to feed the standard interface for
statistical models in \proglang{R}: A formula \code{f}, a family
\code{ztnbinom_bamlss()}, and a data set \code{flash_austria_train}. Within the
\code{bamlss()} call we also provide arguments which are passed forward to the
optimizer and the sampler. We choose the gradient boosting optimizer
\code{boost()} in order to find initial values for the default sampler
\code{GMCMC()}. Gradient boosting proved to offer a very stable method for
finding regression coefficients that serve as initial values for a MCMC sampler
\citep{bamlss:Simon+Mayr+Umlauf+Zeileis:2019}.  We set the number of iteration
to $1000$. For the sampling we allow another $1000$ iterations as burn-in
phase, and apply a thinning of the resulting chain of $5$.  Running
\code{n.iter = 6000} iterations in total leads to $1000$ MCMC samples in the
end:

%%% <<application-model2, eval=FALSE>>=
%%% ## --- fit model ---
%%% b <- bamlss(f, family = "ztnbinom", data = flash_austria_train,
%%%   optimizer = boost, maxit = 1000,                  # boosting arguments
%%%   thin = 5, burnin = 1000, n.iter = 6000            # sampler arguments
%%% )
%%% @
%%% 
\begin{Schunk}
\begin{Sinput}
R> ## --- fit model ---
R> b <- bamlss(f, family = "ztnbinom", data = flash_austria_train,
+    optimizer = boost, maxit = 1000,                  # boosting arguments
+    thin = 5, burnin = 1000, n.iter = 6000            # sampler arguments
+  )
\end{Sinput}
\begin{Soutput}
logLik -36924.3 eps 0.0007 iteration 1000 qsel 7
elapsed time:  5.43min
Starting the sampler...
|********************| 100%  0.00sec 27.73min
\end{Soutput}
\end{Schunk}

The model was fitted on a single core Intel i7-7700 CPU with 3.60GHz and 16~GB memory,
on which the boosting took less than 6~minutes and the MCMC sampling took less the
30~minutes. As a first diagnostic we check the loglikelihood contributions of the
individual terms during the boosting optimization (Fig.~\ref{fig:appboost}).

<<app-plot-boost, eval = FALSE>>=
boost.plot(b, "loglik.contrib", intercept = FALSE)
@

\setkeys{Gin}{width=.8\textwidth}
\begin{figure}
\begin{center}
<<app-plot-boost, fig = TRUE, height = 4, width = 6, echo = FALSE>>=
boost.plot(b, c("loglik.contrib"), intercept = FALSE)
# layout(matrix(c(rep(1,2), rep(2,3)), nrow = 1))
# boost.plot(b, "loglik")
# boost.plot(b, "loglik.contrib")
@
\end{center}
\caption{Contribution to the log-likelihood of individual terms during
gradient boosting.}
\label{fig:appboost}
\end{figure}

After 1000~iterations the term \code{s(sqrt_cape).mu} has the highest contribution
to the loglikelihood with %
\Sexpr{round(flash_austria_model$model.stats$optimizer$boost.summary$summary$mu["s(sqrt_cape)",2])} %
followed by \code{s(q_prof_PC1).mu} with %
\Sexpr{round(flash_austria_model$model.stats$optimizer$boost.summary$summary$mu["s(q_prof_PC1)",2])}.
The term of the parameter $\theta$ \code{s(sqrt_lsp).theta} has a relatively small
contribution with %
\Sexpr{round(flash_austria_model$model.stats$optimizer$boost.summary$summary$theta["s(sqrt_lsp)",2])}.
The overall message of this diagnostic is that the contributions to the
loglikelihood at the end of the boosting procedure are very small and that
the algorithm approached a stable state, which suggest that we retrieve
reasonable initial values for the MCMC sampling.

The MCMC chains are investigated by looking directly at the traces of the chains
and with the auto-correlation function of the chains.

<<app-plot-trace, fig = FALSE, eval = FALSE>>=
plot(b, model = "mu", term = "s(sqrt_cape)", which = "samples")
@

Figure \ref{fig:apptrace} shows the traces and the auto-correlation functions
for two regression coefficients of the term \code{s(sqrt_cape)}. The traces
reveal samples around stables means. This suggests that the 1000 boosting
iterations and the 1000 burn-in samples were sufficient in order to approach
reasonable starting values for the sampling. The auto-correlation functions
reveal that after the thinning hardly any correlation remains between
consecutive samples.

\begin{figure}
\begin{center}
<<echo = FALSE, fig = TRUE>>=
b2 <- b
b2$samples[[1]] <- b2$samples[[1]][, grep("s(sqrt_cape)", colnames(b2$samples[[1]]), fixed = TRUE)][, 1:2]

par(mar = c(4.1, 4.1, 4.1, 1.1))
plot(b2, which = "samples", ask = FALSE)
@
\end{center}
\caption{MCMC trace (left panels) and auto-correlation (right panels) for two
splines from the term \code{s(sqrt\_cape)} of the model \code{mu}.}
\label{fig:apptrace}
\end{figure}

As these diagnostics suggest that a reasonable initial state for the sampling
has been found and the samples are independent draws from the posterior, one
can go further and investigate the estimated effects. The boosting summary
(Fig.~\ref{fig:appboost}) revealed that the terms \code{s(sqrt_cape)} and
\code{s(q_prof_PC1)} had a large contribution for improving the fit. 
Looking at these effects illustrate how the atmospheric parameters
of the reanalyses are related to lightning events (Fig.~\ref{fig:appeffect}),
and thus help to understand the physics associted with lightning events.
The effects are presented on the scale of the linear predictor, i.e., the
log scale.

\begin{figure}
\begin{center}
<<app-plot-effect1, fig = FALSE, eval = FALSE>>=
plot(b, term = c("s(sqrt_cape)", "s(q_prof_PC1)", "s(sqrt_lsp)"),
  rug = TRUE)
@
\setkeys{Gin}{width=1\textwidth}
<<echo = FALSE, fig = TRUE, height = 3, width = 6>>=
par(mfrow = c(1, 3))
plot(b, term = c("s(sqrt_cape)", "s(q_prof_PC1)", "s(sqrt_lsp)"),
  ask = FALSE, spar = FALSE, scheme = 2, grid = 200,
  rug = TRUE, col.rug = "#39393919")
@
\end{center}
\caption{Effect of the terms \code{s(sqrt\_cape)} and \code{s(q\_prof\_PC1)}
from the model \code{mu} and term \code{s(sqrt\_lsp)} from model \code{theta}.
Credible intervals derived from MCMC samples.}
\label{fig:appeffect}
\end{figure}

\code{s(sqrt_cape)} reveals a monotonic increasing shape. In the
range from $0$--$30$ the effect increases linearly with small credible intervals.
For higher values the effect flattens and shows large credibe intervals which are
associated with the samll amount of data in that range. Physically the shape of the
effect is meaningful as more convective available potential energy has the
potential to lead to heavier lightning events.
\code{s(q_prof_PC1)} reveals areas of large credible intervals at the left and
right bounds of the range due to small amount of data. In the mid-range a
increasing effect is identified. As \code{q_prof_PC1} is the leading principal
component of the vertical profile of specific humidity, one has to consider the
corresponding spatial mode (not shown) for interpretation: Higher values of
\code{q_prof_PC1} are linked to more moisture in the lower atmosphere,
which is also available as a source of \emph{latent energy}, i.e., energy that
becomes free when water transfers from the gas to the liquid phase.

Finally it is interesting to look at the effect acting on the link scale of
the parameter $\theta$, \code{s(sqrt_lsp)} (right panel in Fig.~\ref{fig:appeffect}).
\code{sqrt_lsp} is the square root of large scale precipitation, i.e.,
precipitation that is not linked to convective processes and thus it is
not related to strong lightning events. The effect shows following relationship:
Higher values of \code{sqrt_lsp} lead to smaller $\theta$, which increases the
variance of the distribution.

Before applying the model, i.e., predicting lightning cases before 2010, we
check the marginal calibration of the distribution by hanging rootogram, a tool
popular for the evaluation of count data regression models
\citep{kleiber2016visualizing}. First we predict the distributional parameter
on out-of-sample data \code{flash_austria_eval} for which lightning
observations are on hand.

<<app-rootogram1>>=
## --- out-of-sample prediciton ---
fit <- predict(b, newdata = flash_austria_eval, type = "parameter")
str(fit)
@

\code{predict()} returns a \code{list}, of which each element is named
as a distributional parameter and contains by default a vector of
predictions. Each prediction is the average of the preditions obtained
by all MCMC samples. The resulting \code{list} can be used to
derive further quantities by employing the functions of the \pkg{bamlss}
family that can be extracted using \code{family()},

<<app-rootogram2>>=
## --- extract family ---
fam <- family(b)
fam
@



<<app-rootogram3>>=
## --- expected frequencies ---
expect <- sapply(1:50, function(j) sum(fam$d(j, fit)))
names(expect) <- 1:50
expect <- as.table(expect)

## --- observed frequencies ---
obsrvd <- table(flash_austria_eval$counts)[1:50]
@

\setkeys{Gin}{width=.7\textwidth}
\begin{figure}
\begin{center}
<<app-plot-rootogram, fig = TRUE, height = 5, width = 6>>=
## --- plot rootogram ---
library("countreg")
rootogram(obsrvd, expect, xlab = "# Counts", main = "Rootogram")
@
\end{center}
\caption{Hanging rootogram for evaluating calibration of count data model
on out-of-sample data. Red line indicates the expected frequencies on the
square root scale. Gray bars indicate observed frequencies on square root
scale hanging from the red line.}
\label{fig:approoto}
\end{figure}

\setkeys{Gin}{width=1\textwidth}
\begin{figure}
\begin{center}
<<app-plot-case, eval = FALSE>>=
library("sf")
library("ggplot2")
library("colorspace")
library("rnaturalearth")
library("rnaturalearthdata")
  
world <- ne_countries(scale = "medium", returnclass = "sf")

fit <- predict(b, newdata = flash_austria_case, type = "parameter")
d_case$P10 <- 1 - fam$p(9, fit)
  
ggplot() + geom_sf(aes(fill = P10), data = flash_austria_case) +
  scale_fill_continuous_sequential("Inferno", rev = TRUE) +
  geom_sf(data = world, col = "white", fill = NA) +
  coord_sf(xlim = c(8, 17), ylim = c(45.5, 50), expand = FALSE) +
  facet_wrap(~time) + theme_minimal()
@
<<app-plot-case-eval, fig = TRUE, echo = FALSE>>=
library("sf")
library("ggplot2")
library("colorspace")
library("rnaturalearth")
library("rnaturalearthdata")

fit <- predict(b, newdata = flash_austria_case, type = "parameter")
flash_austria_case$P10 <- 1 - fam$p(9, fit)

world <- ne_countries(scale = "medium", returnclass = "sf")

ggplot() + geom_sf(aes(fill = P10), data = flash_austria_case) +
  scale_fill_continuous_sequential("Inferno", rev = TRUE) +
  geom_sf(data = world, col = "white", fill = NA) +
  coord_sf(xlim = c(7.95, 17), ylim = c(45.45, 50), expand = FALSE) +
  facet_wrap(~time, nrow = 2) + theme_minimal() # +
#  theme(legend.position = "bottom")
@
\end{center}
\caption{A probabilistic reconstruction of lightning counts occured on
September 15 2001 at 6~UTC, 17~UTC and 23~UTC and on September 16 2001 at
13~UTC, i.e., the probability of having observed $10$ or more counts within one
grid box.}
\label{fig:appcase}
\end{figure}


\bibliography{bamlss}

\end{document}

