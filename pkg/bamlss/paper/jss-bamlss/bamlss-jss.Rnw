%\documentclass[article]{jss}
\documentclass[nojss]{jss}
\usepackage{amsmath,amssymb,amsfonts,thumbpdf}
\usepackage{multirow,longtable}

%% additional commands
\newcommand{\squote}[1]{`{#1}'}
\newcommand{\dquote}[1]{``{#1}''}
\newcommand{\fct}[1]{{\texttt{#1()}\index{#1@\texttt{#1()}}}}
\newcommand{\class}[1]{\dquote{\texttt{#1}}}
%% for internal use
\newcommand{\fixme}[1]{\emph{\marginpar{FIXME} (#1)}}
\newcommand{\readme}[1]{\emph{\marginpar{README} (#1)}}

%% Authors: NU + rest in alphabetical order
\author{Nikolaus Umlauf\\Universit\"at Innsbruck \And
        Achim Zeileis\\Universit\"at Innsbruck \AND
        Nadja Klein\\Humboldt-University of Berlin \And
        Thorsten Simon\\Universit\"at Innsbruck}
\Plainauthor{Nikolaus Umlauf, Achim Zeileis, Nadja Klein, Thorsten Simon}

\title{\pkg{bamlss}: A Lego Toolbox for {B}ayesian Additive Models for
  Location Scale and Shape (and Beyond)}
\Plaintitle{bamlss: A Lego Toolbox for Bayesian Additive
  Models for Location Scale and Shape (and Beyond)}
\Shorttitle{A Lego Toolbox for BAMLSS (and Beyond)}
	
\Keywords{GAMLSS, distributional regression, backfitting, gradient boosting, LASSO,
  MCMC, neural networks, \proglang{R}}
\Plainkeywords{GAMLSS, distributional regression, backfitting, gradient boosting, LASSO,
  MCMC, neural networks, R}

\Abstract{
During the last decades there has been an increasing interest in distributional regression 
models that allow to model all distributional parameters, such as location, scale and shape 
and thereby the entire data distribution conditional on covariates. In particular, the 
framework of structured additive distributional regression models enables to specify 
different types of effects such as linear, non-linear or interaction effects on all the 
distribution parameters hence providing a very flexible and generic framework suited for 
many complex real data problems. However, the implementation of new models is usually time-consuming
and complex, especially using Bayesian estimation algorithms. We propose an unified modeling
architecture, which is implemented in the \proglang{R} package \pkg{bamlss}, that makes it possible
to embed many different approaches suggested in literature and software. We show that
implementing (new) algorithms, non-standard distributions or effect types, or the integration of
already existing software, is relatively straightforward in this setting. We illustrate the
usefulness of the approach by implementing highly efficient algorithms for fully Bayesian inference
based on MCMC simulation, backfitting algorithms, gradient boosting, LASSO-type penalized models as
well as neural network distributional regression models.
}

\Address{
  Nikolaus Umlauf, Achim Zeileis, Thorsten Simon\\
  Department of Statistics\\
  Faculty of Economics and Statistics\\
  Universit\"at Innsbruck\\
  Universit\"atsstr.~15\\
  6020 Innsbruck, Austria\\
  E-mail: \email{Nikolaus.Umlauf@uibk.ac.at},\\
  \phantom{E-mail: }\email{Achim.Zeileis@R-project.org}\\
  \phantom{E-mail: }\email{Thorsten.Simon@uibk.ac.at}\\
  URL: \url{http://eeecon.uibk.ac.at/~umlauf/},\\
  \phantom{URL: }\url{http://eeecon.uibk.ac.at/~zeileis/}\\

  Nadja Klein\\
  Humboldt-University of Berlin\\
  School of Business and Economics\\
  Applied Statistics\\
  Unter den Linden 6\\
  10099 Berlin, Germany\\
  E-mail: \email{nadja.klein@hu-berlin.de}\\
  URL: \url{https://hu.berlin/NK}
}

\SweaveOpts{engine = R, eps = FALSE, keep.source = TRUE}

<<preliminaries, echo=FALSE, results=hide>>=
options(width = 70, prompt = "R> ", continue = "+  ",
  SweaveHooks = list(fig = function() par(mar = c(4.1, 4.1, 1, 1))))
library("bamlss")
@

\input{defs}

\begin{document}
\SweaveOpts{concordance=TRUE}

\section{Introduction}

Generalized additive models for location, scale and shape
(GAMLSS,~\citealp{bamlss:Rigby+Stasinopoulos:2005}), also known as distributional regression models
\citep{bamlss:Klein+Kneib+Lang+Sohn:2015, bamlss:Klein+Kneib+Klasen+Lang:2015}, provide a very
flexible framework that enables modeling all parameters of a response distribution in terms of
covariates. While most common model specifications (linear models, generalized linear models [GLMs],
generalized additive models [GAMs], etc.) ignore the fact that the distribution of any quantity is
not well characterized by the mean alone, GAMLSS has the clear advantage to minimize the risk of
model misspecification and therefore provides much more reliable probabilistic forecasts.

The \proglang{R} package \pkg{gamlss}
\citep{bamlss:Stasinopoulos:Rigby:2007, bamlss:Stasinopoulos+Rigby:2019} and \pkg{VGAM}
\citep{bamlss:Yee:2009, bamlss:Yee:2019} were the first packages
to follow the distributional regression approach and comprise a quite flexible framework for
implementing response distributions. Variable selection algorithms for GAMLSS based on gradient
boosting were then introduced by the \pkg{gamboostLSS} package
\citep{bamlss:Hofner+Mayr+Schmid:2014, bamlss:Hofner:2018} and more recently, the contributed
base \proglang{R} package \pkg{mgcv} \citep{bamlss:Wood:2017, bamlss:Wood:2018} provides a
fitting routine for the estimation of general smooth models \citep{bamlss:Wood+Pya+Saefken:2016}.
The inferential framework of the packages mentioned is based on maximum likelihood, however, if
taken into account at all. For complex models, e.g., with response
distributions outside the exponential family or when multiple predictors contain several smooth
effects \citep{bamlss:Klein+Kneib+Lang:2015}, the maximum likelihood estimators based on
asymptotic properties might fail. In such situations the fully Bayesian approach using Markov chain
Monte Carlo (MCMC) simulation techniques is particularly attractive since valid credible intervals
are easily obtained from the posterior samples. While Bayesian distributional regression models
can in principle be estimated using general purpose MCMC software like
\pkg{JAGS}~\citep{bamlss:Plummer:2013}, \pkg{Stan}~\citep{bamlss:stan-software:2016} or
\pkg{WinBUGS}~\citep{bamlss:Lunn+Thomas+Best+Spiegelhalter:2000}, they all have the drawback
that sampling time is usually quite long, or it is even impossible to estimate distributional
regression models with GAM-type predictors, e.g., when using large data sets, modeling spatial
effects or complex higher-order interactions. The first software capable of estimating such
GAM-type distributional regression models using MCMC is the standalone package
\pkg{BayesX}~\citep{bamlss:Belitz+Brezger+Kneib+Lang+Umlauf:2017}, which provides highly
efficient sampling schemes for very large data sets as well as (spatial) multilevel models. However,
the implementation of new response distributions (models) in \pkg{BayesX} is not straightforward and users
usually have to ask the maintainers to do so.

In this paper, we present the \proglang{R} package \pkg{bamlss}, a flexible ``Lego toolbox''
for Bayesian distributional regression models (and beyond). The main contributions of \pkg{bamlss}
are the following:
%
\begin{itemize}
\item Easy implementation of (new) models and algorithms,
\item comparison of existing optimization algorithms and samplers,
\item or integration of existing implementations.
\end{itemize}
%
The package builds on the well-established \pkg{mgcv} infrastructures using
\proglang{R}'s formula syntax for model specification. Moreover, the package provides
commonly used extractor functions like \fct{summary}, \fct{plot}, \fct{predict}, etc., such
that users interested in developing new models or algorithms (Bayesian or frequentist) can
really concentrate on it.
The package is made available from the Comprehensive
\proglang{R} Archive Network (CRAN) at \url{http://CRAN.R-project.org/package=bamlss}.

The remainder of this paper is as follows. In Section~\ref{sec:motivation}
motivating examples illustrate the first steps using \pkg{bamlss}. Section~\ref{sec:distreg}
introduces distributional regression models in more detail. A thorough introduction of the
\proglang{R} package \pkg{bamlss}, describing the most important infrastructures and building blocks,
is then given in Section~\ref{sec:package}. In Section~\ref{sec:application} we highlight the
unified modeling approach using a complex distributional regression model for predicting
flash counts.

\newpage

\section{Motivating examples} \label{sec:motivation}

In this section we introduce the \proglang{R} package to the reader using two motivating
examples. Readers more interested in advanced examples using \pkg{bamlss} could skip this
section and move to Section~\ref{sec:application} directly. The first example
demonstrates that the usual look and feel when using well-established model fitting 
functions like \fct{glm} is an elementary part of \pkg{bamlss}, i.e., first steps and
basic handling should be relatively simple. The second example then explains how full
distributional regression models can be estimated.

\subsection{Logit model}

This example is taken from the \pkg{AER} package \citep{bamlss:Kleiber+Zeileis:2008} and is about
labor force participation (yes/no) of women in Switzerland 1981. The data can be loaded with
<<>>=
data("SwissLabor", package = "AER")
@
The data frame contains of 872 observations on 7 variables, where some of them might have
a nonlinear influence on the response labor \code{participation}. Now, a standard Bayesian 
binomial logit model using the default MCMC algorithm can be fitted with
<<echo = FALSE, results = hide>>=
if(!file.exists("SwissLaborModel.rda")) {
  f <- participation ~ income + age + education + youngkids + oldkids + foreign + I(age^2)
  set.seed(123)
  b <- bamlss(f, family = "binomial", data = SwissLabor,
    n.iter = 12000, burnin = 2000, thin = 10)
  save(b, file = "SwissLaborModel.rda")
} else {
  load("SwissLaborModel.rda")
}
@
<<eval = FALSE>>=
library("bamlss")

## Model formula.
f <- participation ~ income + age + education +
  youngkids + oldkids + foreign + I(age^2)

## First, set the seed for reproducibly.
set.seed(123)

## Estimate model.
b <- bamlss(f, family = "binomial", data = SwissLabor,
  n.iter = 12000, burnin = 2000, thin = 10)
@
using 12000 iterations with a burnin-phase of 2000 and a thinning parameter of 10. Prior
the MCMC sampling a backfitting algorithm for posterior mode estimation is performed. The
posterior mode estimates are then used as starting values for MCMC. Using
the main model fitting function \fct{bamlss} all model fitting engines can be exchanged, which is explained in detail in Section~\ref{sec:package} and the application
Section~\ref{sec:application}.

Note, to capture nonlinearities, a quadratic term for variable \code{age} is added to
the model. The resulting object \code{b} is of class \code{"bamlss"} for which standard
extractor functions like \fct{summary}, \fct{coef}, \fct{plot}, \fct{predict}, etc.\
are available. The model summary output is printed by
<<>>=
summary(b)
@
\begin{figure}[!b]
\centering
\setkeys{Gin}{width=1\textwidth}
<<echo = FALSE, fig = TRUE, height = 2.5, width = 8>>=
par(mfrow = c(1, 3), mar = c(4.1, 4.1, 4.1, 1.1))
bsp <- b
bsp$samples <- bsp$samples[, c("pi.p.(Intercept)"), drop = FALSE]
plot(bsp, which = "samples", spar = FALSE, ask = FALSE)
plot(b, which = "max-acf", spar = FALSE, ask = FALSE)
@
\caption{\label{fig:logit_traceplots} Logit model, MCMC trace (left panel),
  auto-correlation for the intercept (middle panel), maximum auto-correlation for all
  parameters (right panel).}
\end{figure}
which suggests ``significant'' effects for all covariates, except for variable \code{education},
since the 95\% credible interval contains zero. Before proceeding the analysis, users usually do
some convergence checks of the MCMC chains by looking at traceplots and auto-correlation.
<<eval = FALSE>>=
plot(b, which = c("samples", "max-acf"))
@
These are visualized in Figure~\ref{fig:logit_traceplots} and indicate convergence of the MCMC
chains, i.e., there is no visible trend in the MCMC chains and the very low auto-correlation shown
for the intercept and the maximum auto-correlation of all parameters suggests
i.i.d.\ sampling from the posterior distribution. Note that the function call would show all
trace- and auto-correlation plots, however, for convenience we only show plots for the intercept.

Model predictions on the probability scale can be obtained by the predict method, e.g.,
to visualize the effect of covariate \code{age} on the probability we can do the following:
<<>>=
## Create new data for prediction.
nd <- data.frame(income = 11, age = seq(2, 6.2, length = 100),
  education = 12, youngkids = 1, oldkids = 1,
  foreign = factor(1, levels = 1:2, labels = c("no", "yes")))

## Predict for both cases.
nd$p.no <- predict(b, newdata = nd, type = "parameter", FUN = c95)
nd$foreign <- factor(2, levels = 1:2, labels = c("no", "yes"))
nd$p.yes <- predict(b, newdata = nd, type = "parameter", FUN = c95)
@
The predict method is applied on all MCMC samples and argument \code{FUN} specifies a function
that can be applied on the predictor or distribution parameter samples. The default is the \fct{mean} function,
however, in this case we additionally extract the empirical 2.5\% and 97.5\% quantiles using
function \fct{c95} to obtain credible intervals. Then, the estimated effect can be visualized with
<<eval = FALSE>>=
plot2d(p.no ~ age, data = nd, ylab = "participation",
  ylim = range(c(nd$p.no, nd$p.yes)), lty = c(2, 1, 2))
plot2d(p.yes ~ age, data = nd, col.lines = "blue", add = TRUE,
  lty = c(2, 1, 2))
@
\begin{figure}[!h]
\centering
\setkeys{Gin}{width=0.47\textwidth}
<<echo = FALSE, fig = TRUE, width = 4.3, height = 3.8>>=
par(mar = c(4.1, 4.1, 0.1, 0.1))
plot2d(p.no ~ age, data = nd, ylab = "participation",
  ylim = range(c(nd$p.no, nd$p.yes)), lty = c(2, 1, 2))
plot2d(p.yes ~ age, data = nd, col.lines = "blue", add = TRUE,
  lty = c(2, 1, 2))
legend("topright", c("foreign yes", "foreign no"), lwd = 1,
  col = c("blue", "black"), box.col = NA, bg = NA)
@
\caption{\label{fig:logit_effects} Effect of covariate \code{age} on estimated
  probabilities for both cases, \code{foreign} \code{"yes"} and \code{"no"}. The solid lines
  represent mean estimates, dashed lines represent the 95\% credible interval.}
\end{figure}
The estimates are shown in Figure~\ref{fig:logit_effects} and suggest a clear difference 
for the effect of \code{age} between both cases of factor variable \code{foreign}.

\subsection{Normal location-scale model}

\section{Distributional regression} \label{sec:distreg}

\subsection{Model structure}

\subsubsection{Response distribution}
In structured additive distributional regression we assume that the distribution of a $D$-dimensional random variable $(y_{i1},\ldots,y_{iD})'$, $i=1,\ldots,n$ given some covariates $\xvec_i$  has a parametric density

$$p(y_{i1},\ldots,y_{iD}|\vartheta_{i1},\ldots,\vartheta_{iK})\equiv p_i.$$ 

% \paragraph{Examples:}
% 
% \textbf{maybe add here the distribution from the intro and introduce \texttt{familyname_bamlss()}?}
% 

The basic idea to implicitely model the entire distribution is to link each parameter $\vartheta_{ik}$ $k=1,\ldots,K$ of $p_i$ to a semiparametric structured additive predictor $\eta_{ik}$ formed of the covariates with the help of monotone, twice differentiable response functions $h_k$, such that 
$$\vartheta_{ik}=h_k(\eta_{ik})\mbox{ and }\eta_{ik}=h_k^{-1}(\vartheta_{ik}).$$ 
Note, that  each predictor and each distribution parameter can be summarised in vectors of length $n$, $\etavec_k=(\eta_{1k},\ldots,\eta_{nk})'$, $\varthetavec_k=(\vartheta_{1k},\ldots\vartheta_{nk})'$, and that the subscript $k$ in the predictor is a notation to indicate which parameter the predictor belongs to.

% \paragraph{Example:}
% 
% \textbf{something in R code here?}


The response function is usually chosen to maintain restrictions on the parameter space, like 
\begin{itemize}
\item the identity function $\vartheta_{ik}=\exp(\eta_{ik})$ if $\vartheta_{ik}\in\mathds{R}$,
\item the exponential function $\vartheta_{ik}=\exp(\eta_{ik})$ to ensure a parameter with values on the positive real half axis, 
\item the identity function if the parameter space is unrestricted or $\vartheta_{ik}=\frac{\eta_{ik}}{\sqrt{1+(\eta_{ik})^2}}$ if $\vartheta_{ik}\in[-1,1]$,
\item the logit or probit link for $\vartheta_{ik}\in [0,1]$
\end{itemize}

\subsubsection{Structured predictors}
For any parameter of the distribution $p_i$, the semiparametric predictor has the general form
\begin{equation}\label{eq:predictor}
 \eta_{i}^{\vartheta_k} = \sum_{j=1}^{J_k}f_{j}^{\vartheta_k}(\nuvec_i)
\end{equation}
comprising various functions $f_j^{\vartheta_k}(\nuvec_i)$ defined on the complete covariate information $\nuvec_i$. Specific components may for instance be given by
\begin{itemize}
\item linear functions $f_{j}^{\vartheta_k}(\nuvec_i)=\xvec'_{i}\beta_j^{\vartheta_k}$, including the overall level of the predictor as an intercept $\beta_{0j}^{\vartheta_k}$  and  $\xvec_i$ is a subvector of  $\nuvec_i$. Note that $\xvec_i$ may be chosen specifically for each parameter $\vartheta_k$ but we suppress this potential dependency in our notation.
\item continuous functions $f_{j}^{\vartheta_k}(\nuvec_i)=f_{j}^{\vartheta_k}(x_i)$, where $x_i$ is a single element of $\nuvec_i$ and $f_j$ is an appropriate smooth function to represent the effect of $x_i$ on $\vartheta_{ik}$.
\item spatial variations $f_{j}^{\vartheta_k}(\nuvec_i)=f_{j}^{\vartheta_k}(s_i)$, where $s_i$ represents spatial information, e.g. coordinate information in terms of longitude and latitude or discrete spatial information representing a fixed set of (administrative) geographical units.
\item random effects $f_{j}^{\vartheta_k}(\nuvec_i)=\beta_{j,g_i}^{\vartheta_k}$, where $g_i$ is a cluster variable that groups the observations.
\end{itemize}


We represent smooth terms in the models using basis functions, as a result of which the predictors can always be written in the generic matrix notation
\[
 \etavec^{\vartheta_k} = \sum_{j=1}^{J_k}\mZ_{j}^{\vartheta_k}\betavec_j^{\vartheta_k}
\]
where the design matrices $\mZ_{j}^{\vartheta_k}$ are obtained by evaluating the basis functions at observed covariate values  and  $\betavec_j^{\vartheta_k}$ are the vectors of basis coefficients to be estimated.

Specific properties of the basis coefficients such as smoothness are regularised by assuming possibly improper Gaussian priors
\begin{eqnarray}\label{eq:priorspec}
p\left(\betavec_j^{\vartheta_k}\bigg|(\tau_j^{\vartheta_k})^2\right) \propto \left(\frac{1}{(\tau_j^{\vartheta_k})^2}\right)^{\rank\left(\mK_j^{\vartheta_k}\right)/2}\exp\left(-\frac{1}{2(\tau_j^{\vartheta_k})^2}\left(\betavec_j^{\vartheta_k}\right)'\mK_j^{\vartheta_k}\betavec_j^{\vartheta_k}\right)
\end{eqnarray}
where $\mK_j^{\vartheta_k}$ is a prior precision matrix and $(\tau_j^{\vartheta_k})^2$ are smoothing variances. \textbf{discuss different choices of hyperpriors here?} %The latter are supplemented with inverse gamma hyperpriors, i.e. $(\tau_j^{\vartheta_k})^2~\sim\IGD(a_j^{\vartheta_k},b_j^{\vartheta_k})$ in order to obtain a data-driven amount of smoothness with $a_j^{\vartheta_k}=b_j^{\vartheta_k}=0.001$ as default values for practical analyses.
%Specific examples will be given in Section~\ref{sec:CM}.

As a result, each term $\fvec_j^{\vartheta_k}=(f_j^{\vartheta_k}(\nuvec_1),\ldots,f_j^{\vartheta_k}(\nuvec_n))'=\mZ_j^{\vartheta_k}\betavec_j^{\vartheta_k}$ is determined by a design matrix $\mZ_j^{\vartheta_k}$ and a prior precision or penalty matrix $\mK_j^{\vartheta_k}$. We give two specific examples in the following (suppressing the index $j$ and superscript $\vartheta_k$):

\begin{itemize}
\item {Continuous Covariates:}
To approximate potentially nonlinear effects, we use Bayesian P-splines, compare \citet{eilers} and \citet{BreLan2006} for detailed explanations. The $n\times S$ design matrix $\mZ$ in this setting is composed of $S$ B-spline basis functions evaluated at observed covariates $x_i$. Assuming equidistant knots for the spline expansion, a first or second order random walk is a sensible choice for the prior of $\betavec$, i.e.
\begin{eqnarray*}
\beta_s|\beta_{s-1},\tau^2&\sim&\ND\left(\beta_{s-1},\tau^2\right),\quad s=2,\ldots,S\\
\text{or}\quad\quad\quad\qquad\qquad\quad\qquad\\
\beta_s|\beta_{s-1},\beta_{s-2},\tau^2&\sim&\ND\left(2\beta_{s-1}-\beta_{s-2},\tau^2\right),\quad s=3,\ldots,S
\end{eqnarray*}
with noninformative priors for initial values. This prior structure yields the penalty matrix $\mK = \mD'\mD$ where $\mD$ is a difference matrix of first or second order.

\item {Spatial Effects:}
For discrete spatial effects observed on a lattice or regions, we consider Markov random fields, see \citet{RueHel2005}. Let $s_i\in\lbrace 1,\ldots,S\rbrace$ denote the index or region observation $i$ belongs to. Then $f(s_i) = \beta_{s_i}$ is assumed such that we estimate separate parameters $\beta_1,\ldots,\beta_S$ for each region. As a consequence, the $n\times S$ design matrix is an incidence matrix, i.e. $\mZ[i,s]=1$ if observation $i$ belongs to location $s$ and zero otherwise. The simplest Markov random field prior for the coefficients $\beta_s$ is defined by
\begin{equation*}
\beta_{s}|\beta_{r},\;r\neq s,\;\tau^2\sim\ND\left(\sum_{r\in\partial_s}\frac{1}{N_s}\beta_{r},\frac{\tau^2}{N_s}\right),
\end{equation*}
where $\partial_s$ denotes the set of neighbours of region $s$ and $N_s$ is the number of regions in $\partial_s$. The penalty matrix is then given by
\begin{eqnarray*}
 \mK[s,r] &=& \begin{cases}
 -1 & s\neq r,\quad r\in\partial_s\\
 0 & s\neq r,\quad r\notin\partial_s\\
 N_s& s=r.
 \end{cases}
\end{eqnarray*}
\end{itemize}

For detailed explanations on structured additive regression with further examples we refer the reader to~\citet{fahkne13}.

\subsection{Posterior Estimation}

\subsubsection{The posterior distribution}


\subsubsection{Maximising the posterior mode}

\subsubsection{Posterior sampling}
\begin{itemize}
\item \emph{Derivative-based Metropolis-Hastings}: \label{sec:dmh}
  Probably the most important algorithm, because of its generality and ease of implementation, is
  random-walk Metropolis. The sampler proceeds by drawing a candidate
  $\boldsymbol{\beta}_{jk}^{\star}$ from a symmetric jumping
  distribution $q(\boldsymbol{\beta}_{jk}^{\star}| \, \boldsymbol{\beta}_{jk}^{(t)})$ which is 
  commonly a normal distribution
  $\mathcal{N}(\boldsymbol{\beta}_{jk}^{(t)}, \boldsymbol{\Sigma}_{jk})$ centered at the current 
  iterate. However, in complex settings this sampling scheme is usually not efficient and tuning
  the covariance matrix $\boldsymbol{\Sigma}_{jk}$ in an adaptive phase is difficult and does not
  necessarily result in i.i.d.\ behavior of the Markov chain. Therefore, a commonly-used
  alternative for the covariance matrix of the jumping distribution is to use the local curvature
  information
  $\boldsymbol{\Sigma}_{jk} = -\left( \partial^2 \pi(\boldsymbol{\vartheta}; \mathbf{y}, \mathbf{X}) /
    \partial \boldsymbol{\beta}_{jk}\boldsymbol{\beta}_{jk}^\top \right)^{-1}$,
  or its expectation, computed at the posterior mode estimate $\hat{\boldsymbol{\beta}}_{jk}$,
  requiring building blocks \ref{item:7a} and \ref{item:8}.
  However, fixing $\boldsymbol{\Sigma}_{jk}$ during MCMC simulation might still lead to undesired
  behavior of the Markov chain especially when parameter samples move into regions with low
  probability mass of the posterior distribution. A solution with good mixing properties is to
  construct approximate full conditionals $\pi(\boldsymbol{\beta}_{jk} | \cdot)$ that are based on a
  second order Taylor series expansion of the log-posterior centered at the last state 
  \citep{bamlss:Gamerman:1997, bamlss:Klein+Kneib+Lang:2015}.
  The resulting proposal density $q(\boldsymbol{\beta}_{jk}^\star | \boldsymbol{\beta}_{jk}^{(t)})$
  is again normal (see supplements Section~4) with precision matrix and mean given by
  $$
  \left(\boldsymbol{\Sigma}_{jk}^{(t)}\right)^{-1} = -\mathbf{H}_{kk}\left(\boldsymbol{\beta}_{jk}^{(t)}\right) \qquad
  \boldsymbol{\mu}_{jk}^{(t)} =  \boldsymbol{\beta}_{jk}^{(t)} -
  \left[\mathbf{J}_{kk}\left( \boldsymbol{\beta}_{jk}^{(t)} \right) +
    \mathbf{G}_{jk}(\boldsymbol{\tau}_{jk})\right]^{-1}\mathbf{s}\left( \boldsymbol{\beta}_{jk}^{(t)} \right),
  $$
  which is equivalent to the updating function given in (\ref{eqn:blockblocknewton}) and can again
  be build using blocks \ref{item:7a} and \ref{item:8}.
  Hence, the mean is simply one Newton or Fisher scoring iteration towards the posterior mode at
  the current step.

  Again, assuming a basis function approach for $f_{jk}( \cdot )$ the precision and mean are
  $$
  \left(\boldsymbol{\Sigma}_{jk}^{(t)}\right)^{-1} =
    \mathbf{X}_{jk}^\top\mathbf{W}_{kk}\mathbf{X}_{jk} + \mathbf{G}_{jk}(\boldsymbol{\tau}_{jk})
  \qquad
  \boldsymbol{\mu}_{jk}^{(t)} =
    \boldsymbol{\Sigma}_{jk}^{(t)}\mathbf{X}_{jk}^\top\mathbf{W}_{kk}\left(\mathbf{z}_k  -
      \boldsymbol{\eta}^{(t)}_{k,-j}\right)
  $$
  with weights $\mathbf{W}_{kk} = -\mathrm{diag}(\partial^2 \ell(\boldsymbol{\beta}; \mathbf{y}, \mathbf{X}) /
\partial \boldsymbol{\eta}_k \partial \boldsymbol{\eta}_k^\top)$, or the corresponding expectation,
  as in posterior mode updating using building blocks \ref{item:7b} and \ref{item:8}, with
  working observations
  $\mathbf{z}_k = \boldsymbol{\eta}_k^{(t)} + \mathbf{W}_{kk}^{-1}\mathbf{u}_k^{(t)}$
  (see also the supplemental material Section~4 for a detailed derivation). Note again,
  the computation of the mean is equivalent to a full Newton step as given in updating function
  (\ref{eqn:blockblocknewton}), or Fisher scoring when using
  $-E(\partial^2 \ell(\boldsymbol{\beta}; \mathbf{y}, \mathbf{X}) /
    \partial \boldsymbol{\eta}_k \partial \boldsymbol{\eta}_k^\top)$,
  in each iteration of the MCMC sampler using iteratively weighted least squares (IWLS). If
  the computation of the weights is expensive, one simple strategy is to update
  $\mathbf{W}_{kk}$ only after samples of all parameters of $\boldsymbol{\theta}_k$ are drawn.

\item \emph{Slice sampling}: \label{sec:smcmc}
  Slice sampling \citep{bamlss:Neal:2003} is a gradient free MCMC sampling scheme that produces
  samples with $100\%$ acceptance rate. Therefore, and because of the simplicity of the algorithm,
  slice sampling is especially useful for automated general purpose MCMC implementations that allow
  for sampling from many distributions.  Updates for the $i$-th parameter in $\boldsymbol{\beta}_{jk}$ are
  generated by:
  \begin{enumerate}
  \item Sample $h \sim \mathcal{U}(0, \pi(\beta_{ijk}^{(t)} | \, \cdot))$.
  \item Sample $\beta_{ijk}^{(t+1)} \sim \mathcal{U}(S)$ from the horizontal slice
    $S = \{\beta_{ijk}: h < \pi(\beta_{ijk} | \, \cdot)\}$.
  \end{enumerate}
\end{itemize}


\subsubsection{Boosting}

\subsection{Model evaluation}

\subsubsection{Model Choice and measures of performance}

\begin{itemize}
\item information criteria
\item Quantile residuals
\item scoring rules
\end{itemize}

\subsubsection{Model evaluation}
\begin{itemize}
\item plotting
\item Quantile residuals
\item scoring rules
\end{itemize}


\section{The bamlss package} \label{sec:package}

\section{Application} \label{sec:application}

<<application-load-model, echo=FALSE>>=
load("application.rda")
f <- formula(sel)
@

<<application-model, eval=FALSE>>=
## --- data ---
load("ALDIS_ERA5_subset.rda")

## --- specify formula ---
f <- list(
  mu = counts ~ s(d2m, bs = "ps") + s(q_prof_PC1, bs = "ps") +
    s(cswc_prof_PC4, bs = "ps") + s(t_prof_PC1, bs = "ps") +
    s(v_prof_PC2, bs = "ps") + s(sqrt_cape, bs = "ps"), 
  theta = ~s(sqrt_lsp, bs = "ps")
)

## --- fit model ---
b <- bamlss(f, data = d_train,                # standard interface
  family = "ztnbinom", binning = TRUE,        # general arguments
  optimizer = boost, maxit = 1000,            # boosting arguments
  thin = 5, burnin = 1000, n.iter = 6000      # sampler arguments
)
@

\begin{figure}
\begin{center}
<<app-plot-boost, fig = TRUE>>=
boost.plot(b, "loglik.contrib")
@
\end{center}
\caption{Contribution to the log-likelihood of individual terms during
gradient boosting.}
\label{fig:appboost}
\end{figure}

\begin{figure}
\begin{center}
<<app-plot-effect1, fig = FALSE, eval = FALSE>>=
plot(b, term = c("s(sqrt_cape)", "s(q_prof_PC1)", "s(sqrt_lsp)"),
  rug = TRUE)
@
\setkeys{Gin}{width=1\textwidth}
<<echo = FALSE, fig = TRUE, height = 3, width = 6>>=
par(mfrow = c(1, 3))
plot(b, term = c("s(sqrt_cape)", "s(q_prof_PC1)", "s(sqrt_lsp)"),
  ask = FALSE, spar = FALSE, scheme = 2, grid = 200, rug = TRUE)
@
\end{center}
\caption{Effect of the terms \code{s(sqrt\_cape)} and \code{s(q\_prof\_PC1)}
from the model \code{mu} and term \code{s(sqrt\_lsp)} from model \code{theta}.
Credible intervals derived from MCMC samples.}
\label{fig:appeffect}
\end{figure}

\begin{figure}
\begin{center}
<<app-plot-trace, fig = FALSE, eval = FALSE>>=
plot(b, model = "mu", term = "s(sqrt_cape)", which = "samples")
@
<<echo = FALSE, fig = TRUE>>=
b2 <- b
b2$samples[[1]] <- b2$samples[[1]][, grep("s(sqrt_cape)", colnames(b2$samples[[1]]), fixed = TRUE)][, 1:2]

par(mar = c(4.1, 4.1, 4.1, 1.1))
plot(b2, which = "samples", ask = FALSE)
@
\end{center}
\caption{MCMC trace (left panels) and auto-correlation (right panels) for two
splines from the term \code{s(sqrt\_cape)} of the model \code{mu}.}
\label{fig:apptrace}
\end{figure}

\begin{figure}
\begin{center}
<<app-plot-rootogram, fig = TRUE>>=
library("countreg")

## --- out-of-sample prediciton ---
fit <- predict(b, newdata = d_eval, type = "parameter")
d_eval <- cbind(d_eval, as.data.frame(fit))

## --- compute quantities for rootogram ---
obsrvd <- table(d_eval$counts)[1:50]

fam <- family(b)
par <- subset(d_eval, select = c("mu", "theta"))
expect <- sapply(1:50, function(j) sum(fam$d(j, par)))
names(expect) <- 1:50
expect <- as.table(expect)

## --- plot rootogram ---
rootogram(obsrvd, expect, xlab = "#Counts", main = "Rootogram")
@
\end{center}
\caption{Hanging rootogram for evaluating calibration of count data model
on out-of-sample data. Red line indicates the expected frequencies on the
square root scale. Gray bars indicate observed frequencies on square root
scale hanging from the red line.}
\label{fig:approoto}
\end{figure}

\begin{figure}
\begin{center}
<<app-plot-case, eval = FALSE>>=
library("sf")
library("ggplot2")
library("colorspace")
  
fit <- predict(b, newdata = d_case, type = "parameter")
d_case$P10 <- 1 - fam$p(9, fit)
  
ggplot() + geom_sf(aes(fill = P10), data = d_case) +
  scale_fill_continuous_sequential("Inferno", rev = TRUE) +
  geom_sf(data = gadm_countries, col = "white", fill = NA) +
  facet_wrap(~time) + theme_minimal()
@
<<app-plot-case-eval, fig = TRUE, echo = FALSE>>=
library("sf")
library("ggplot2")
library("colorspace")

fit <- predict(b, newdata = d_case, type = "parameter")
d_case$P10 <- 1 - fam$p(9, fit)
  
ggplot() + geom_sf(aes(fill = P10), data = d_case) +
  scale_fill_continuous_sequential("Inferno", rev = TRUE) +
  geom_sf(data = gadm_countries, col = "white", fill = NA) +
  facet_wrap(~time, nrow = 2) + theme_minimal() # +
#  theme(legend.position = "bottom")
@
\end{center}
\caption{A probabilistic reconstruction of lightning counts occured on
September 15 2001 at 6~UTC, 17~UTC and 23~UTC and on September 16 2001 at
13~UTC, i.e., the probability of having observed $10$ or more counts within one
grid box.}
\label{fig:appcase}
\end{figure}


\bibliography{bamlss}

\end{document}

