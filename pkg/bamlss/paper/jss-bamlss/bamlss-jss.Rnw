%\documentclass[article]{jss}
\documentclass[nojss]{jss}
\usepackage{amsmath,amssymb,amsfonts,thumbpdf}
\usepackage{multirow,longtable}
\usepackage{tikz}
\usepackage{multirow}

\usetikzlibrary{fit,backgrounds,arrows,calc}

\usetikzlibrary{arrows.meta}
\tikzset{%
  >={Latex[width=2mm,length=2mm]},
  % Specifications for style of nodes:
  base/.style = {rectangle, rounded corners, draw=black,
                 minimum width=3.2cm, minimum height=0.8cm,
                 text centered, font=\ttfamily},
  input/.style = {base, minimum width=2.2cm, fill=blue!30},
  processing/.style = {base, fill=yellow!30},
  estimation/.style = {base, fill=red!30},
  stats/.style = {base, fill=green!30},
  output/.style = {base, minimum width=2.2cm, fill=orange!15},
  outer/.style={draw=gray, dashed, fill=green!1, thick, inner sep=5pt, rounded corners},
}

\newcommand*{\conH}[4][]{
  \draw[#1] (#3) -| ($(#3) !#2! (#4)$) |- (#4);
}
\newcommand*{\conV}[4][]{
  \draw[#1] (#3) |- ($(#3) !#2! (#4)$) -| (#4);
}

%% additional commands
\newcommand{\squote}[1]{`{#1}'}
\newcommand{\dquote}[1]{``{#1}''}
\newcommand{\fct}[1]{{\texttt{#1()}\index{#1@\texttt{#1()}}}}
\newcommand{\class}[1]{\dquote{\texttt{#1}}}
%% for internal use
\newcommand{\fixme}[1]{\emph{\marginpar{FIXME} (#1)}}
\newcommand{\readme}[1]{\emph{\marginpar{README} (#1)}}

%% Authors: NU + rest in alphabetical order
\author{Nikolaus Umlauf\\Universit\"at Innsbruck \And
        Achim Zeileis\\Universit\"at Innsbruck \AND
        Nadja Klein\\Humboldt Universit\"at zu Berlin \And
        Thorsten Simon\\Universit\"at Innsbruck}
\Plainauthor{Nikolaus Umlauf, Achim Zeileis, Nadja Klein, Thorsten Simon}

\title{\pkg{bamlss}: A Lego Toolbox for {B}ayesian Additive Models for
  Location Scale and Shape (and Beyond)}
\Plaintitle{bamlss: A Lego Toolbox for Bayesian Additive
  Models for Location Scale and Shape (and Beyond)}
\Shorttitle{A Lego Toolbox for BAMLSS (and Beyond)}
	
\Keywords{GAMLSS, distributional regression, backfitting, gradient boosting, LASSO,
  MCMC, \proglang{R}}
\Plainkeywords{GAMLSS, distributional regression, backfitting, gradient boosting, LASSO,
  MCMC, R}

\Abstract{
During the last decades there has been an increasing interest in distributional regression 
models that allow to model the entire data distribution conditional on covariates. In particular,
the  framework of structured additive distributional regression models enables to specify 
different types of effects such as linear, non-linear or interaction effects on all the 
distribution parameters (such as location scale and shape) hence providing a very flexible and
generic framework suited for 
many complex real data problems. However, the implementation of new models is usually time-consuming
and complex, especially using Bayesian estimation algorithms. We propose an unified modeling
architecture, which is implemented in the \proglang{R} package \pkg{bamlss}, that makes it possible
to embed many different approaches suggested in literature and software. We show that
implementing (new) algorithms, non-standard distributions or effect types, or the integration of
already existing software, is relatively straightforward in this setting. We illustrate the
usefulness of the approach by implementing highly efficient algorithms for fully Bayesian inference
based on MCMC simulation, backfitting algorithms, gradient boosting, LASSO-type penalized models as
well as neural network distributional regression models.
}

\Address{
  Nikolaus Umlauf, Achim Zeileis, Thorsten Simon\\
  Department of Statistics\\
  Faculty of Economics and Statistics\\
  Universit\"at Innsbruck\\
  Universit\"atsstr.~15\\
  6020 Innsbruck, Austria\\
  E-mail: \email{Nikolaus.Umlauf@uibk.ac.at},\\
  \phantom{E-mail: }\email{Achim.Zeileis@R-project.org}\\
  \phantom{E-mail: }\email{Thorsten.Simon@uibk.ac.at}\\
  URL: \url{http://eeecon.uibk.ac.at/~umlauf/},\\
  \phantom{URL: }\url{http://eeecon.uibk.ac.at/~zeileis/}\\

  Nadja Klein\\
  Humboldt Universit\"at zu Berlin\\
  School of Business and Economics\\
  Applied Statistics\\
  Unter den Linden 6\\
  10099 Berlin, Germany\\
  E-mail: \email{nadja.klein@hu-berlin.de}\\
  URL: \url{https://hu.berlin/NK}
}

\SweaveOpts{engine = R, eps = FALSE, keep.source = TRUE}

<<preliminaries, echo=FALSE, results=hide>>=
options(width = 70, prompt = "R> ", continue = "+  ",
  SweaveHooks = list(fig = function() par(mar = c(4.1, 4.1, 1, 1))))
library("bamlss")
@

\input{defs}

\begin{document}
\SweaveOpts{concordance=TRUE}

\section{Introduction}

Generalized additive models for location, scale and shape
(GAMLSS,~\citealp{bamlss:Rigby+Stasinopoulos:2005}), also known as distributional regression models
\citep{bamlss:Klein+Kneib+Lang+Sohn:2015, bamlss:Klein+Kneib+Klasen+Lang:2015}, provide a very
flexible framework that enables modeling all parameters of a response distribution in terms of
covariates. While most common model specifications (linear models, generalized linear models [GLMs],
generalized additive models [GAMs], etc.) ignore the fact that the distribution of any quantity is
not well characterized by the mean alone, GAMLSS has the clear advantage to alleviate the risk of
model misspecification and therefore provides much more reliable probabilistic forecasts.

The \proglang{R} package \pkg{gamlss}
\citep{bamlss:Stasinopoulos:Rigby:2007, bamlss:Stasinopoulos+Rigby:2019} and \pkg{VGAM}
\citep{bamlss:Yee:2009, bamlss:Yee:2019} were the first packages
to follow the distributional regression approach and comprise a quite flexible framework for
implementing response distributions. Variable selection algorithms for GAMLSS based on gradient
boosting were then introduced by the \pkg{gamboostLSS} package
\citep{bamlss:Hofner+Mayr+Schmid:2014, bamlss:Hofner:2018} and more recently, the contributed
base \proglang{R} package \pkg{mgcv} \citep{bamlss:Wood:2017, bamlss:Wood:2019} provides a
fitting routine for the estimation of general smooth models \citep{bamlss:Wood+Pya+Saefken:2016}.
The inferential framework of the packages mentioned is based on maximum likelihood, however, if
taken into account at all. For complex models, e.g., with response
distributions outside the exponential family combined with complex predictor structures of
several smooth effects \citep{bamlss:Klein+Kneib+Lang:2015}, the \pkg{gamlss} package may
suffer from numerical instabilities. In such situations the fully Bayesian approach using Markov chain
Monte Carlo (MCMC) simulation techniques is particularly attractive since valid credible intervals
are easily obtained from the posterior samples. While Bayesian distributional regression models
can in principle be estimated using general purpose MCMC software like
\pkg{JAGS}~\citep{bamlss:Plummer:2013}, \pkg{Stan}~\citep{bamlss:stan-software:2016} or
\pkg{WinBUGS}~\citep{bamlss:Lunn+Thomas+Best+Spiegelhalter:2000}, they all have the drawback
that sampling time is usually quite long, or it is even impossible to estimate distributional
regression models with GAM-type predictors, e.g., when using large data sets, modeling spatial
effects or complex higher-order interactions. The first software capable of estimating such
GAM-type distributional regression models using MCMC is the standalone package
\pkg{BayesX}~\citep{bamlss:Brezger+Kneib+Lang:2005, bamlss:Umlauf:2015, bamlss:Belitz+Brezger+Kneib+Lang+Umlauf:2017},
which provides highly efficient sampling schemes for very large
data sets as well as (spatial) multilevel models. However, the implementation of new response
distributions (models) in \pkg{BayesX} is not straightforward and users usually have to ask the
maintainers to do so.

In this paper, we present the \proglang{R} package \pkg{bamlss}, a flexible ``Lego toolbox''
for Bayesian distributional regression models (and beyond). The main contributions of \pkg{bamlss}
are the following:
%
\begin{itemize}
\item Estimate distributional regression models with usual \proglang{R} modeling ``look \& feel'',
\item easy implementation of (new) models and algorithms (Bayesian or frequentist),
\item comparison of existing optimization algorithms and samplers,
\item or integration of existing implementations.
\end{itemize}
%
The package builds on the well-established \pkg{mgcv} infrastructures using
\proglang{R}'s formula syntax for model specification. Moreover, the package provides
commonly used extractor functions like \fct{summary}, \fct{plot}, \fct{predict}, etc., such
that users interested in developing new models or algorithms (Bayesian or frequentist) can
really concentrate on it.
The package is made available from the Comprehensive
\proglang{R} Archive Network (CRAN) at \url{http://CRAN.R-project.org/package=bamlss}.

The remainder of this paper is as follows. In Section~\ref{sec:motivation},
four motivating examples illustrate the first steps using \pkg{bamlss} and show cases the flexibility
of the provided infrastructures. Section~\ref{sec:distreg}
introduces distributional regression models in more detail. A thorough introduction of the
\proglang{R} package \pkg{bamlss}, describing the most important building blocks,
is then given in Section~\ref{sec:package}. In Section~\ref{sec:application} we highlight the
unified modeling approach using a complex distributional regression model for predicting
flash counts.


\section{Motivating examples} \label{sec:motivation}

This section gives a first quick overview of the functionality of the package. The first
example demonstrates that the usual ``look \& feel'' when using well-established model fitting 
functions like \fct{glm} is an elementary part of \pkg{bamlss}, i.e., first steps and
basic handling of the package should be relatively simple. The second example then
explains how full distributional regression models can be estimated and show
cases the flexibility of the provided modeling infrastructures. The third example shows that the
package can also deal with completely different estimation methods by simply exchanging the
model fitting functions, in this case by a LASSO-type estimation engine. The last example explains
how special user defined smooth terms can be used with the default estimation engines.

\subsection{Logit model} \label{sec:logitmodel}

This example data is taken from the \pkg{AER} package \citep{bamlss:Kleiber+Zeileis:2008} and is about
labor force participation (yes/no) of women in Switzerland 1981 \citep{bamlss:Gerfin:1996}.
The data can be loaded with
<<>>=
data("SwissLabor", package = "AER")
@
The data frame contains of 872 observations on 6 variables, where some of them might have
a nonlinear influence on the response labor \code{participation}. Now, a standard Bayesian 
binomial logit model using the default MCMC algorithm can be fitted with
<<echo = FALSE, results = hide>>=
if(!file.exists("SwissLaborModel.rda")) {
  f <- participation ~ income + age + education + youngkids + oldkids + foreign + I(age^2)
  set.seed(123)
  b <- bamlss(f, family = "binomial", data = SwissLabor,
    n.iter = 12000, burnin = 2000, thin = 10)
  save(b, file = "SwissLaborModel.rda")
} else {
  load("SwissLaborModel.rda")
}
@
<<eval = FALSE>>=
library("bamlss")

## Model formula.
f <- participation ~ income + age + education +
  youngkids + oldkids + foreign + I(age^2)

## First, set the seed for reproducibility.
set.seed(123)

## Estimate model.
b <- bamlss(f, family = "binomial", data = SwissLabor,
  n.iter = 12000, burnin = 2000, thin = 10)
@
using 12000 iterations with a burnin-phase of 2000 and a thinning parameter of 10. To obtain
reasonable starting values for the MCMC sampler we run a backfitting algorithm that optimizes
the posterior mode. Using
the main model fitting function \fct{bamlss} all model fitting engines can be exchanged, 
which is explained in detail in Section~\ref{sec:package} and the application
Section~\ref{sec:application}. The default model fitting engines use family objects
(see Section~\ref{sec:package}), similar to the families that can be used
with the \fct{glm} function, which enables easy implementation of new distributions (models).

Note, to capture nonlinearities, a quadratic term for variable \code{age} is added to
the model. The resulting object \code{b} is of class \code{"bamlss"} for which standard
extractor functions like \fct{summary}, \fct{coef}, \fct{plot}, \fct{predict}, etc.\
are available. \hypertarget{msummary}{}The model summary output is printed by~\label{logitmodel_summary}
<<>>=
summary(b)
@
and is based on MCMC samples, which suggest ``significant'' effects for all covariates, 
except for variable \code{education}, since the 95\% credible interval contains zero.
In addition, the acceptance probabilities \code{alpha} are reported and indicate
proper behavior of the MCMC algorithm.
The column \code{parameters} shows respective posterior mode estimates of the regression
coefficients, which are calculated by the upstream backfitting algorithm. Before 
proceeding the analysis, users usually perform additional convergence checks of the
MCMC chains by looking at traceplots and auto-correlation.
<<eval = FALSE>>=
plot(b, which = c("samples", "max-acf"))
@
\begin{figure}[!ht]
\centering
\setkeys{Gin}{width=1\textwidth}
<<echo = FALSE, fig = TRUE, height = 3, width = 8>>=
par(mfrow = c(1, 3), mar = c(4.1, 4.1, 4.1, 1.1))
bsp <- b
bsp$samples <- bsp$samples[, c("pi.p.(Intercept)"), drop = FALSE]
plot(bsp, which = "samples", spar = FALSE, ask = FALSE)
plot(b, which = "max-acf", spar = FALSE, ask = FALSE)
@
\caption{\label{fig:logit_traceplots} Logit model, MCMC trace (left panel),
  auto-correlation for the intercept (middle panel), maximum auto-correlation for all
  parameters (right panel).}
\end{figure}
These are visualized in Figure~\ref{fig:logit_traceplots} and reveal convergence of
the MCMC chains, i.e., there is no visible trend and the very low
auto-correlation shown for the intercept and the maximum auto-correlation of all
parameters suggest close to i.i.d.\ samples from the posterior distribution.
Note that the function call would compute all trace- and auto-correlation plots, however,
for convenience we only show plots for the intercept. In addition, samples can also be
extracted using function \fct{samples}, which returns an object of class \code{"mcmc"},
a class provided by the \pkg{coda} package \citep{bamlss:Plummer+Best+Cowles+Vines:2006}.
This package includes a rich infrastructure for further convergence diagnostic checks,
e.g., Gelman and Rubin's convergence diagnostic
\citep{bamlss:Gelman+Rubin:1992, bamlss:Brooks+Gelman:1998} or
Heidelberger and Welch's convergence diagnostic
\citep{bamlss:Heidelberger+Welch:1981, bamlss:Heidelberger+Welch:1983}.

Model predictions on the probability scale can be obtained by the predict method, e.g.,
to visualize the effect of covariate \code{age} on the probability we can do the following:
<<>>=
## Create new data for prediction.
nd <- data.frame(income = 11, age = seq(2, 6.2, length = 100),
  education = 12, youngkids = 1, oldkids = 1, foreign = "no")

## Predict for both cases of variable foreign.
nd$p.no <- predict(b, newdata = nd, type = "parameter", FUN = c95)
nd$foreign <- "yes"
nd$p.yes <- predict(b, newdata = nd, type = "parameter", FUN = c95)
@
The predict method is applied on all MCMC samples and argument \code{FUN} specifies a function
that can be applied on the predictor or distribution parameter samples. The default is the \fct{mean} function,
however, in this case we additionally extract the empirical 2.5\% and 97.5\% quantiles
using function \fct{c95} to obtain credible intervals (note, individual samples can be
extracted by passing \code{FUN = function(x) \{ return(x) \}}, i.e., this way users can
easily generate their own statistics). Then, the estimated effect can be visualized with
<<eval = FALSE>>=
plot2d(p.no ~ age, data = nd, ylab = "participation",
  ylim = range(c(nd$p.no, nd$p.yes)), lty = c(2, 1, 2))
plot2d(p.yes ~ age, data = nd, col.lines = "blue", add = TRUE,
  lty = c(2, 1, 2))
@
\begin{figure}[!ht]
\centering
\setkeys{Gin}{width=0.4\textwidth}
<<echo = FALSE, fig = TRUE, width = 4.5, height = 3.8>>=
par(mar = c(4.1, 4.1, 0.1, 1.1))

myblue <- function(n, ...) {
  rev(diverge_hcl(n * 2, h = c(260, 260), p1 = 0.8)[1:n])
}

plot2d(p.no ~ age, data = nd, ylab = "participation",
  ylim = range(c(nd$p.no, nd$p.yes)), scheme = 2,
  fill.select = c(0, 1, 0, 1))
plot2d(p.yes ~ age, data = nd, add = TRUE,
  scheme = 2, fill.select = c(0, 1, 0, 1), s2.col = myblue, axes = FALSE)
plot2d(p.yes[, "Mean"] ~ age, data = nd, add = TRUE,
  col.lines = "blue", axes = FALSE)
legend("topright", c("foreign yes", "foreign no"), lwd = 1,
  col = c("blue", "black"), box.col = NA, bg = NA)
@
\caption{\label{fig:logit_effects} Effect of covariate \code{age} on estimated
  probabilities for both cases, \code{foreign} \code{"yes"} and \code{"no"}. The solid lines
  represent mean estimates, the shaded areas show 95\% credible intervals.}
\end{figure}
The estimates are shown in Figure~\ref{fig:logit_effects} and suggest a clear difference 
for the effect of \code{age} between both cases of factor variable \code{foreign}.

\subsection{Gaussian location-scale model} \label{sec:locscalemodel}

In this example we will now extend the framework and estimate a complete distributional regression
model using a small textbook example of the well-known simulated motorcycle accident data
\citep{bamlss:Silverman:1985}.
<<>>=
data("mcycle", package = "MASS")
@
The data set contains measurements of the head acceleration
(in $g$, variable \code{accel}) in a simulated motorcycle accident, recorded in milliseconds after
impact (variable \code{times}). To estimate a Gaussian location-scale model with
$$
\texttt{accel} \sim \mathcal{N}(\mu = f_{\mu}(\texttt{times}),
  \log(\sigma) = f_{\sigma}(\texttt{times}))
$$
where functions $f_{\mu}( \cdot )$ and $f_{\sigma}( \cdot )$ are unspecified smooth
functions, which are estimated using regression splines. The log-link for parameter
$\sigma$ ensures positivity. We can use the following model formula for estimation
<<>>=
f <- list(accel ~ s(times, k = 20), sigma ~ s(times, k = 20))
@
where function \fct{s} is the smooth term constructor from the \pkg{mgcv} package
\citep{bamlss:Wood:2019}, the default of \fct{s} are thin-plate regression splines.
Note that model formulae are provided as lists of formulae, i.e., each list entry
represents one parameter of the response distribution. Moreover, note that all smooth
terms, i.e., \fct{te}, \fct{ti}, etc., are supported by \pkg{bamlss}. This way, it is
also possible to incorporate user defined model terms. A full Bayesian semi-parametric 
distributional regression model can be estimated with
<<echo = FALSE, results = hide>>=
if(!file.exists("toymodel.rda")) {
  set.seed(456)
  b <- bamlss(f, data = mcycle, family = "gaussian",
    n.iter = 12000, burnin = 2000, thin = 10)
  save(b, file = "toymodel.rda")
} else {
  load("toymodel.rda")
}
@
<<eval = FALSE>>=
## Set the seed for reproducibility.
set.seed(456)

## Estimate normal location-scale model.
b <- bamlss(f, data = mcycle, family = "gaussian",
  n.iter = 12000, burnin = 2000, thin = 10)
@
again using 12000 iterations for the MCMC chain, a burnin of 2000 and a thinning
of parameter of 10. After the estimation algorithms are finished, the estimated effects can be
visualized instantly using the plotting method.
<<eval = FALSE>>=
plot(b, model = c("mu", "sigma"))
@
\begin{figure}[!ht]
\centering
\setkeys{Gin}{width=0.4\textwidth}
<<echo = FALSE, fig = TRUE, width = 4.5, height = 3.8>>=
par(mar = c(4.1, 4.1, 0.1, 1.1))
plot(b, model = "mu", spar = FALSE, scheme = 2, grid = 200)
@
<<echo = FALSE, fig = TRUE, width = 4.5, height = 3.8>>=
par(mar = c(4.1, 4.1, 0.1, 1.1))
plot(b, model = "sigma", spar = FALSE, scheme = 2, grid = 200)
@
\caption{\label{fig:toy_effects} Estimated effects of \code{times} on parameter
  $\mu$ and $\sigma$ of the normal location-scale model. The grey shaded areas represent
  95\% credible intervals.}
\end{figure}
The estimated effects are shown in Figure~\ref{fig:toy_effects} depicting a clear nonlinear
relationship for parameter $\mu$ and $\sigma$.

For judging how well the model fits to the data the user can inspect randomized quantile
residuals \citep{bamlss:Dunn:Smyth:1996} using histograms or quantile-quantile plots. 
Residuals can be extracted using function \fct{residuals} and has a plotting method. 
Alternatively, residuals can be investigated with
<<eval = FALSE>>=
plot(b, which = c("hist-resid", "qq-resid"))
@
\begin{figure}[!ht]
\centering
\setkeys{Gin}{width=0.4\textwidth}
<<echo = FALSE, fig = TRUE, width = 4.5, height = 4.7>>=
par(mar = c(4.1, 4.1, 4.1, 1.1))
plot(b, which = "hist-resid", spar = FALSE, col = "lightgray")
@
<<echo = FALSE, fig = TRUE, width = 4.5, height = 4.7>>=
par(mar = c(4.1, 4.1, 4.1, 1.1))
plot(b, which = "qq-resid", spar = FALSE)
@
\caption{\label{fig:toy_resids} Histogram and quantile-quantile plot of the
  resulting randomized quantile residuals of the normal location-scale model.}
\end{figure}
According the histogram and the quantile-quantile plot of the resulting randomized
quantile residuals in Figure~\ref{fig:toy_resids}, the model seems to fit relatively well. 
Only for very low and very high values of \code{accel} the fitted distributions seem
to be less appropriate.

Besides residuals, users can evaluate the model performance, e.g., for model selection 
based on the deviance information criterion (DIC), which can be extracted using function 
\fct{DIC}
<<>>=
DIC(b)
@
and is also reported in the model summary output. Furthermore, statistical calibration
of fitted models can be assessed by scoring rules
\citep{bamlss:Gneiting+Raftery:2007, bamlss:Gneiting+Balabdaoui+Raftery:2007}. For example, 
the \proglang{R} package \pkg{scoringRules} \citep{bamlss:Jordan+Krueger+Lerch:2018} 
provides easy evaluation of the continuous rank probability score (CRPS) for a couple of 
distributions. Moreover, the Appendix~\ref{appendix:crps} provides a code snippet that computes
the CRPS using numerical integration for the normal distribution.

\subsection{LASSO-type penalization} \label{sec:lassointro}

<<echo = FALSE>>=
SwissLabor$c.income <- cut(SwissLabor$income,
  breaks = quantile(SwissLabor$income, prob = seq(0, 1, length = 10)),
  include.lowest = TRUE)

SwissLabor$c.age <- cut(SwissLabor$age,
  breaks = quantile(SwissLabor$age, prob = seq(0, 1, length = 10)),
  include.lowest = TRUE)

SwissLabor$c.education <- with(SwissLabor, cut(education,
  unique(quantile(SwissLabor$education, prob = seq(0, 1, length = 10))),
  include.lowest = TRUE))

SwissLabor$youngkids <- ordered(SwissLabor$youngkids)
SwissLabor$oldkids[SwissLabor$oldkids > 4] <- 5
SwissLabor$oldkids <- ordered(SwissLabor$oldkids)

if(!file.exists("LassoSL.rda")) {
  f <- participation ~ la(c.income,fuse=2) + la(c.age,fuse=2) + la(c.education,fuse=2) +
    la(youngkids,fuse=2) + la(oldkids,fuse=2) + la(foreign,fuse=2)
    
  b <- bamlss(f, family = "binomial", data = SwissLabor,
    optimizer = lasso, sampler = FALSE, upper = exp(5), lower = 1,
    criterion = "BIC", nlambda = 1000)

  save(b, file = "LassoSL.rda")
} else {
  load("LassoSL.rda")
}
@

This section illustrates that estimation engines can easily exchanged using the flexible
infrastructure provided in \pkg{bamlss}. Again, the \code{SwissLabor} data and binomial logit model
of Section~\ref{sec:logitmodel} is used, however, in this example a fused LASSO algorithm is applied
for estimation.
The algorithm performs variable selection in combination with factor fusion (clustering) and can also
be used to identify interpretable nonlinearities. Methodological details on LASSO-type penalization
using \pkg{bamlss} are provided in \citet{bamlss:Groll+Hambuckers+Kneib+Umlauf:2019}.
To apply the fused LASSO and to capture possible nonlinearities, the numeric variables \code{age},
\code{income} and \code{education} are categorized using empirical quantiles. E.g., for variable
\code{age} an ordered factor variable can be obtained by
<<eval = FALSE>>=
SwissLabor$c.age <- cut(SwissLabor$age,
  breaks = quantile(SwissLabor$age, prob = seq(0, 1, length = 10)),
  include.lowest = TRUE)
@
Variables \code{income} and \code{education} are transformed in the same way. Similarly,
variable \code{youngkids} and \code{oldkids} are transformed to ordered factors using
function \fct{ordered}. The formula for the fused LASSO model is then specified with the
special \fct{la} model term constructor function provided in \pkg{bamlss}
<<>>=
f <- participation ~ la(c.income,fuse=2) + la(c.age,fuse=2) +
  la(c.education,fuse=2) + la(youngkids,fuse=2) + la(oldkids,fuse=2) +
  la(foreign,fuse=2)
@
where argument \code{fuse} specifies the type of fusion (nominal fusion \code{fuse=1},
ordered fusion \code{fuse=2}). To estimate the fused LASSO model only the default \code{optimizer}
function in the \fct{bamlss} wrapper function call needs to exchanged
<<eval = FALSE>>=
b <- bamlss(f, family = "binomial", data = SwissLabor,
  optimizer = lasso, sampler = FALSE,
  criterion = "BIC")
@
The optimum shrinkage parameter $\lambda$ is selected by the BIC.
Note that no MCMC sampling is used after the \fct{lasso} estimation engine is applied,
argument \code{sampler = FALSE} in the \fct{bamlss} call.

The BIC curve and the coefficient paths including the optimum shrinkage parameter $\lambda$ can
be visualized with
<<eval = FALSE>>=
lasso.plot(b)
@
and are shown in Figure~\ref{fig:lasso}.
\begin{figure}[!ht]
\centering
\setkeys{Gin}{width=0.32\textwidth}
<<echo = FALSE, fig = TRUE, width = 4, height = 5>>=
par(mar = c(4.1, 4.1, 4.1, 1.1))
lwd <- 2
lasso.plot(b, 1, spar = FALSE, main = "criterion", lwd = lwd)
@
<<echo = FALSE, fig = TRUE, width = 4, height = 5, results = hide>>=
par(mar = c(4.1, 4.1, 4.1, 4.1))
lasso.plot(b, 2, spar = FALSE, name = "pi.s.la(c.income).c.income", main = "income", lwd = lwd)
@
<<echo = FALSE, fig = TRUE, width = 4, height = 5, results = hide>>=
par(mar = c(4.1, 4.1, 4.1, 4.1))
lasso.plot(b, 2, spar = FALSE, name = "pi.s.la(c.age).c.age", main = "age", lwd = lwd)
@
\newline
<<echo = FALSE, fig = TRUE, width = 4, height = 5, results = hide>>=
par(mar = c(4.1, 4.1, 4.1, 4.1))
lasso.plot(b, 2, spar = FALSE, name = "pi.s.la(c.education).c.education", main = "education", lwd = lwd)
@
<<echo = FALSE, fig = TRUE, width = 4, height = 5, results = hide>>=
par(mar = c(4.1, 4.1, 4.1, 4.1))
lasso.plot(b, 2, spar = FALSE,
  name = c("pi.s.la(youngkids).youngkids", "pi.s.la(oldkids).oldkids", "pi.s.la(foreign).foreign"),
  main = "kids & foreign", lwd = lwd)
legend("bottomleft", c("youngkids", "oldkids", "foreign"), lwd = lwd, col = rev(rainbow_hcl(3)),
  box.col = NA, bg = NA)
@
\caption{\label{fig:lasso} Top row left panel, BIC curve with optimum shrinkage parameter $\lambda$
  of the LASSO example model. All other panels show the coefficient paths of categorized variables
  and true factor covariates.}
\end{figure}
The BIC curves shows a clear minimum. The coefficient paths obviously depict that the algorithm
can either shrink categories out of the model (shrink to zero), or even fuses them, e.g.,
as shown for variable \code{youngkids} where categories \code{1}, \code{2} and \code{3} are
all fused and have the same negative effect.

\begin{figure}[!ht]
\centering
\setkeys{Gin}{width=0.32\textwidth}
<<echo = FALSE, fig = TRUE, width = 4, height = 4.3, results = hide>>=
pincome <- predict(b, term = "c.income", intercept = FALSE, mstop = lasso.stop(b))
page <- predict(b, term = "c.age", intercept = FALSE, mstop = lasso.stop(b))
peducation <- predict(b, term = "c.education", intercept = FALSE, mstop = lasso.stop(b))

yr <- range(c(pincome, page, peducation))

plot2d(pincome ~ income, data = SwissLabor, rug = TRUE, ylim = yr, lwd = 2)
@
<<echo = FALSE, fig = TRUE, width = 4, height = 4.3>>=
plot2d(page ~ age, data = SwissLabor, rug = TRUE, ylim = yr, lwd = 2)
@
<<echo = FALSE, fig = TRUE, width = 4, height = 4.3, results = hide>>=
plot2d(peducation ~ education, data = SwissLabor, rug = TRUE, ylim = yr, lwd = 2)
@
\caption{\label{fig:lasso_effects} Fused LASSO, estimated nonlinear effects of categorized variables
  \code{age}, \code{income} and \code{education} with added \fct{rug} representation
  of the data (small bottom vertical lines).}
\end{figure}
In Figure~\ref{fig:lasso_effects}, the estimated effects of the categorized variables \code{age}, 
\code{income} and \code{education} are shown. The effects are computed by predicting without
intercept using the optimum stopping iteration, which is selected by BIC and can be extracted with
function \fct{lasso.stop}. The stopping iteration is passed to the \fct{predict} method by specifying
the \code{mstop} argument.
<<eval = FALSE>>=
p.age <- predict(b, term = "c.age", intercept = FALSE,
  mstop = lasso.stop(b))
@
The figure is then generated using the untransformed original covariate
<<eval = FALSE>>=
plot2d(p.age ~ age, data = SwissLabor, rug = TRUE)
@
Using the fused LASSO estimation some nonlinearities
can be identified, moreover, compared to the insignificant effect
of variable \code{education} in the \hyperlink{msummary}{summary output} of Section~\ref{sec:logitmodel} on
page~\pageref{logitmodel_summary}, the fused LASSO identifies a positive effect on labor
\code{participation} for years of education above 12. The effect size for covariate \code{age} is
the largest according the y-axis of Figure~\ref{fig:lasso_effects}, indicating that \code{age} has
the highest influence on labor \code{participation}.

\subsection{Special model terms} \label{sec:specialterms}

The default estimation engines \fct{bfit} and \fct{GMCMC} (also the gradient boosting optimizer function
\fct{boost}) in \pkg{bamlss} provide support for the implementation of special model terms, i.e.,
model terms that cannot be represented by the \pkg{mgcv} smooth term constructor infrastructures.
One simple example of such a special model term is a nonlinear growth curve, e.g., a nonlinear
Gompertz curve
$$
f(x; \boldsymbol{\beta}) = \beta_1 \cdot \exp(-\beta_2 \cdot \exp(-\beta_3 \cdot x)),
$$
but also the LASSO model term constructor \fct{la} presented in Section~\ref{sec:lassointro} is a
special \pkg{bamlss} model term. The special model term constructor is needed in this case, since
the growth curve is nonlinear in the parameters $\boldsymbol{\beta}$, hence, the default backfitting
and sampling strategies cannot be applied. Fortunately, estimation algorithms in distributional
regression can be split into separate updating equations (see also Section~\ref{sec:algos}). This
means that each model term can have its own updating function. The user interested in this feature
only needs to write a new \fct{smooth.construct} and \fct{Predict.matrix} method, which is provided
in Appendix~\ref{appendix:gc} for the Gompertz growth curve.

Special model terms can then be used with the constructor function \fct{s2}. To illustrate the
this feature in \pkg{bamlss}, we simulate heteroskedastic growth data with
$$
\texttt{y} \sim \mathcal{N}(\mu = 2 + 1 / (1 + \exp(0.5 \cdot (15 - \texttt{time}))),
  \log(\sigma) = -3 + 2 \cdot \cos(\texttt{time}/30 \cdot 6 - 3))
$$
and estimate the model with slice sampling \citep{bamlss:Neal:2003} for $\boldsymbol{\beta}$ in the
MCMC algorithm using the following \proglang{R} code
<<echo = FALSE, results = hide>>=
smooth.construct.gc.smooth.spec <- function(object, data, knots) 
{
  object$X <- matrix(as.numeric(data[[object$term]]), ncol = 1)
  center <- if(!is.null(object$xt$center)) {
    object$xt$center
  } else TRUE
  object$by.done <- TRUE
  if(object$by != "NA")
    stop("by variables not supported!")

  ## Begin special elements to be used with bfit() and GMCMC().
  object$fit.fun <- function(X, b, ...) {
    f <- b[1] * exp(-b[2] * exp(-b[3] * drop(X)))
    if(center)
      f <- f - mean(f)
    f
  }
  object$update <- bfit_optim
  object$propose <- GMCMC_slice
  object$prior <- function(b) { sum(dnorm(b, sd = 1000, log = TRUE)) }
  object$fixed <- TRUE
  object$state$parameters <- c("b1" = 0, "b2" = 0.5, "b3" = 0.1)
  object$state$fitted.values <- rep(0, length(object$X))
  object$state$edf <- 3
  object$special.npar <- 3 ## Important!
  ## End special elements.

  ## Important, This is a special smooth constructor!
  class(object) <- c("gc.smooth", "no.mgcv", "special")

  object
}

Predict.matrix.gc.smooth <- function(object, data, knots) 
{
  X <- matrix(as.numeric(data[[object$term]]), ncol = 1)
  X
}

set.seed(111)

d <- data.frame("time" = 1:30)
d$y <- 2 + 1 / (1 + exp(0.5 * (15 - d$time))) +
  rnorm(30, sd = exp(-3 + 2 * cos(d$time/30 * 6 - 3)))

f <- list(
  y ~ s2(time, bs = "gc"),
  sigma ~ s(time)
)

if(!file.exists("gc.rda")) {
  b <- bamlss(f, data = d, optimizer = bfit, sampler = GMCMC)
  save(b, file = "gc.rda")
} else {
  load("gc.rda")
}
@
<<eval = FALSE>>=
## Simulate data.
set.seed(111)

d <- data.frame("time" = 1:30)
d$y <- 2 + 1 / (1 + exp(0.5 * (15 - d$time))) +
  rnorm(30, sd = exp(-3 + 2 * cos(d$time/30 * 6 - 3)))

## Special model terms must be called with s2()!
f <- list(
  y ~ s2(time, bs = "gc"),
  sigma ~ s(time)
)

## Fit model with special model term.
b <- bamlss(f, data = d, optimizer = bfit, sampler = GMCMC)

## Plot estimated effects.
plot(b)
@
The estimated effects are shown in Figure~\ref{fig:gcurve}.
\begin{figure}[!ht]
\centering
\setkeys{Gin}{width=0.4\textwidth}
<<echo = FALSE, fig = TRUE, width = 4.5, height = 4.7>>=
par(mar = c(4.1, 4.1, 4.1, 1.1))
p <- predict(b, model = "mu", FUN = c95)
plot(y ~ time, data = d, main = expression(mu))
plot2d(p ~ time, data = d, add = TRUE, axes = FALSE,
  fill.select = c(0, 1, 0, 1), scheme = 2)
points(d$time, d$y)
@
<<echo = FALSE, fig = TRUE, width = 4.5, height = 4.7>>=
par(mar = c(4.1, 4.1, 4.1, 1.1))
plot(b, model = "sigma", spar = FALSE, scheme = 2, main = expression(log(sigma)))
@
\caption{\label{fig:gcurve} Estimated nonlinear effects on parameter $\mu$ and $\sigma$ of the
  simulated growth curve example. Gray shaded areas represent 95\% credible intervals.}
\end{figure}
The growth curve mean function estimate seems to fit the data quite well. Also, the nonlinear
relationship for parameter $\sigma$ could be captured by the model.

In summary, in order to build up special \pkg{bamlss} model terms only a few things have to be
considered. The example \proglang{R} code for the Gompertz smooth constructor given in
Appendix~\ref{appendix:gc} is a good starting point for readers interested in using this
feature.

\section{Distributional regression} \label{sec:distreg}

This section briefly summarizes the BAMLSS modeling framework. For a detailed
methodological description please refer to \citet{bamlss:Umlauf+Klein+Zeileis:2018},
as well as to the \hyperlink{somerefs}{references} given below on page \pageref{somerefs2}.

\subsection{Model structure}

Within the framework of GAMLSS or distributional regression models
all parameters of the response distribution can be modeled by
explanatory variables such that
\begin{equation} \label{eqn:dreg}
y \sim \mathbf{\mathcal{D}}\left(h_{1}(\theta_{1}) = \eta_{1}, \,\,
  h_{2}(\theta_{2}) = \eta_{2}, \dots, \,\, h_{K}(\theta_{K}) =
  \eta_{K}\right),
\end{equation}
where $\mathbf{\mathcal{D}}$ denotes a parametric distribution for the response
variable $y$ with $K$ parameters $\theta_k$, $k = 1, \ldots, K$, that are linked to 
additive predictors using known monotonic and twice differentiable functions
$h_{k}(\cdot)$. Note that the response may also be a
$q$-dimensional vector $\mathbf{y} = (y_{1}, \ldots, y_{q})^\top$, e.g., when
$\mathbf{\mathcal{D}}$ is a multivariate distribution
(see, e.g., \citealp{bamlss:Klein+Kneib+Klasen+Lang:2015}).
The additive predictor for the $k$-th parameter is given by
\begin{equation} \label{eqn:addpred}
\boldsymbol{\eta}_k = \eta_k(\mathbf{X}; \boldsymbol{\beta}_k) =
  f_{1k}(\mathbf{X}; \boldsymbol{\beta}_{1k}) + \ldots + f_{J_kk}(\mathbf{X}; \boldsymbol{\beta}_{J_kk}),
\end{equation}
based on $j = 1, \ldots, J_k$ unspecified (possibly nonlinear) functions $f_{jk}(\cdot)$, 
applied to each row of the generic data matrix $\mathbf{X}$, encompassing all available 
covariate information. The corresponding parameters
$\boldsymbol{\beta}_k = (\boldsymbol{\beta}_{1k}, \ldots, \boldsymbol{\beta}_{J_kk})^\top$ 
are typically regression coefficients pertaining to model matrices
$\mathbf{X}_k = (\mathbf{X}_{1k}, \ldots, \mathbf{X}_{J_kk})^\top$,
whose structure only depend on the type of covariate(s) and prior assumptions about
$f_{jk}( \cdot )$.

Usually, functions $f_{jk}( \cdot )$ are based on a basis function approach, where $\eta_k$ then
is a typical GAM-type or so-called structured additive predictor
(STAR,~\citealp{bamlss:Fahrmeir+Kneib+Lang:2004}). \citet{bamlss:Umlauf+Klein+Zeileis:2018}
relax this assumption and let $f_{jk}(\cdot)$ be an unspecified composition of covariate data
and regression coefficients. For example, functions $f_{jk}(\cdot)$ could also represent nonlinear
growth curves, a regression tree, a neural network or LASSO-penalized model terms as shown
in Section~\ref{sec:lassointro}.

For full Bayesian inference, priors need to be assigned to the regression coefficients
$\boldsymbol{\beta}_{jk}$. To be as flexible as possible, \citet{bamlss:Umlauf+Klein+Zeileis:2018}
use the rather general prior $
p_{jk}(\boldsymbol{\beta}_{jk}; \boldsymbol{\tau}_{jk}, \boldsymbol{\alpha}_{jk}) 
$ for the $j$-th model term of the $k$-th parameter, where the form of $p_{jk}( \cdot )$ depends
on the type of function $f_{jk}( \cdot )$. Here,
$\boldsymbol{\tau} =
(\boldsymbol{\tau}_{11}^\top, \ldots, \boldsymbol{\tau}_{J_11}^\top, \ldots,
\boldsymbol{\tau}_{1K}^\top, \ldots, \boldsymbol{\tau}_{J_KK}^\top)^\top$ is
the vector of all assigned hyper-parameters, e.g., representing smoothing variances (shrinkage parameters).
Similarly, $\boldsymbol{\alpha}_{jk}$ is the set of all prior specifications. In most
situations the prior $p_{jk}(\boldsymbol{\beta}_{jk}; \boldsymbol{\tau}_{jk}, \boldsymbol{\alpha}_{jk})$
is based on a multivariate normal kernel for $\boldsymbol{\beta}_{jk}$ and on inverse gamma
distributions for each $\boldsymbol{\tau}_{jk} = (\tau_{1jk}, \ldots, \tau_{L_{jk}jk})^\top$,
but as indicated previously, in principle any type of prior can be used
(see \citealp{bamlss:Gelman:2006, bamlss:Polson+Scott:2012, bamlss:Klein+Kneib:2016,
bamlss:Umlauf+Klein+Zeileis:2018} for
more detailed discussions on priors for $\boldsymbol{\beta}_{jk}$ and $\boldsymbol{\tau}_{jk}$).

\hypertarget{somerefs}{}Examples of distributional models that fit well in this
framework are the ones for: \label{somerefs2}
\begin{itemize}
  \item Univariate responses of any type, e.g.~counts with zero-inflation and or overdispersion as
    proposed in \citet{bamlss:Klein+Kneib+Lang:2015, bamlss:Herwartz:Klein+Strumann:2016},
    continuous responses with spikes, skewness, heavy tails or bounded support as in
    \citet{bamlss:Klein+Kneib+Lang+Sohn:2015,bamlss:Klein+Denuit+Kneib+Lang:2014}, as well as
    responses for extreme events \citep{bamlss:Umlauf+Kneib:2018}.
 \item Multivariate responses such as multivariate normal, multivariate t or Dirichlet regression
   (for analyzing compositional data, \citealp{bamlss:Klein+Kneib+Klasen+Lang:2015}).
 \item Multivariate responses with more complex dependence structures modeled through copulas
   \citet{bamlss:Klein+Kneib:2016b}.
 \item Survival data and joint modeling~\citep{bamlss:Koehler+Umlauf+Greven:2016, bamlss:Koehler+Umlauf+Greven:2018}.
\end{itemize}

\subsection{Posterior estimation} \label{sec:algos}

Estimation typically requires to evaluate the log-likelihood
$\ell(\boldsymbol{\beta}; \mathbf{y}, \mathbf{X})$ function and its derivatives w.r.t.\
all regression coefficients $\boldsymbol{\beta}$ a number of times. For fully Bayesian inference
the log-posterior is either used for posterior mode estimation, or for solving
high-dimensional integrals. e.g., for posterior mean estimation MCMC samples need to be computed.

Although the types of models that can be fitted within the flexible BAMLSS framework
can be quite complex, \citet{bamlss:Umlauf+Klein+Zeileis:2018} show that there are a number
of similarities between optimization and sampling concepts. Fortunately, and albeit the different
model term complexity, algorithms for posterior mode and
mean estimation can be summarized into a partitioned updating scheme with
separate updating equations using leapfrog or zigzag iteration \citep{bamlss:Smyth:1996}, e.g.,
with updating equations
\begin{equation} \label{eqn:blockblockupdate}
(\boldsymbol{\beta}_{jk}^{(t + 1)}, \boldsymbol{\tau}_{jk}^{(t + 1)}) =
  U_{jk}(\boldsymbol{\beta}_{jk}^{(t)}, \boldsymbol{\tau}_{jk}^{(t)}; \, \cdot \,) \qquad
    j = 1, \ldots, J_k, \quad k = 1, \ldots, K,
\end{equation}
where function $U_{jk}( \cdot )$ is an updating function, e.g., for generating one Newton-Raphson
step or for getting the next step in an MCMC simulation, a.o.

Using a basis function approach, the updating functions $U_{jk}( \cdot )$ for posterior mode
(frequentist penalized likelihood) estimation or MCMC for $\boldsymbol{\beta}_{jk}$ share an iteratively
weighted least squares updating step (IWLS,~\citealp{bamlss:Gamerman:1997})
\begin{equation} \label{eqn:blockbackfit}
  \boldsymbol{\beta}_{jk}^{(t+1)}
  = U_{jk}(\boldsymbol{\beta}_{jk}^{(t)}; \, \cdot \,) =
    (\mathbf{X}_{jk}^\top\mathbf{W}_{kk}\mathbf{X}_{jk} +
      \mathbf{G}_{jk}(\boldsymbol{\tau}_{jk}))^{-1}\mathbf{X}_{jk}^\top\mathbf{W}_{kk}(
      \mathbf{z}_k - \boldsymbol{\eta}_{k, -j}^{(t+1)}),
\end{equation}
with weight matrices $\mathbf{W}_{kk}$ and working responses $\mathbf{z}_k$, similarly to the
well-known IWLS updating scheme for generalized linear models (GLM,~\citealp{bamlss:Nelder+Wedderburn:1972}).
The matrices $\mathbf{G}_{jk}(\boldsymbol{\tau}_{jk})$ are derivative matrices of the priors
$p_{jk}(\boldsymbol{\beta}_{jk}; \boldsymbol{\tau}_{jk}, \boldsymbol{\alpha}_{jk})$ w.r.t.\ the
regression coefficients $\boldsymbol{\beta}_{jk}$, e.g.,
$\mathbf{G}_{jk}(\boldsymbol{\tau}_{jk})$ can be a penalty matrices that penalizes the complexity of
$f_{jk}( \cdot )$ using a P-spline representation \citep{bamlss:Eilers+Marx:1996}.

Even if the functions $f_{jk}( \cdot )$ are not based on a basis function approach, the
updating scheme (\ref{eqn:blockbackfit}) can be further generalized to
$$
\boldsymbol{\beta}_{jk}^{(t + 1)} = U_{jk}\left(\boldsymbol{\beta}_{jk}^{(t)},
  \mathbf{z}_k - \boldsymbol{\eta}_{k, -j}^{(t+1)}; \, \cdot \,\right),
$$
i.e., theoretically any updating function applied to the ``partial residuals''
$\mathbf{z}_k - \boldsymbol{\eta}_{k, -j}^{(t+1)}$ can be used. (For detailed derivations see
\citealp{bamlss:Umlauf+Klein+Zeileis:2018}.)

The great advantage of this modular architecture is, that the concept does not limit to modeling
of the distributional parameters $\theta_k$ in (\ref{eqn:dreg}), e.g.\ as mentioned above,
based on the survival function,
\citet{bamlss:Koehler+Umlauf+Greven:2016} and \citet{bamlss:Koehler+Umlauf+Greven:2018} implement
Bayesian joint models for survival and longitudinal data. Moreover, the updating schemes do not
restrict to any particular estimation engine, e.g.,
\citet{bamlss:Groll+Hambuckers+Kneib+Umlauf:2019} use the framework to implement LASSO-type
penalization for GAMLSS and \citet{bamlss:Simon+Fabsic+Mayr+Umlauf+Zeileis:2018}
investigate gradient boosting with stability selection algorithms
(see also Section~\ref{sec:application}). Very recently, \citet{bamlss:Umlauf+Klein:2019}
implement neural network distributional regression models.

\subsection{Model choice and evaluation}

\subsubsection{Measures of performance}

Model choice and variable selection is important in distributional regression due to the large
number of candidate models. The following lists commonly used tools:
\begin{itemize}
\item \emph{Information criteria} can be used to compare different model specifications.  
  For posterior mode estimation, the Akaike information criterion (AIC), or the corrected
  AIC, as well as the Bayesian information criterion (BIC), can be used. Estimation of model
  complexity is based on the so-called equivalent degrees of freedom (EDF). 

  For MCMC based estimation, model choice mainly relies on the deviance information criterion
  (DIC, \citealp{SpiBesCarLin2002}) and the widely applicable information criterion
  (WAIC, \citealp{watanabe2010asymptotic}). 
\item \emph{Quantile residuals} \citep{bamlss:Dunn:Smyth:1996} can be used to evaluate the model
  fit. The residuals can be assessed by quantile-quantile-plots, probability integral transforms
  (PIT) histograms \citep{bamlss:Gneiting+Balabdaoui+Raftery:2007} or worm plots
  \citep{bamlss:Buuren+Miranda:2001}.
\item \emph{Scoring rules}: Sometimes it is helpful to evaluate the performance on a test data set
  (or for instance based on cross validation). For this, proper scoring rules
  \citep{bamlss:Gneiting+Raftery:2007, bamlss:Gneiting+Balabdaoui+Raftery:2007} can be utilized.
\end{itemize}

\subsubsection{Evaluation and interpretation}
 \begin{itemize}
\item \emph{Plotting}: Estimated functions $\hat{f}_{jk}( \cdot )$ are usually centered around their
  mean, therefore, simple effect plots are a straightforward method to evaluate individual model
  term importance and can also be used for respective interpretations. Sometimes it can be useful in
  distributional regression to look at transformations of the original model parameters such as
  expected value or variance of the response variable $\mathbf{y}$.
\item \emph{Predictions}: For obtaining such transformations model predictions need to
  be computed. This can be done either manually by the corresponding \fct{predict} method, or by
  the \proglang{R} package \pkg{distreg.vis} \citep{bamlss:distreg.vis}, which provides a
  graphical user interface for visualization of distributional regression models.
 \end{itemize}

\newpage

\section{The bamlss package} \label{sec:package}

The \proglang{R} package \pkg{bamlss} provides a modular software architecture for
distributional regression models (and beyond). The implementation follows the conceptional framework
presented in \citet{bamlss:Umlauf+Klein+Zeileis:2018}, which supports Bayesian and/or frequentist
estimation engines using complex possibly nonlinear model terms of any type. The highlights of the
package are:
\begin{itemize}
\item A unified model description where a \code{formula} specifies how to set up the predictors
  from the \code{data} and the \code{family}, which holds information about the response
  distribution, the model.
\item A generic method for setting up model terms and a \fct{model.frame} for BAMLSS, the
  \fct{bamlss.frame}, along with the corresponding prior structures. A \fct{transform}
  function can
  optionally set up modified terms, e.g., using mixed model representation for smooth terms.
\item Support for modular and exchangeable updating functions or complete model fitting engines
  in order to optionally implement either algorithms for maximization of the log-posterior for
  posterior mode estimation or for solving high-dimensional integrals, e.g., for posterior mean
  or median estimation.
  First, an (optional) \fct{optimizer} function can be run, e.g., for computing posterior mode
  estimates. Second, a \fct{sampler} is employed for full Bayesian inference with MCMC, which uses the
  posterior mode estimates from the \fct{optimizer} as staring values. An additional step can be used
  for preparing the \fct{results}, e.g., for creating model term effect plots.
\item Standard post-modeling extractor functions to create sampling statistics, visualizations,
  predictions, etc.
\end{itemize}
\begin{figure}[ht!]
\centering
\begin{tikzpicture}[node distance=1.5cm,
    every node/.style={fill=white, font=\sffamily}, align=center]
  % Specification of nodes (position, etc.)
  \node (data)        [input]                               {\texttt{data}};
  \node (formula)     [input, left of=data, xshift=-4.5cm]  {\texttt{formula}};
  \node (family)      [input, right of=data, xshift=-4.5cm]   {\texttt{family}};

  \begin{pgfonlayer}{background}
  \node[outer,fit=(data) (formula) (family)] (A) {};
  \node (text) [right of=A, xshift=-6.5cm] {Input};
  \end{pgfonlayer}

  \node (bframe) [processing, below of=family, yshift=-0.3cm] {\texttt{bamlss.frame()}};
  \node (trans)  [processing, below of=bframe, yshift=0.3cm] {\texttt{transform()}};

  \coordinate[below of=family, yshift=0.6cm] (invisible6);
  \draw[->, shorten >=3pt] (invisible6) -- (bframe);

  \draw[] (data)    -- (invisible6);
  \draw[] (family)  -- (invisible6);
  \draw[] (formula) -- (invisible6);
  \draw[->, shorten >=1pt, line width=0.8mm, gray] (bframe)  -- (trans);

  \begin{pgfonlayer}{background}
  \node[outer,fit=(bframe) (trans), minimum width=8.55cm] (B) {};
  \node (text) [right of=B, xshift=-7.2cm] {Pre-processing};
  \end{pgfonlayer}

  \node (opt) [estimation, below of=trans] {\texttt{optimizer()}};
  \node (sampler) [estimation, below of=opt, yshift=0.3cm] {\texttt{sampler()}};

  \coordinate[right of=sampler, yshift=0.15cm] (invisible5);

  \draw[->, shorten >=3pt, line width=0.8mm, gray] (trans) -- (opt);
  \draw[->, shorten >=1pt, line width=0.8mm, gray] (opt)   -- (sampler);
  \draw[->, shorten >=3pt, line width=0.8mm, gray] (trans.west) to [out=-180,in=180] (sampler.west);
  \draw[->, shorten >=6pt, line width=0.8mm, gray] (bframe.east) to [out=10,in=10] (invisible5);
  \draw[->, shorten >=3pt, line width=0.8mm, gray] (bframe.east) to [out=10,in=10] (opt.east);

  \begin{pgfonlayer}{background}
  \node[outer,fit=(opt) (sampler), minimum width=8.55cm] (C) {};
  \node (text) [right of=C, xshift=-6.9cm] {Estimation};
  \end{pgfonlayer}

  \node (sstats) [stats, below of=sampler] {\texttt{samplestats()}};
  \node (results) [stats, below of=sstats, yshift=0.3cm] {\texttt{results()}};

  \begin{pgfonlayer}{background}
  \node[outer,fit=(sstats) (results), minimum width=8.55cm] (D) {};
  \node (text) [right of=D, xshift=-7.25cm] {Post-processing};
  \end{pgfonlayer}

  \draw[->, shorten >=3pt, line width=0.8mm, gray] (sampler) -- (sstats);
  \draw[->, shorten >=1pt, line width=0.8mm, gray] (sstats)  -- (results);

  \node (plot) [output, below of=results, yshift=-0.5cm] {\texttt{plot()}};
  \node (summary) [output, left of=plot, xshift=-1.5cm]  {\texttt{summary()}};
  \node (predict) [output, right of=plot, xshift=1.5cm]  {\texttt{predict()}};

  \begin{pgfonlayer}{background}
  \node[outer,fit=(summary) (plot) (predict)] (E) {};
  \node (text) [right of=E, xshift=-6.6cm] {Output};
  \end{pgfonlayer}

  \coordinate[below of=results, yshift=0.55cm] (invisible);
  \coordinate[below of=results, yshift=0.7cm] (invisible3);

  \draw[->, shorten >=3pt] (invisible) -- (plot);
  \draw[->, shorten >=3pt] (invisible) -- (summary);
  \draw[->, shorten >=3pt] (invisible) -- (predict);
  \draw[] (results) -- (invisible);

  \draw[->, shorten >=3pt, line width=0.8mm, gray] (opt.west) to [out=-180,in=180] (results.west);

  \coordinate[left of=opt, xshift=-2cm] (invisible2);
  \coordinate[right of=sampler, xshift=2cm] (invisible4);
  \draw[line width=0.8mm, gray] (opt.west) |- (invisible2);
  \draw[line width=0.8mm, gray] (sampler.east) |- (invisible4);
  \draw[->, shorten >=3pt, line width=0.8mm, gray] (invisible2) |- (invisible3);
  \draw[->, shorten >=3pt, line width=0.8mm, gray] (invisible4) |- (invisible3);

  %%\draw[->, shorten >=3pt, line width=0.8mm, gray, -stealth] (opt.west) to [out=-180,in=180] (invisible.west);
  \draw[->, shorten >=3pt, line width=0.8mm, gray] (sampler.east) to [out=10,in=10] (results.east);

  \end{tikzpicture}
\caption{\label{fig:flowchart} Flow chart of the \pkg{bamlss} modeling architecture. Thick gray
  lines represent optional paths, e.g., after building the \fct{bamlss.frame} the user can either
  run an \fct{optimizer} function prior running the \fct{sampler}, or run the \fct{sampler}
  function directly.}
\end{figure}
The modular architecture of \pkg{bamlss} is illustrated in Figure~\ref{fig:flowchart}. As
mentioned above, the first
step in model development is to setup design and penalty matrices for a model that is specified
by the \code{family} object. Therefore a \code{formula} is processed together with the \code{data}
using the \fct{bamlss.frame} function. In a second pre-processing step, the returned model frame
may also transformed. The BAMLSS model frame can then be used with \fct{optimizer} and/or
\fct{sampler} functions in the estimation step. This is probably the main advantage of the
architecture, users can easily exchange and integrate user defined estimation functions.
The only requirement is to keep the structure of the \fct{bamlss.frame} function, as well for
\fct{optimizer} and \fct{sampler} functions. After the estimation step optional post-processing
functions can be applied to create additional sampling statistics, function \fct{samplestats},
or results that can be used for plotting the estimated effects, function \fct{results}. Note
that the post-processing step is optional since it is not necessarily needed in the last
output step, e.g., for computing predictions. This feature is especially important when using
large data sets, because the run time for computing \fct{samplestats} or \fct{results} can be
quite long or computations can even lead to memory problems. In summary, the architecture is very flexible
such that users interested in implementing new models only need to focus on the
estimation step, i.e., write \fct{optimizer} or \fct{sampler} functions and get all post-processing
and extractor functionalities ``for free''. This way, prototyping
becomes relatively easy, but also the integration/implementation of (new) high-performance
estimation engines is facilitated.

Table~\ref{tab:functions} provides an overview of current available functions.
\begin{table}[ht!]
\centering
\begin{tabular}{lll}
Step & Type        & Function \\ \hline
\multirow{3}{*}{Pre-processing} & Parser      & \code{bamlss.frame()} \\ \cline{2-3}
 & \multirow{2}{*}{Transformer} & \code{bamlss.engine.setup()}, \code{randomize()} \\
 & & \code{lasso.transform()} \\ \hline
\multirow{4}{*}{Estimation} & \multirow{2}{*}{Optimizer} & \code{bfit()}, \code{bbfit()},
                              \code{boost()}, \code{lasso()} \\
 & &\code{cox.mode()}, \code{jm.mode()}  \\ \cline{2-3}
 & \multirow{2}{*}{Sampler} & \code{GMCMC()}, \code{BayesX()}, \code{JAGS()} \\
 & & \code{cox.mcmc()}, \code{jm.mcmc()} \\ \hline
Post-processing & Stats \& Results & \code{samplestats()}, \code{results.bamlss.default()}
\end{tabular}
\caption{\label{tab:functions} Current available functions that can be used for
  pre-processing, estimation and post-processing within the \pkg{bamlss} framework.}
\end{table}

To exemplify the presented ``Lego toolbox'', the following \proglang{R} code estimates the
logit model using the \code{SwissLabor} data presented in Section~\ref{sec:logitmodel}. First,
the data is loaded and the model formula is specified with
<<>>=
data("SwissLabor", package = "AER")

## Model formula.
f <- participation ~ income + age + education +
  youngkids + oldkids + foreign + I(age^2)
@
In the second step, the necessary design matrices are constructed using the model frame
parser function \fct{bamlss.frame}
<<>>=
bf <- bamlss.frame(f, data = SwissLabor, family = "binomial")
@
Then, posterior mode estimates are obtained by using the implemented backfitting estimation
function \fct{bfit}
<<results = hide>>=
pm <- with(bf, bfit(x, y, family))
@
The estimated parameters returned from function \fct{bfit} can then be used as starting
values for the MCMC sampler function \fct{GMCMC}
<<results = hide>>=
samps <- with(bf, GMCMC(x, y, family, start = pm$parameters))
@
Using the parameters samples returned from function \fct{GMCMC}, statistics like the DIC are
computed using the \fct{samplestats} function
<<>>=
stats <- with(bf, samplestats(samps, x, y, family))
print(unlist(stats))
@
As can be seen in the code above, estimation engines have common arguments \code{x} (holding the
design and penalty matrices), \code{y} (the response data) and \code{family} (the \pkg{bamlss}
family object). For implementing new estimation engines, users only need to keep the argument
structures and the return values, i.e., for \fct{optimizer} functions a named numeric vector
of estimated parameters and for \fct{sampler} functions parameter samples of class \code{"mcmc"}
or \code{"mcmc.list"} (see package \pkg{coda}, \citealp{bamlss:Plummer+Best+Cowles+Vines:2006}).
More details on the naming convention and the structure of the return value of \fct{bamlss.frame}
are given in Section~\ref{sec:bf}.

To ease the modeling process, all the single modeling steps presented in the above can be executed
using the \pkg{bamlss} wrapper function \fct{bamlss}. The main arguments of \fct{bamlss} are
\begin{Code}
  bamlss(formula, family = "gaussian", data = NULL,
    transform = NULL,                         ## Pre-processing.
    optimizer = NULL, sampler = NULL,         ## Estimation.
    samplestats = NULL, results = NULL, ...)  ## Post-processing.
\end{Code}
where the first line basically represents the standard model frame specifications
\citep[see][]{bamlss:Chambers+Hastie:1992}. All other arguments represent functions presented
in Table~\ref{tab:functions} and can be exchanged. Note that the default for argument
\code{optimizer} is the backfitting estimation function \fct{bfit} and the default for argument
\code{sampler} is the \fct{GMCMC} sampling function.

The returned fitted model object is a list of class \class{bamlss}, which is supported by several
standard methods and extractor functions, such as \fct{plot}, \fct{summary} and \fct{predict}.

As already exemplified in Section~\ref{sec:motivation}, using the model fitting wrapper function
\fct{bamlss} it is straightforward to use different modeling approaches by simply exchanging
the estimation engines. This feature can be particularly important in complex modeling situation,
where good mixing of the MCMC algorithm requires very good starting values. A use case is presented
in Section~\ref{sec:application}, where for stability reasons posterior mode estimates are obtained
using the gradient boosting optimizer function \fct{boost}. Afterwards the MCMC sampling engine
\fct{GMCMC} is applied with the boosting estimates as starting values.

\subsection{The BAMLSS model frame} \label{sec:bf}

Similar to the well-known \fct{model.frame} function that is used, e.g., by the linear model 
fitting function \fct{lm}, or for generalized linear models \fct{glm}, the \fct{bamlss.frame}
function extracts a ``model frame'' for fitting distributional regression models. 
Internally, the function parses model formulae, one for each parameter of the distribution,
using the \pkg{Formula} package infrastructures \citep{bamlss:Zeileis+Croissant:2010} in combination
with \fct{model.matrix} processing for linear effects and \fct{smooth.construct} processing of
the \pkg{mgcv} package to setup design and penalty matrices for unspecified smooth function 
estimation (\citealp{bamlss:Wood:2019}, see also, e.g., the documentation of function \fct{s} and
\fct{te}).

The most important arguments are
\begin{Code}
  bamlss.frame(formula, data = NULL, family = "gaussian",
    weights = NULL, subset = NULL, offset = NULL,
    na.action = na.omit, contrasts = NULL, ...)
\end{Code}
The argument \code{formula} can be a classical model formulae, e.g., as used by the \fct{lm} function,
or an extended \pkg{bamlss} formula including smooth term specifications like \fct{s} or \fct{te},
that is internally parsed by function \fct{bamlss.formula}.
Note that the \pkg{bamlss} package uses special \code{family} objects, that can be passed either as
a character without the \code{"\_bamlss"} extension of the \pkg{bamlss} family name (see the manual
\code{?bamlss.family} for a list of available families), or the family function itself.
In addition, all families of the \pkg{gamlss} \citep{bamlss:Stasinopoulos+Rigby:2019} and
\pkg{gamlss.dist} \citep{bamlss:gamlss.dist} package are supported.

The returned object, a named list of class \code{"bamlss.frame"}, can be employed with the model
fitting engines listed in Table~\ref{tab:functions}. The most important elements used for
estimation are:
\begin{itemize}
\item \code{x}: A named list, the elements correspond to the parameters that are specified within
  the \code{family} object. For each distribution parameter, the list contains all design and
  penalty matrices needed for modeling (see the upcoming example).
\item \code{y}: The response data.
\item \code{family}: The processed \pkg{bamlss} \code{family}.
\end{itemize}
To better understand the structure of the \code{"bamlss.frame"} object a print method is provided.
For illustration, we simulate data and set up a \code{"bamlss.frame"} object for a Gaussian
distributional regression model including smooth terms.
<<>>=
## Simulate data.
d <- GAMart()
## Model formula.
f <- list(
  num ~ x1 + s(x2) + s(x3) + te(lon,lat),
  sigma ~ x1 + s(x2) + s(x3) + te(lon,lat)
)
## Create a "bamlss.frame".
bf <- bamlss.frame(f, data = d, family = "gaussian")
## Show the structure.
print(bf)
@
For writing a new estimation engine, the user can directly work with the \code{model.matrix}
elements, for linear effects, and the \code{smooth.construct} list, for smooth effects respectively.
The \code{smooth.construct} is a named list which is compiled using the \fct{smoothCon} function
of the \pkg{mgcv} package using the generic \fct{smooth.construct} method for setting up
smooth terms.
<<>>=
print(names(bf$x$mu$smooth.construct))
@
In this example, the list contains three smooth term objects for parameter \code{mu} and
\code{sigma}.

As shown in Section~\ref{sec:specialterms} the \fct{bamlss.frame} function can also
process special model terms, i.e., model terms that are not necessarily represented by
a linear matrix vector product.

\subsection{Family objects} \label{sec:families}

Family objects are important building blocks in the design of BAMLSS models.
They specify the distribution by collecting functions of the density,
respective log-likelihood, first-order derivatives of the log-likelihood w.r.t.\
predictors (the score function), and (optionally) second-order derivatives of
the log-likelihood w.r.t.\ predictors or their expectation (the Hessian).

The \pkg{bamlss} package can be easily extended by constructing families for specific
tasks, i.e., problems for which a likelihood can be formulated.
However, commonly used distributions are already implemented in \pkg{bamlss}; and the ones
from the \pkg{gamlss} package can also be accessed through the \pkg{bamlss} package.

We illustrate how to build a \pkg{bamlss} family by hand along the Gaussian
distribution, with density
$$
f(y\,|\,\mu,\sigma) = \frac{1}{\sqrt{2\pi}\sigma} \cdot \exp
\left( \frac{-(y-\mu)^2}{2\sigma^2} \right),
$$
and log-likelihood function
$$
\ell(\mu,\sigma\,|\,y) = - \frac{1}{2} \log(2\pi) - \log(\sigma) -
\frac{(y-\mu)^2}{2\sigma^2},
$$
for an individual observation. The sum of the log-likelihood function
over all observations is the target function of the optimization problem.

In the distributional regression framework the parameters are linked
to predictors by link functions,
$$
\mu = \eta_\mu, \qquad \log(\sigma) = \eta_\sigma.
$$
For the Gaussian $\mu$ and $\sigma$ are linked to $\eta_\mu$ and $\eta_\sigma$
by the identity function and the logarithm, respectively.

The score functions in \pkg{bamlss} are the first derivatives of the log-likelihood w.r.t.\
the predictors:
$$
s_\mu = \frac{\partial\ell}{\partial\eta_\mu} = \frac{\partial\ell}{\partial\mu} \cdot
\frac{\partial\mu}{\partial\eta_\mu} = \frac{y-\mu}{\sigma^2},
$$
and
$$
s_\sigma = \frac{\partial\ell}{\partial\eta_\sigma} = \frac{\partial\ell}{\partial\sigma} \cdot
\frac{\partial\sigma}{\partial\eta_\sigma} = -1 + \frac{(y-\mu)^2}{\sigma^2}.
$$

\begin{table}[ht!]
\centering
\begin{tabular}{lp{12cm}}
Name of element & Value \\ \hline
\code{family} & Character string with the name of the family. \\
\code{names} & Vector of character strings with the names of the parameters. \\
\code{links} & Vector of character strings with the names of the link functions \\
\code{d} & A function returning the density with arguments \code{y}, \code{par},
  \code{log = FALSE} (see below). \\
\code{score} & A list with functions (one for each parameter) returning the first derivatives of
  the log-likelihood w.r.t.\ predictors. \\
\code{hess} & A list with functions (one for each parameter) returning the negative second
  derivatives of the log-likelihood w.r.t.\ predictors.
\end{tabular}
\caption{\label{tab:families} Elements of the Gaussian distribution \code{"bamlss.family"} object.}
\end{table}

For the second derivative of the log-likelihood we are able to obtain the
negative expectation,
$$
\mathsf{E}(-\partial^2\ell / \partial\eta_{\mu}^{2} ) = \sigma^{-2},
$$
and
$$
\mathsf{E}(-\partial^2\ell / \partial\eta_{\sigma}^{2} ) = 2.
$$

Now we have to write a function that returns a \code{family.bamlss} object (S3)
which encapsulates functions for density, score and Hessian, and the names of
the family, parameter and link functions. The required elements are listed
in Table~\ref{tab:families}.

Merely all functions take as first argument the response \code{y} and as second
argument a named list holding the evaluated parameters \code{par} of the
distribution. The example implementation is shown in Appendix~\ref{appendix:families}.

Optionally, the \code{"family.bamlss"} object can be extended by functions for
\begin{itemize}
\item the cumulative distribution function \code{p(y, par, ...)},
\item the quantile function (the inverse cdf) \code{q(p, par)},
\item a random number generator \code{r(n, par)},
\item the log-likelihood \code{loglik(y, par)},
\item the expectation \code{mu(par, ...)},
\item initial values for optimization, which has to be a list containing a
  function for each parameter,
\item \code{...},
\end{itemize}
which can help to speed up optimization, or be convenient for
predictions and simulations.

For a list of all implemented families, please see the documentation of \code{?bamlss.family}.

\subsection{Estimation engines} \label{sec:engines}

Estimation engines in \pkg{bamlss} are usually based on the model frame setup function
\fct{bamlss.frame} (see Section~\ref{sec:bf}), i.e., the functions all have a \code{x} argument,
which contains all the necessary model and penalty matrices, and a \code{y} argument, which is the
response (univariate or multivariate). In addition, an estimation engine usually has a \code{family}
argument, which specifies the model to be estimated. However, this is not a mandatory
argument, i.e., one could write an estimation function that is designed for one specific
problem, only.

The modeling setup is best explained by looking at the main estimation engines provided by
\pkg{bamlss}. The default optimizer using the \fct{bamlss} wrapper function is \fct{bfit}, which is a
backfitting routine. The most important arguments are
\begin{Code}
  bfit(x, y, family, start = NULL, weights = NULL, offset = NULL, ...)
\end{Code}
The default sampling engine in \pkg{bamlss} is \fct{GMCMC}, again the most important
arguments are
\begin{Code}
  GMCMC(x, y, family, start = NULL, weights = NULL, offset = NULL, ...)
\end{Code}
So basically, the arguments of the optimizer and the sampling function are the same, the
main difference is the return value. In \pkg{bamlss} optimizer functions usually return
a vector of estimated regression coefficients (parameters), while sampling functions 
return a matrix of parameter samples of class \code{"mcmc"} or \code{"mcmc.list"} (for details
see the  documentation of the \pkg{coda} package).

Internally, what the optimizer or sampling function is actually processing is not important
for the \code{bamlss()} wrapper function as long as a vector or matrix of parameters is
returned. For optimizer functions the return value needs to be named list with an element
\code{"parameters"}, the vector (also a matrix, e.g., for \fct{lasso} and \fct{boost} optimizers)
of estimated parameters. The most important requirement to make use of all extractor
functions like \fct{summary.bamlss}, \fct{predict.bamlss}, \fct{plot.bamlss},
\fct{residuals.bamlss}, etc., is to follow the naming convention of the returned estimates.
The parameter names are based on the names of the distribution parameters as specified in
the family object. For example, the family object \fct{gaussian\_bamlss} has parameter names
\code{"mu"} and \code{"sigma"}
<<>>=
gaussian_bamlss()$names
@
Then, each distributional parameter can be modeled by parametric (linear) and
nonlinear smooth effect terms. The parametric part is indicated with \code{"p"} and the smooth part with
\code{"s"}. The names of the parametric coefficients are the names of the corresponding model
matrices as returned from \fct{bamlss.frame}. E.g., if two linear effects, with variables \code{"x1"}
and \code{"x2"}, enter the model for 
distributional parameter \code{"mu"}, then the final names are \code{"mu.p.x1"} and \code{"mu.p.x2"}.
Similarly for the smooth parts, if we model a variable \code{"x3"} using a regression spline as
provided by the \fct{s} function of the \pkg{mgcv} package, the
name is based on the names that are used by \fct{bamlss.frame} for the \fct{smooth.construct}
object. In this case the parameter names start with \code{"mu.s.s(x3)"}. If this smooth
term has 10 regression coefficients, then the final name must be
<<>>=
paste0("mu.s.s(x3)", ".b", 1:10)
@
i.e., all smooth term parameters are named with \code{"b"} and a numerated.

An example of how to setup an estimation engine for \pkg{bamlss} for linear regression models is
given in Appendix~\ref{appendix:engine}. The example also provides details on the naming convention
and return values of optimizer and sampler functions.


\section{Application} \label{sec:application}

This section illustrates the workflow with \pkg{bamlss} along a small case study.
We want to build a statistical model linking positive counts of cloud-to-ground
lightning discharges to atmospheric quantities from a reanalysis dataset.

The region we focus on are the European Eastern Alps. Cloud-to-ground lightning
discharges---detected by the Austrian Lightning Detection and Information System
\citep[ALDIS, ][]{schulz2005}---are counted on grids with a mesh size of $32~km$.
The lightning observations are available for the period 2010--2018.
The reanalysis data comes from the fifth generation of the ECMWF (European
Centre for Medium-Range Weather Forecasts) atmosphheric reanalyses of the
global climate \citep{era5}. ERA5 provides a globally complete and consistent
pseudo-observations of the atmosphere using the laws of physics. The horizontal
resolution is approx.\ $32~km$, while the temporal resolution is hourly and covers the years
from 1979 to present.
In this example application we work only with a small subset of the data, which
can be assessed from the accompanying \proglang{R} package \pkg{FlashAustria}
\citep{FlashAustria}.

<<application-load-model, echo=FALSE>>=
## Load the data.
# data("flash_austria", package = "FlashAustria")
data("fitted_model", package = "FlashAustria")
b <- flash_austria_model

## Specify formula.
f <- list(
  counts ~ s(d2m, bs = "ps") + s(q_prof_PC1, bs = "ps") +
    s(cswc_prof_PC4, bs = "ps") + s(t_prof_PC1, bs = "ps") +
    s(v_prof_PC2, bs = "ps") + s(sqrt_cape, bs = "ps"),
  theta ~ s(sqrt_lsp, bs = "ps")
)
@

<<application-data>>=
## Load the data.
data("flash_austria", package = "FlashAustria")
head(flash_austria_train)
nrow(flash_austria_train)
@

The motivation for this application is as follows: Lightning counts are not
modeled within the atmospheric reanalyses. Lightning observations are only
available for the period 2010--2018. With a statistical model on hand one could
predict lightning counts for the time before 2010 and thus analyze lightning
events in the past for which no observations are available.

The response of our statistical model are positive counts, with a mean of
\Sexpr{round(mean(flash_austria_train$counts), 2)}, and a variance of
\Sexpr{round(var(flash_austria_train$counts), 2)}. Thus, we are facing a
truncated count data distribution which is highly overdispersive.  In order to
capture the truncation of the data and its overdispersion we employ a
zero-truncated negative binomial distribution \citep{cameron2013count}, which
is specified by two parameters $\mu > 0$ and $\theta > 0$. $\mu$ is the
expectation of the underlying untruncated negative binomial, and $\theta$
modifies the variance of the untruncated negative binomial by
$\mathrm{VAR}(\tilde{Y}) = \mu + \mu^2/\theta$, where $\tilde{Y}$ is a latent
random variable following the underlying untruncated negative binomial
distribution.

The zero-truncated negative binomial distribution is implemented as
\code{ztnbinom_bamlss()} within \pkg{bamlss}.  In order to specify smooth terms
form both distributional parameter, the formula has to be a \code{list}. The
first element is named \code{mu} and specifies terms for the paramter $\mu$,
but also specifies that \code{counts} is the response of the regression model.
The second element is named \code{theta} and specifies a smooth term for the
paramter $\theta$. Hence well known for their sampling properties, we are
applying P-splines \citep{bamlss:Eilers+Marx:1996} for all terms. Specifying
smooth terms within \pkg{bamlss} formulae builds on the \pkg{mgcv} infrastructure
\citep{bamlss:Wood:2019} provided by \code{s()}, which leads to the following
specification of the model:

<<application-model1, eval=FALSE>>=
## Specify formula.
f <- list(
  counts ~ s(d2m, bs = "ps") + s(q_prof_PC1, bs = "ps") +
    s(cswc_prof_PC4, bs = "ps") + s(t_prof_PC1, bs = "ps") +
    s(v_prof_PC2, bs = "ps") + s(sqrt_cape, bs = "ps"), 
  theta ~ s(sqrt_lsp, bs = "ps")
)
@

Now we have all ingredients on hand to feed the standard interface for
statistical models in \proglang{R}: A formula \code{f}, a family
\code{ztnbinom_bamlss()}, and a data set \code{flash_austria_train}. Within the
\code{bamlss()} call we also provide arguments which are passed forward to the
optimizer and the sampler. We choose the gradient boosting optimizer
\code{boost()} in order to find initial values for the default sampler
\code{GMCMC()}. Gradient boosting proved to offer a very stable method for
finding regression coefficients that serve as initial values for a MCMC sampler
\citep{bamlss:Simon+Mayr+Umlauf+Zeileis:2019}.  We set the number of iteration
to $1000$. For the sampling we allow another $1000$ iterations as burn-in
phase, and apply a thinning of the resulting chain of $5$.  Running
\code{n.iter = 6000} iterations in total leads to $1000$ MCMC samples in the
end:

%%% <<application-model2, eval=FALSE>>=
%%% ## Fit model.
%%% b <- bamlss(f, family = "ztnbinom", data = flash_austria_train,
%%%   optimizer = boost, maxit = 1000,                ## Boosting arguments.
%%%   thin = 5, burnin = 1000, n.iter = 6000          ## Sampler arguments.
%%% )
%%% @
%%% 
\begin{Schunk}
\begin{Sinput}
R> ## Fit model.
R> b <- bamlss(f, family = "ztnbinom", data = flash_austria_train,
+    optimizer = boost, maxit = 1000,                 ## Boosting arguments.
+    thin = 5, burnin = 1000, n.iter = 6000           ## Sampler arguments.
+  )
\end{Sinput}
\begin{Soutput}
logLik -36924.3 eps 0.0007 iteration 1000 qsel 7
elapsed time:  5.43min
Starting the sampler...
|********************| 100%  0.00sec 27.73min
\end{Soutput}
\end{Schunk}

The model was fitted on a single core Intel i7-7700 CPU with 3.60GHz and 16~GB memory,
on which the boosting took less than 6~minutes and the MCMC sampling took less the
30~minutes. As a first diagnostic we check the log-likelihood contributions of the
individual terms during the boosting optimization (Fig.~\ref{fig:appboost}).

<<app-plot-boost, eval = FALSE>>=
boost.plot(b, which = "loglik.contrib", intercept = FALSE)
@

\setkeys{Gin}{width=.8\textwidth}
\begin{figure}
\begin{center}
<<app-plot-boost, fig = TRUE, height = 4, width = 6, echo = FALSE>>=
boost.plot(b, c("loglik.contrib"), intercept = FALSE)
# layout(matrix(c(rep(1,2), rep(2,3)), nrow = 1))
# boost.plot(b, "loglik")
# boost.plot(b, "loglik.contrib")
@
\end{center}
\caption{Contribution to the log-likelihood of individual terms during
gradient boosting.}
\label{fig:appboost}
\end{figure}

After 1000~iterations the term \code{s(sqrt_cape).mu} has the highest contribution
to the log-likelihood with %
\Sexpr{round(flash_austria_model$model.stats$optimizer$boost.summary$summary$mu["s(sqrt_cape)",2])} %
followed by \code{s(q_prof_PC1).mu} with %
\Sexpr{round(flash_austria_model$model.stats$optimizer$boost.summary$summary$mu["s(q_prof_PC1)",2])}.
The term of the parameter $\theta$ \code{s(sqrt_lsp).theta} has a relatively small
contribution with %
\Sexpr{round(flash_austria_model$model.stats$optimizer$boost.summary$summary$theta["s(sqrt_lsp)",2])}.
The overall message of this diagnostic is that the contributions to the
log-likelihood at the end of the boosting procedure are very small and that
the algorithm approached a stable state, which suggest that we retrieve
reasonable initial values for the MCMC sampling.

The MCMC chains are investigated by looking directly at the traces of the chains
and with the auto-correlation function of the chains.

<<app-plot-trace, fig = FALSE, eval = FALSE>>=
plot(b, model = "mu", term = "s(sqrt_cape)", which = "samples")
@

Figure \ref{fig:apptrace} shows the traces and the auto-correlation functions
for two regression coefficients of the term \code{s(sqrt_cape)}. The traces
reveal samples around stables means. This suggests that the 1000 boosting
iterations and the 1000 burn-in samples were sufficient in order to approach
reasonable starting values for the sampling. The auto-correlation functions
reveal that after the thinning hardly any correlation remains between
consecutive samples.

\begin{figure}
\begin{center}
<<echo = FALSE, fig = TRUE>>=
b2 <- b
b2$samples[[1]] <- b2$samples[[1]][, grep("s(sqrt_cape)", colnames(b2$samples[[1]]), fixed = TRUE)][, 1:2]

par(mar = c(4.1, 4.1, 4.1, 1.1))
plot(b2, which = "samples", ask = FALSE)
@
\end{center}
\caption{MCMC trace (left panels) and auto-correlation (right panels) for two
splines from the term \code{s(sqrt\_cape)} of the model \code{mu}.}
\label{fig:apptrace}
\end{figure}

As these diagnostics suggest that a reasonable initial state for the sampling
has been found and the samples are independent draws from the posterior, one
can go further and investigate the estimated effects. The boosting summary
(Fig.~\ref{fig:appboost}) revealed that the terms \code{s(sqrt_cape)} and
\code{s(q_prof_PC1)} had a large contribution for improving the fit. 
Looking at these effects illustrate how the atmospheric parameters
of the reanalyses are related to lightning events (Fig.~\ref{fig:appeffect}),
and thus help to understand the physics associated with lightning events.
The effects are presented on the scale of the linear predictor, i.e., the
log scale.

<<app-plot-effect1, fig = FALSE, eval = FALSE>>=
plot(b, term = c("s(sqrt_cape)", "s(q_prof_PC1)", "s(sqrt_lsp)"))
@

\begin{figure}
\begin{center}
\setkeys{Gin}{width=1\textwidth}
<<echo = FALSE, fig = TRUE, height = 3, width = 6>>=
par(mfrow = c(1, 3))
plot(b, term = c("s(sqrt_cape)", "s(q_prof_PC1)", "s(sqrt_lsp)"),
  ask = FALSE, spar = FALSE, scheme = 2, grid = 200,
  rug = TRUE, col.rug = "#39393919")
@
\end{center}
\caption{Effect of the terms \code{s(sqrt\_cape)} and \code{s(q\_prof\_PC1)}
from the model \code{mu} and term \code{s(sqrt\_lsp)} from model \code{theta}.
Credible intervals derived from MCMC samples.}
\label{fig:appeffect}
\end{figure}

\code{s(sqrt_cape)} reveals a monotonic increasing shape. In the
range from $0$--$30$ the effect increases linearly with small credible intervals.
For higher values the effect flattens and shows large credible intervals which are
associated with the small amount of data in that range. Physically the shape of the
effect is meaningful as more convective available potential energy has the
potential to lead to heavier lightning events.
\code{s(q_prof_PC1)} reveals areas of large credible intervals at the left and
right bounds of the range due to small amount of data. In the mid-range a
increasing effect is identified. As \code{q_prof_PC1} is the leading principal
component of the vertical profile of specific humidity, one has to consider the
corresponding spatial mode (not shown) for interpretation: Higher values of
\code{q_prof_PC1} are linked to more moisture in the lower atmosphere,
which is also available as a source of \emph{latent energy}, i.e., energy that
becomes free when water transfers from the gas to the liquid phase.

Finally it is interesting to look at the effect acting on the link scale of
the parameter $\theta$, \code{s(sqrt_lsp)} (right panel in Fig.~\ref{fig:appeffect}).
\code{sqrt_lsp} is the square root of large scale precipitation, i.e.,
precipitation that is not linked to convective processes and thus it is
not related to strong lightning events. The effect shows following relationship:
Higher values of \code{sqrt_lsp} lead to smaller $\theta$, which increases the
variance of the distribution.

Before applying the model, i.e., predicting lightning cases before 2010, we
check the marginal calibration of the distribution by hanging rootogram, a tool
popular for the evaluation of count data regression models
\citep{kleiber2016visualizing}. First we predict the distributional parameter
on out-of-sample data \code{flash_austria_eval} for which lightning
observations are on hand.

<<app-rootogram1>>=
## Out-of-sample prediction.
fit <- predict(b, newdata = flash_austria_eval, type = "parameter")
str(fit)
@

\code{predict()} returns a \code{list}, of which each element is named
as a distributional parameter and contains by default a vector of
predictions. Each prediction is the average of the predictions obtained
by all MCMC samples. The resulting \code{list} can be used to
derive further quantities by employing the functions of the \pkg{bamlss}
family that can be extracted using \code{family()},

<<app-rootogram2>>=
## Extract family.
fam <- family(b)
fam
@

The family contains functions to map the predictors to the parameter
scale, density, cumulative distribution function, log-likelihood, and
scores and hessian. We apply the density to compute the expected
frequencies of the positive counts. The function \code{...$d} takes the
quantile as first argument, and the \code{list} with the parameters,
as returned by \code{predict()}, as a second argument. 

<<app-rootogram3>>=
## Expected frequencies.
expect <- sapply(1:50, function(j) sum(fam$d(j, fit)))
@

In order to plot the rootogram, we have to name the vector and
coerce it to an object of class \code{table}. The verifying
observed frequencies can be directly obtained by \code{table}.

<<app-rootogram4>>=
names(expect) <- 1:50
expect <- as.table(expect)

## Observed frequencies.
obsrvd <- table(flash_austria_eval$counts)[1:50]
@

The observed and expected frequencies can be plugged into the
default method of \code{rootogram} from the \pkg{countreg}
package \citep{zeileis2008regression}.

<<app-plot-rootogram1, eval = FALSE>>=
## Plot rootogram.
library("countreg")
rootogram(obsrvd, expect, xlab = "# Counts", main = "Rootogram")
@

\setkeys{Gin}{width=.7\textwidth}
\begin{figure}
\begin{center}
<<app-plot-rootogram2, fig = TRUE, height = 5, width = 6, echo = FALSE>>=
## Plot rootogram.
library("countreg")
rootogram(obsrvd, expect, xlab = "# Counts", main = "Rootogram")
@
\end{center}
\caption{Hanging rootogram for evaluating calibration of count data model
on out-of-sample data. Red line indicates the expected frequencies on the
square root scale. Gray bars indicate observed frequencies on square root
scale hanging from the red line.}
\label{fig:approoto}
\end{figure}

The rootogram reveals reasonable calibration of the model though it is slightly
underestimating the number of events with a single lightning discharge.
Now given good convergence and sample characteristics of the gradient
boosting optimizer and MCMC sampler, physically interpretable effects, and
good out-of-sample calibration, we can take our model and predict a case
for the period before 2010, for which no lightning data are available.
The case of interest is a front moving from the West to the East on the
Northern side of the Alps on 2001-09-15 and 2001-09-16.
The case data \code{flash_austria_case} contains additional columns containing
time and space information, and is of class \code{sf} \citep{pebesma2018sf}.
We predict the parameters for this case, and derive the probability of
observing 10 or more flashes within a grid box conditioned on a thunderstorm activity,
by applying the cumulative distribution function \code{...$p} of the family.

<<app-plot-case1>>=
## Predict case.
library("sf")
fit <- predict(b, newdata = flash_austria_case, type = "parameter")
flash_austria_case$P10 <- 1 - fam$p(9, fit)
@

We visualize this case by employing \code{ggplot()} \citep{wickham2016gg},
and the Inferno color scale from the \pkg{colorspace} package
\citep{zeileis2019colorspace}. The country borders \code{world} are retrieved
from the \pkg{rnaturalearth} package \citep{south2017rnaturalearth}. 

<<app-plot-case2, eval = FALSE>>=
## Plot case.
library("ggplot2")
library("colorspace")

ggplot() + geom_sf(aes(fill = P10), data = flash_austria_case) +
  scale_fill_continuous_sequential("Inferno", rev = TRUE) +
  geom_sf(data = world, col = "white", fill = NA) +
  coord_sf(xlim = c(8, 17), ylim = c(45.5, 50), expand = FALSE) +
  facet_wrap(~time) + theme_minimal()
@

\setkeys{Gin}{width=1\textwidth}
\begin{figure}
\begin{center}
<<app-plot-case-eval, fig = TRUE, echo = FALSE>>=
library("ggplot2")
library("colorspace")

library("rnaturalearth")
library("rnaturalearthdata")
world <- ne_countries(scale = "medium", returnclass = "sf")

ggplot() + geom_sf(aes(fill = P10), data = flash_austria_case) +
  scale_fill_continuous_sequential("Inferno", rev = TRUE) +
  geom_sf(data = world, col = "white", fill = NA) +
  coord_sf(xlim = c(7.95, 17), ylim = c(45.45, 50), expand = FALSE) +
  facet_wrap(~time, nrow = 2) + theme_minimal() # +
#  theme(legend.position = "bottom")
@
\end{center}
\caption{A probabilistic reconstruction of lightning counts occured on
September 15 2001 at 6~UTC, 17~UTC and 23~UTC and on September 16 2001 at
13~UTC, i.e., the probability of having observed $10$ or more counts within one
grid box.}
\label{fig:appcase}
\end{figure}

The figure reveals that the probability for strong lightning events increases
during 2001-09-15 between 6 and 17~UTC. During night time the front occurs,
which can be nicely seen at 23~UTC. The propagation of the front is blocked
by the main Alpine ridge located at $47^\circ~N$. On the subsequent day
2001-09-16 one can see that the probability on the downwind side of the
Alps has increased.

\section{Achknowledgements}
T.~Simon acknowledges the funding by the Austrian Science Fund (FWF, grant no.~P31836)

\bibliography{bamlss}

\clearpage

\begin{appendix}

\section[Custom CRPS() function]{Custom \fct{CRPS} function}
\label{appendix:crps}

The \proglang{R} package \pkg{scoringRules} \citep{bamlss:Jordan+Krueger+Lerch:2018} provides
tools for model calibration checks. A commonly used measure is the CRPS. Since the number of
candidate distributions in BAMLSS is quite large, it can happen that the CRPS for some
distributions is not implemented. In such a case the reader can implement the CRPS using
numerical integration. The following \proglang{R} code implements the \fct{CRPS} function for
the normal distribution to be used with \pkg{bamlss} and the simulated motocycle data
presented in Section~\ref{sec:locscalemodel}.
<<>>=
CRPS <- function(par) {
  n <- nrow(mcycle)
  crps <- rep(0, n)
  for(i in 1:n) {
    tpar <- list("mu" = par$mu[i], "sigma" = par$sigma[i])
    foo <- function(y) {
      (fam$p(y, tpar) - 1 * (y >= mcycle$accel[i]))^2
    }
    crps[i] <- integrate(foo, -Inf, Inf)$value
  }
  return(crps)
}
@

\section[Growth curve implementation]{Growth curve implementation}
\label{appendix:gc}

The following \proglang{R} code implements a Gompertz growth curve model term given by
$$
f(x; \boldsymbol{\beta}) = \beta_1 \cdot \exp(-\beta_2 \cdot \exp(-\beta_3 \cdot x)),
$$
which can be used by the default optimizer function \fct{bfit} and sampling function \fct{GMCMC}
of the \pkg{bamlss} package (see Section~\ref{sec:specialterms}).

The new \fct{smooth.construct} method is
<<>>=
smooth.construct.gc.smooth.spec <- function(object, data, knots) 
{
  object$X <- matrix(as.numeric(data[[object$term]]), ncol = 1)
  center <- if(!is.null(object$xt$center)) {
    object$xt$center
  } else TRUE
  object$by.done <- TRUE
  if(object$by != "NA")
    stop("by variables not supported!")

  ## Begin special elements to be used with bfit() and GMCMC().
  object$fit.fun <- function(X, b, ...) {
    f <- b[1] * exp(-b[2] * exp(-b[3] * drop(X)))
    if(center)
      f <- f - mean(f)
    f
  }
  object$update <- bfit_optim
  object$propose <- GMCMC_slice
  object$prior <- function(b) { sum(dnorm(b, sd = 1000, log = TRUE)) }
  object$fixed <- TRUE
  object$state$parameters <- c("b1" = 0, "b2" = 0.5, "b3" = 0.1)
  object$state$fitted.values <- rep(0, length(object$X))
  object$state$edf <- 3
  object$special.npar <- 3 ## Important!
  ## End special elements.

  ## Important, This is a special smooth constructor!
  class(object) <- c("gc.smooth", "no.mgcv", "special")

  object
}
@
In principle, the setup is very similar to the smooth constructor functions provided by the
\pkg{mgcv} package. Only few elements need to be added:
\begin{itemize}
\item \fct{fit.fun}: A function of the data \code{X} and parameter vector \code{b} that
  evaluates the fitted values.
\item \fct{update}: An updating function to be used with optimizer \fct{bfit}.
\item \fct{propose}: A MCMC propose function to be used with sampler \fct{GMCMC}.
\item \fct{prior}: Function of the parameters \code{b} that evaluates the log-prior. Note,
  additional functions can be \fct{grad} and \code{hess} that evaluate the first and second
  derivative of the log-prior w.r.t.\ the parameters \code{b}.
\item \code{fixed}: Is the number of degrees of freedom fixed or not?
\item \code{state}: This is a named list with starting values for the \code{"parameters"},
  the \code{"fitted.values"} and degrees of freedom \code{"edf"}. Note that regression
  coefficients are always named with \code{"b*"} and shrinkage or smoothing variances with
  \code{"tau2*"} in the \code{"parameters"} vector.
\item \code{special.npar}: How many parameters does this model term have in total? This is needed
  for internal setup, because the Gompertz function has three parameters but the design matrix only one
  column.
\end{itemize}
To compute predictions of this model term a new method for the \fct{Predict.matrix} function needs
to be implemented, too.
<<>>=
Predict.matrix.gc.smooth <- function(object, data, knots) 
{
  X <- matrix(as.numeric(data[[object$term]]), ncol = 1)
  X
}
@

\section[Gaussian family object]{Gaussian family object} \label{appendix:families}

The following \proglang{R} code shows an example implementation of the Gaussian distribution
as presented in Section~\ref{sec:families}.
<<>>=
Gauss_bamlss <- function(...) {
  f <- list(
    "family" = "mygauss",
    "names"  = c("mu", "sigma"),
    "links"  = c(mu = "identity", sigma = "log"),
    "d" = function(y, par, log = FALSE) {
      dnorm(y, mean = par$mu, sd = par$sigma, log = log)
    },
    "score" = list(
      mu = function(y, par, ...) {
        drop((y - par$mu) / (par$sigma^2))
      },
      sigma = function(y, par, ...) {
        drop(-1 + (y - par$mu)^2 / (par$sigma^2))
      }
    ),
    "hess" = list(
      mu = function(y, par, ...) {
        drop(1 / (par$sigma^2))
      },
      sigma = function(y, par, ...) { 
        rep(2, length(y))
      }
    )
  )
  class(f) <- "family.bamlss"
  return(f)
}
@


\section[Engines for linear regression]{Model fitting engines for linear regression} \label{appendix:engine}

In the following, to explain the setup and the naming convention of estimation engines in
more detail, we implement
\begin{itemize}
\item a new family object for simple linear models $y = x^{\top}\boldsymbol{\beta} + \varepsilon$ with
  $\varepsilon \sim N(0, \sigma^2)$,
\item and set up an optimizer function,
\item and additionally a MCMC sampling function.
\end{itemize}
For illustration, the family object is kept very simple, we only model the mean function 
in terms of covariates.
<<>>=
lm_bamlss <- function(...) {
  f <- list(
    "family" = "LM",
    "names" = "mu",
    "links" = "identity",
    "d" = function(y, par, log = FALSE) {
      sigma <- sqrt(sum((y - par$mu)^2) / (length(y) - .lm_bamlss.p))
      dnorm(y, mean = par$mu, sd = sigma, log = log)
    },
    "p" = function(y, par, ...) {
      sigma <- sqrt(sum((y - par$mu)^2) / (length(y) - .lm_bamlss.p))
      pnorm(y, mean = par$mu, sd = sigma, ...)
    }
  )
  class(f) <- "family.bamlss"
  return(f)
}
@
Now, for setting up the estimation functions we first simulate some data using the
\fct{GAMart} function, afterwards the necessary \code{"bamlss.frame"} can be created with
<<>>=
## Simulate some data.
d <- GAMart()

## Setup a "bamlss.frame" object, that is used for
## developing the estimation functions for the linear model.
bf <- bamlss.frame(num ~ x1 + x2, data = d, family = "lm")

## Print the structure of the "bamlss.frame".
print(bf)
@
As noted above, the object is a named list with elements \code{"x"} and \code{"y"}, which will be
passed to the estimation functions. For the moment, since we only implement a linear 
model, we need to work with the linear model matrix that is part of the \code{bf} object.
<<>>=
print(head(bf$x$mu$model.matrix))
@
and the response \code{"y"}
<<>>=
print(head(bf$y))
@
to setup the optimizer function with:
<<>>=
## Linear model optimizer function.
lm.opt <- function(x, y, ...)
{
  ## Only univariate response.
  y <- y[[1L]]

  ## For illustration this is easier to read.
  X <- x$mu$model.matrix

  ## Estimate model parameters.
  par <- drop(chol2inv(chol(crossprod(X))) %*% crossprod(X, y))

  ## Set parameter names.
  names(par) <- paste0("mu.p.", colnames(X))

  ## Return estimated parameters and fitted values.
  rval <- list(
    "parameters" = par,
    "fitted.values" = drop(X %*% par),
    "edf" = length(par),
    "sigma" = drop(sqrt(crossprod(y - X %*% par) / (length(y) - ncol(X))))
  )

  ## Set edf within .GlobalEnv for the
  ## loglik() function in the lm_bamlss() family.
  .lm_bamlss.p <<- length(par)

  return(rval)
}
@
This optimizer function can already be used with the \fct{bamlss} wrapper function and
all extractor functions are readily available.
<<>>=
## Model formula with polynomial model terms.
f <- num ~ x1 + poly(x2, 5) + poly(x3, 5)

## Estimate model with new optimizer function.
b <- bamlss(f, data = d, family = "lm", optimizer = lm.opt, sampler = FALSE)

## Summary output.
summary(b)

## Predict for term x2.
nd <- data.frame("x2" = seq(0, 1, length = 100))
nd$p <- predict(b, newdata = nd, term = "x2")
@
Plot the estimated effect of variable \code{x2}.
<<eval = FALSE>>=
plot2d(p ~ x2, data = nd)
@

The next step is to setup a full Bayesian MCMC sampling function. Fortunately, if we
assume multivariate normal priors for the regression coefficients and an inverse Gamma prior
for the variance, a Gibbs sampler with multivariate normal and inverse Gamma full conditionals
can be created. The MCMC algorithm consecutively samples for $t = 1, \ldots, T$ from the full
conditionals
$$
\boldsymbol{\beta}^{(t)} | \cdot \sim N\left(\boldsymbol{\mu}_{\boldsymbol{\beta}}^{(t - 1)}, 
  \boldsymbol{\Sigma}_{\boldsymbol{\beta}}^{(t - 1)}\right)
$$
and
$$ 
{\sigma^2}^{(t)} | \cdot \sim IG\left({a^{\prime}}^{(t - 1)}, {b^{\prime}}^{(t - 1)}\right),
$$
where $IG( \cdot )$ is the inverse Gamma distribution for sampling the variance parameter.
The covariance matrix for $\boldsymbol{\beta}$ is given by
$$
\boldsymbol{\Sigma}_{\boldsymbol{\beta}} = \left(\frac{1}{\sigma^2}\mathbf{X}^\top\mathbf{X} +
  \frac{1}{\sigma^2}\mathbf{M}^{-1}\right)^{-1}
$$
and the mean
$$
\boldsymbol{\mu}_{\boldsymbol{\beta}} = \boldsymbol{\Sigma}_{\boldsymbol{\beta}}
  \left(\frac{1}{\sigma^2}\mathbf{X}^\top\mathbf{y} +
  \frac{1}{\sigma^2}\mathbf{M}^{-1}\mathbf{m}\right),
$$
where $\mathbf{m}$ is the prior mean and $\mathbf{M}$ the prior covariance matrix.
Similarly, for $\sigma^2$ paramaters $a^{\prime}$ and $b^{\prime}$ are computed by
$$
a^{\prime} = a + \frac{n}{2} + \frac{p}{2}
$$
and
$$
b^{\prime} = b + \frac{1}{2}(\mathbf{y} -
  \mathbf{X}\boldsymbol{\beta})^\top(\mathbf{y} - \mathbf{X}\boldsymbol{\beta}) +
  \frac{1}{2} (\boldsymbol{\beta} - \mathbf{m})^\top \mathbf{M}^{-1}(\boldsymbol{\beta}
  - \mathbf{m}),
$$
where $a$ and $b$ are usually set small, e.g., with $a = 1$ and $b = 0.0001$, such that the prior is
flat and uninformative.

We can implement the MCMC algorithm in the following sampling function
<<>>=
lm.mcmc <- function(x, y, start = NULL,
  n.iter = 12000, burnin = 2000, thin = 10,
  m = 0, M = 1e+05,
  a = 1, b = 1e-05,
  verbose = TRUE, ...)
{
  ## How many samples are saved?
  itrthin <- seq.int(burnin, n.iter, by = thin)
  nsaves <- length(itrthin)

  ## Only univariate response.
  y <- y[[1L]]

  ## For illustration this is easier to read.
  X <- x$mu$model.matrix

  ## Again, set edf within .GlobalEnv for the
  ## loglik() function in the lm_bamlss() family.
  .lm_bamlss.p <<- ncol(X)

  ## Number of observations and parameters.
  n <- length(y)
  p <- ncol(X)

  ## Matrix saving the samples.
  samples <- matrix(0, nsaves, p + 1L)

  ## Stick to the naming convention.
  pn <- paste0("mu.p.", colnames(X))
  colnames(samples) <- c(
    pn,      ## Regression coefficients and
    "sigma"  ## variance samples.
  )

  ## Setup coefficient vector,
  ## again, use correct names.
  beta <- rep(0, p)
  names(beta) <- pn
  sigma <- sd(y)

  ## Check for starting values obtained,
  ## e.g., from lm.opt() from above.
  if(!is.null(start)) {
    sn <- names(start)
    for(j in names(beta)) {
      if(j %in% sn)
        beta[j] <- start[j]
    }
  }

  ## Process prior information.
  m <- rep(m, length.out = p)
  if(length(M) < 2)
    M <- rep(M, length.out = p)
  if(!is.matrix(M))
    M <- diag(M)
  Mi <- solve(M)

  ## Precompute cross products.
  XX <- crossprod(X)
  Xy <- crossprod(X, y)

  ## Inverse gamma parameter.
  a <- a + n / 2 + p / 2

  ## Start sampling.
  ii <- 1
  for(i in 1:n.iter) {
    ## Sampling sigma
    b2 <- b + 1 / 2 * t(y - X %*% beta) %*% (y - X %*% beta) +
      1 / 2 * t(beta - m) %*% Mi %*% (beta - m)
    sigma2 <- sqrt(1 / rgamma(1, a, b2))

    ## Sampling beta.
    sigma2i <- 1 / sigma2
    Sigma <- chol2inv(chol(sigma2i * XX + sigma2i * Mi))
    mu <- Sigma %*% (sigma2i * Xy + sigma2i * Mi %*% m)
    beta <- MASS::mvrnorm(1, mu, Sigma)
      
    if(i %in% itrthin) {
      samples[ii, pn] <- beta
      samples[ii, "sigma"] <- sqrt(sigma2)
      ii <- ii + 1
    }
    if(verbose) {
      if(i %% 1000 == 0)
        cat("iteration:", i, "\n")
    }
  }

  ## Convert to "mcmc" object.
  samples <- as.mcmc(samples)

  return(samples)
}
@
The new sampling function can be directly used with the \fct{bamlss} wrapper
<<>>=
b <- bamlss(f, data = d, family = "lm", optimizer = lm.opt, sampler = lm.mcmc)

summary(b)

## Predict for all terms including 95% credible intervals
nd$x1 <- nd$x3 <- seq(0, 1, length = 100)
for(j in c("x1", "x2", "x3")) {
  nd[[paste0("p.", j)]] <- predict(b, newdata = nd, term = j,
    FUN = c95, intercept = FALSE)
}
@
The estimated effects can be plotted with:
<<eval = FALSE>>=
par(mfrow = c(1, 3))
plot2d(p.x1 ~ x1, data = nd, fill.select = c(0, 1, 0, 1), lty = c(2, 1, 2))
plot2d(p.x2 ~ x2, data = nd, fill.select = c(0, 1, 0, 1), lty = c(2, 1, 2))
plot2d(p.x3 ~ x3, data = nd, fill.select = c(0, 1, 0, 1), lty = c(2, 1, 2))
@

\end{appendix}

\end{document}

